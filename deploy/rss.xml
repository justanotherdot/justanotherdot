<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>justanotherdot</title>
    <link>https://justanotherdot.com</link>
    <atom:link href="https://justanotherdot.com/rss.xml" rel="self" type="application/rss+xml" />
    <description>Personal blog of Ryan James Spencer</description>
    <category>Technology</category>
    <copyright>2018 Ryan James Spencer</copyright>
    <language>en-us</language>
    
    <item>
      <title>A Plea For Style Guides</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a_plea_for_style_guides.html</link>
      <guid>https://justanotherdot.com/posts/a_plea_for_style_guides.html</guid>
      <pubDate>Wed, 28 Aug 2019 20:16:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You commonly hear two particular attributes that drive style guides, and,
subsequently, automatic formatting tools: 'consistency' and 'readability'. The
argument goes that a developer reads a codebase far more than any other
interaction.</p>
<p>Now, you could always take a codebase that has a style and use <a href="https://github.com/antlr/codebuff">machine learning
to generate a formatter</a> to keep things
'consistent' and 'readable'. This would get around the subjective definition of
readability because it's what the team picked through usage. Some feel that a
community driven style guide is ideal because then the codebase's 'readability'
is 'consistent' with the larger ecosystem, so formatting tools should simply be
blindly adopted.</p>
<p>Unfortunately, they are focusing on the wrong thing.</p>
<p>I have read a lot of code. I care about it as a practice and I like teaching
others how to do it, but I don't think it's the right metric for a style guide.
Bar things like the <a href="https://www.ioccc.org/">obfuscated C contest</a>, minified
markup and javascript, and many other mind-melting formats , most code I see is
actually quite 'readable'. Consistency is no better because you can have a style
that is consistently spaghetti.</p>
<p>In truth, <strong>developers change code far more often than they read and write new
code and they sure as hell should be <em>deleting</em> code with a frantically high
frequency, as well, if they aren't already.</strong></p>
<p>About two years back someone mentioned <a href="https://elm-lang.org/docs/style-guide">the elm style
guide</a> to me. The focus on
ease-of-modification <em>for a human</em> was eye-opening. With this mindset, alignment
was pointless. What good would it do a developer to re-align things after making
a change than to simply let them make the change by itself, communicate it
simply to their peers, and get it into master ASAP?</p>
<p>Then, later, another practice I adopted was adding newlines to assignments/let
bindings, taken from a team of brilliant software engineers. Every time I wrote
an <code>=</code> I would hit enter, allowing the name of the thing and its guts to be
distinct. The contents of the variable could be expanded, shrunk, removed
entirely, turned into an error, whatever. The name could be changed to better
suit constantly shifting needs and not highlight the guts in code review. It was
a handy pair.</p>
<p>I found sometimes having stuff as modifiable or deletable meant you would get
the other for free, similar to what people claim about consistency and
readability. In the end, the specific practices aren't important here. What is
important is that gaining momentum and keeping up against inertia is pivotal
when keeping a project relevant and rot-free. And, if you do the same thing
everywhere, you'll inevitably get a 'consistent' codebase, anyways!</p>
<p>Next time you write a style guide, try to think about the sea of changes that
will need to take place and the stuff that will get old and need to die before
you consider your codebase as another magnum opus.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>May You Be The Author of 2^N Programs</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/may_you_be_the_author_of_two_to_the_n_programs.html</link>
      <guid>https://justanotherdot.com/posts/may_you_be_the_author_of_two_to_the_n_programs.html</guid>
      <pubDate>Mon, 26 Aug 2019 20:00:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>The sheer propensity of articles detailing productivity tips for software
developers under the auspices of them becoming better employees is alarming.
What about our mental health? Why not more articles about our productivity from
the point of view of improving our ability to deal with discomfort, burn out,
imposter syndrome, and so on?</p>
<p><strong>Progress is key.</strong></p>
<p><a href="http://jsomers.net/blog/speed-matters">Speed matters</a> because it principally
enables us to make progress with our work and our life. Half-finished projects
weigh on us like mangled fruit on a dying tree. Project deadlines sneer at us
right around the corner and we put unfair stress on ourselves to both reach a
deadline <em>and</em> achieve excellence. <strong>We cannot be happy and, therefore, productive
if we are unable to be flexible. The only way is to accept mess.</strong></p>
<p>The book <a href="https://www.goodreads.com/book/show/187633.Art_and_Fear">Art and Fear</a>
is chiefly about two things:</p>
<ul>
<li>Proliferation and practice are the way to improvement</li>
<li>Successes can only be determined based on one's own history</li>
</ul>
<p>One analogy that is given in the book is a pottery instructor who divides his
class in two. Half of the class will be judged by quantity and the other half by
quality. In the end, it is the quantity group that has the best work through the
simple act of constantly learning from their failures. Reflection and planning
<em>are</em> crucial but the quantity-based group's ability to accept the mess that
comes with failures is what drives their progress. Fantastic! All we need to do
is just constantly crank things out and we'll be masters of our medium in no
time.</p>
<p>Except for some of us mess instills great discomfort.</p>
<p>At the beginning of the year I went to therapy. I was burnt out from work and
other stressors in life. Many sessions later I am diagnosed with Obsessive
Compulsive Disorder, which in some ways was a shock and in others not a
surprise. Eventually, exposure therapy comes up as a tool to help tackle the
anxiety and stress from parts of my affliction. There I was, sitting with my
discomfort, watching it, not trying to solve it nor run away from it, and sure
enough, the discomfort would melt away with time. I started convincing some part
of myself that the discomfort I was dredging up was <em>not</em> based on fact and I
found myself able to do things I had been so slow or entirely unable to do for
ages.</p>
<p><strong>It has dawned on me that sitting with the discomfort of what you produce is part
of learning from the failures.</strong></p>
<p>You <em>can</em> defer some decisions for later and put in rudimentary solutions in the
meantime. You <em>will</em> probably save time by making progress in this manner that
will allow you to revisit those same problems you deferred. Sure, this may not
always be the case for those trapped in <a href="https://cutle.fish/blog/12-signs-youre-working-in-a-feature-factory">feature
factories</a>,
but that's an organisational issue rather than a personal one. You owe it to
your mental health to grow more flexible by accepting mess.</p>
<p>May you the author of 2^N programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Move Fast and Tuck Code Into the Shadows</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/move_fast_and_tuck_code_into_the_shadows.html</link>
      <guid>https://justanotherdot.com/posts/move_fast_and_tuck_code_into_the_shadows.html</guid>
      <pubDate>Fri, 23 Aug 2019 21:51:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Migrations are a part of life as a dev. They help <a href="https://lethain.com/migrations/">cut down tech
debt</a> but they can be risky. It's always less
risky merging in <em>new</em> and <em>different</em> sets of changes rather than changing code
in-place. This buys you time. <em>You</em> gain the control over the switch granted
switching doesn't adversely affect some shared, mutable store of data.</p>
<p>The <a href="http://sevangelatos.com/john-carmack-on-parallel-implementations/">parallel implementation
approach</a> is
brilliance incarnate; you keep a functional reference implementation and you
copy it as your 'experimental' version whose sole aim is to eventually replace
(and hence become) the new reference. However, Carmack hits a good point,</p>
<blockquote>
<p>It is often tempting to shortcut this by passing in some kind of option flag
to existing code, rather than enabling a full parallel implementation. It is
a grey area, but I have been tending to find the extra path complexity with
the flag approach often leads to messing up both versions as you work, and
you usually compromise both implementations to some degree.</p>
</blockquote>
<p>I am keen to start experimenting more with the Carmack approach, though. Some
things I've already thought about:</p>
<ul>
<li>Having a duplicated directories messes up navigation for a lot of editors and
is unnecessary bloat</li>
<li><code>git flow</code> styled approaches and any vcs-based approach will never work
because it lends into the 'change in place' idea by merging the reference with
the experiment</li>
</ul>
<p>Otherwise, there are many ways to define clear boundaries between the reference
and experimental implementation. The most popular solution out of many is
feature flag services but I recommend switching between whole modules rather
than having a lot of logic caked into modules to check flags. Keeping flags
macro and mutually exclusive is important because it means changes are kept
cohesive and conflict free. One thing people tend to forget about is the
original feature-flag: versioning. In the end it doesn't matter which technique
you employ so long as you can 1. <strong>toggle between changes</strong> and 2. <strong>keep
differences clear</strong>.</p>
<p>You can start something similar to this approach by focusing on leaving written
code in a disconnected state but being aggressive about it finding its way to
master. This is healthy because it will change the incumbent attitude of
&quot;production means done&quot; to &quot;production means refinement&quot;. I like this approach
and do it as often as I can remember to because it means PRs are kept small
(great for code review) and when I finally do want to rig everything up I can
focus squarely on the plumbing, rather than juggling the correctness of the core
implementation <em>and</em> the coupling to the rest of the system.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Make a home</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/make_a_home.html</link>
      <guid>https://justanotherdot.com/posts/make_a_home.html</guid>
      <pubDate>Thu, 22 Aug 2019 20:49:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I'd like to preface this article that analogies are rough comparisons. That is
why, after all, they are analogies. We say that one thing is <em>like</em> another, but
it is not to say they are the same, point-for-point.</p>
<p>The analogies to construction and architecture in software are abundant. We say
that we are 'building' a codebase. We assign people the role of 'architect'. One
camp of people regale <a href="http://www.laputan.org/mud/">The Big Ball of Mud</a>,
esteeming the progress they make as their rationale for constructing a wobblying
shantytown. Another camp sit high in <a href="https://blog.codinghorror.com/ivory-tower-development/">The Ivory
Tower</a>, planning the
sanctuaries that may give their inhabitants stamina as they rest in its glory.</p>
<p>In Architecture of Happiness, Alain de Boton waxes,</p>
<blockquote>
<p>Beneath the pleasure generated by the juxtaposition of order and complexity,
we can identify the subsidiary architectural virtue of <em>balance</em>. Beauty is a
likely outcome whenever architects skillfully mediate between any number of
oppositions, including the old and the new, the natural and the man-made, the
luxurious and the modest, and the masculine and the feminine.</p>
</blockquote>
<p>What analogy sits between these two extremes? Where is the balance between
progress and stamina?</p>
<p>I feel like it's building a home.</p>
<p>Recently, I watched <a href="https://www.youtube.com/watch?v=AxM9FYSs8V4">a man building his own log
cabin</a>, slowly outfitting large
portions while also doing his chores from day to day. You can tell the focus
placed on making it a lovely, warm place people can relax but also the
willingness to accept the mess where it need to be accepted. As Jonathan Blow
eloquently puts it in a <a href="https://www.youtube.com/watch?v=6XAu4EPQRmY">portion of one of his live
streams</a>,</p>
<blockquote>
<p>We are ignoring [a pointer problem] for now and we are making a note that that
problem needs to be solved. You don't [sic] so here's the thing, in a big
project you just don't have to solve every problem at once in fact if you try
you will not get very far at all. You'll just get crushed under the load under
all the things you have to do and of never getting anything done ...</p>
</blockquote>
<p>Jonathan goes on to state how he subdivides problems into ones he wants to
seriously tackle now, and ones where he is putting in a rudimentary solution so
long as it gains him progress. All of this, to me, is the same sort of balance
that comes in the mental idea of having one's own house.</p>
<p>Most likely several things are demanding ones attention for fixing; you might
need to get a wall repaired, perhaps a door is more important since it's
front-facing, and you need to rake out front and put out the trash. Despite all
this effort, you spend many relaxing moments in your home, recharging and
growing with the environment built up around you. Alain de Boton feels homes
represent our ideal lifestyles, and experienced programmers do the very same on
their codebases; they make it home not only for themselves but for others.</p>
<p>Maybe we all ought to make homes so we can grow and feel happy in them, without
sacrificing progress and stamina.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Custom Search Functionality for Coding</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/custom_search_functionality_for_coding.html</link>
      <guid>https://justanotherdot.com/posts/custom_search_functionality_for_coding.html</guid>
      <pubDate>Mon, 19 Aug 2019 16:10:36 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This may not be revelatory to some, but it's a cool trick I use daily and I
thought I'd write about since it's managed to surprise enough colleagues and
friends when I've used it. Credit where credit is due, I was taught this trick
two years ago by Charles O'Farrell.</p>
<p>Firefox and Chrome both support this functionality but are setup differently.
Let's say you have a github codebase with a particular org (which are also,
confusingly, demarcated as users in github search). You want to find a repo
quickly; you can quickly go to your search bar and hit <code>repos a_project</code> (or in
the case of Chrome, <code>repos&lt;tab&gt; a_project</code>), hammer the enter key and you wind
up at <code>https://github.com/search?q=user%3Aorg+a_project</code>. How?</p>
<p>In both Chrome and Firefox, you can add a custom search engine by right-clicking
on the search 'bar' (form) you'd like to add, except in Firefox the mechanism
works via bookmarks and Chrome has the functionality as it's own thing (seems
like a first class citizen). There <em>are</em> custom search engines for Firefox you
can add (I've noticed I can add them for things like crates.io, docs.rs, amazon,
et. al.) out of the box given a specific version (I'm not sure which) of Firefox
but you'll still need to bookmark approach for most cases. Once you have the
search URL you care about just replace the term you searched for with <code>%s</code> and
all is well, e.g. <code>https://github.com/search?q=user%3Aorg+%s</code>.</p>
<p>Some other examples of uses are:</p>
<ul>
<li><code>code term</code> - similar to the <code>repos</code> keyword above but searches across all
repositories of an org for <code>term</code></li>
<li><code>(docs.rs|crates.rs|younameit) &lt;module&gt;</code> - looks for module documentation,
listing in some large store of knowledge</li>
<li><code>rstd term</code> - search for <code>term</code> in the rust std lib (handy when paired with
something like rust-tags so you can jump to definition inside of std in your
editor)</li>
</ul>
<p>The above are rust centric because it's what I've been in the headspace of but
you could easily set this up for things like hoogle, amazon, shortening things
like youtube to <code>y</code>, <code>hex.pm</code>, and so on. Personally, it's been empowering to
gain a handle on parameterizing search functionality with your address bar.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Review for 2018</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/reading_review_2018.html</link>
      <guid>https://justanotherdot.com/posts/reading_review_2018.html</guid>
      <pubDate>Wed, 12 Dec 2018 20:09:20 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Being a voracious reader, one thing that helps bring the mountain of things I
want to read down to something manageable is being able to read things faster.
Reading <em>faster</em> itself probably hurts comprehension and digestion of core
concepts, but getting digested content from others who have already read the
main body of work can drastically reduce the amount of fluff you'll have to wade
through yourself. If you've ever read a book review that basically told you
everything you needed to know before you've read the book, you'll know what I'm
talking about.</p>
<p>I've personally felt this way a lot towards reviews and have found it
invaluable. As such, I feel it's necessary for me to pay this back. It's not
worthwhile giving rundowns for <em>everything</em> I've read so the purpose of this
article is to focus on the things I've found fascinating. This is by no means a
comprehensive list and you'll probably find the details per body of work a tad
thin as time has waned on since I've read them. However, I do wish to make this
more of a regular habit and, in the end, hopefully I can at least express the
delight I've had getting through some of these wonderful wading pools of words.</p>
<h2><a href="https://www.amazon.com/Coders-Work-Reflections-Craft-Programming/dp/1430219483/ref=sr_1_1?ie=UTF8&amp;qid=1544954623&amp;sr=8-1&amp;keywords=coders+at+work">Coders at Work</a></h2>
<p>This had so many fantastic insights into programming in the large as well as the
small for some of my heroes as well as some people I had never heard of before.
Of particular note was the recurring question Peter Seibel kept bringing up:
&quot;How do you read code?&quot;. In fact, Seibel even did a follow-up blog post on the
subject, but my particular take was the specifics that people actually replied
with. In fact, the subject matter drove me to write an article about reading
code I dubbed <a href="https://justanotherdot.com/posts/Reading_Code_is_Decoding.html">'Reading Code is
Decoding'</a></p>
<p>One other thing I thought fascinating was that one of the leaders of the Haskell
language, Simon Peyton Jones, had feelings towards types as proof systems that
were less intense as I had originally assumed. In fact, SPJ repeatedly mentions
that the idea is about <em>confidence</em> and not about mathematical fact.</p>
<p>Lastly, guys like Brad Fitzpatrick and Jamie Zawinski 'got things done', but
they also had a lot of sensibility towards quality instead of simply being
<a href="https://www.joelonsoftware.com/2009/09/23/the-duct-tape-programmer/">'duct tape
programmers'.</a>
In my mind, the book does a phenomenal job of showing how several people can
think of simplicity from different angles, whether that's avoiding the barbarism
of C++ in the face of Netscape's pre-existing C codebase, avoiding unnecessary
enterprise software in exchange for possibly less-than-ideal open-source
solutions to keep Live Journal up and running, or sussing out obtuse assembly
language for an unknown system by mathematically dissecting hunks of code.</p>
<h2><a href="https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth-ebook/dp/B06XP3GJ7F/ref=sr_1_2?ie=UTF8&amp;qid=1544954644&amp;sr=8-2&amp;keywords=the+manager%27s+path">The Manager's Path</a></h2>
<p>Soft skills are hard, and we'll see in a few other reviews that they are even
harder to write about and absolutely harder to put into practice properly! But
that still doesn't mean people can't try. Things of note:</p>
<ul>
<li>
<p>You will sometimes need to do hard things and tell people hard news, but it's
better you do that than try to pretend everything is amazing. In other words,
It's better to be 'kind' (honest but considerate) than simply 'nice' (always
accommodating)</p>
</li>
<li>
<p>As you may find yourself with more managerial duties, you're ability to bridge
the gap between technical and non-technical members of the business increases;
this can be with metrics and even how you handle explaining or using
alternative jargon</p>
</li>
<li>
<p>A healthy codebase is an active one, but the rate of change needs to be kept
in check with the rate of errors. There is a subtle hint here about developing
a devops culture so that people are aware of how they can best help ship
software and know if they are within a predefined error budget. If you impede
the error budget, it's time to start focusing more on fixing things, which
means having flexible scheduling percentages.</p>
</li>
<li>
<p>Learning is the most vital aspect of a business and orienting it's processes
around it is pivotal in it's success</p>
</li>
<li>
<p>Having technical chops before you wind up in any managerial capacity is
crucial but it's also important to know that things like leadership and
empathy takes just as much effort to refine and perfect</p>
</li>
</ul>
<p>I feel like a good follow up to this was reading John Allspaw's article <a href="https://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/"><em>On Being a Senior Engineer</em></a>.
In it, Allspaw hits on several soft skills that 'senior' software engineers
absolutely must uphold on a day to day basis, and I really don't see them being
that different from soft skills that others should be upholding as well as they
increase in seniority.</p>
<h2><a href="http://mcfunley.com/choose-boring-technology">Choose Boring Tech</a>, <a href="https://www.intercom.com/blog/run-less-software/">Run Less Software</a>, and [You Need a Novelty</h2>
<p>Budget](https://www.shimweasel.com/2018/08/25/novelty-budgets)</p>
<p>I mention these three articles in tandem because they were thoughts I had
faintly felt being in various organisations but could not pinpoint with words
quite as well as these three articles do. The core ideas are:</p>
<ul>
<li>
<p>You don't always need to map every problem to an ideal solution, in fact,
doing so is problematic because you will wind up with too much tech to
maintain, and maintenance cost must weight in considering adoption</p>
</li>
<li>
<p>Every new piece of tech you acquire into your stack means more things people
need to know about and the more people need to know means the less they can be
experts.</p>
</li>
<li>
<p>Choosing or building swanky libraries, technology, and services may be
exhilarating but it's also problematic for the above reasons. Having a
'novelty budget' can help prevent 'fancy tech creep' into the codebase and
infrastructure. In fact, it may help to run as many crazy projects 'on the
side' (i.e. outside of work) as possible to help reduce the urge of
introducing shiny-new-things</p>
</li>
</ul>
<p>I kept going back to these articles at least every month or two; they serve as a
basis of what I think is productive coding: write boring code that <a href="https://www.youtube.com/watch?v=4Y0tOi7QWqM">fits in your
head</a>, is heavily tested, focuses
on some clear specification (the proper abstraction), and satisfies &quot;small is
beautiful/less is more&quot;. I can't imagine that being the end of the proper
characteristics and that list keeps morphing as I keep coding, but it seems
sensible enough to mention those characteristics in relation to these posts.</p>
<h2><a href="https://www.amazon.com/Twelve-Steps-Compassionate-Karen-Armstrong-ebook/dp/B003WUYPBA/ref=sr_1_1?ie=UTF8&amp;qid=1544954743&amp;sr=8-1&amp;keywords=12+steps+to+a+compassionate+life">12 Steps to a Compassionate Life</a></h2>
<p>For trust to cultivate between members working together to build impossible
programs and systems you need respect and you can't build respect until you have
empathy for others. You don't need to feel the same thing they feel, per se, but
compassion itself is, in my mind and shaped from the sentiments in the book, the
difficult but extremely rewarding practice of cultivating concern for others,
their suffering and feelings.</p>
<p>In the book, Armstrong equips us with a particular process of better developing
empathy for others via religious, sociological, and psychological references.
Since this is a heavy process-oriented book, I can't simply give a quick tl;dr,
but I do want to recommend this book heavily. It and <a href="https://www.amazon.com/How-Talk-Kids-Will-Listen-ebook/dp/B005GG0MXI/ref=sr_1_4?ie=UTF8&amp;qid=1544953942&amp;sr=8-4&amp;keywords=how+to+talk">How to Talk so Kids Will
Listen and Listen so Kids Will
Talk</a>
are both fantastic resources on this subject, although I've read the latter a
number of years before this post.</p>
<h2><a href="https://blog.acolyer.org/2018/08/20/filter-before-you-parse-faster-analytics-on-raw-data-with-sparser/">Filter Before you Parse</a></h2>
<p>The Morning Paper is probably <em>the</em> best resource for mind blowing information
on the internet for software engineers, and I could ramble off many articles
that absolutely twisted my brain this year. If you don't have a subscription to
the The Morning Paper, you need to go subscribe this instance! Despite it not
being the most mind-bending of articles this year, I wanted to point out this
particular piece and associated paper because it's such a &quot;oh <strong>of course</strong>&quot;
kind of moment.</p>
<p>The gist? Do a fast search on the JSON payload <em>before</em> transforming it into
it's in-memory representation.</p>
<h2><a href="https://lemire.me/blog/2018/05/03/how-fast-can-you-parse-json/">How Fast Can You Parse JSON?</a></h2>
<p>Daniel Lemire is a performance and database nut (and so I'm easily a fan). I
wanted to include this since I had also included the JSON parsing related paper
from The Morning Paper. In this short article, Daniel explores a few industrial
grade parser implementations and tries to get at how many cycles it would take
to parse per byte, eventually coming to the closing line of:</p>
<blockquote>
<p>So you should expect to spend 2 or 3 seconds parsing one gigabyte of JSON data.</p>
</blockquote>
<p>I am personally a fan of &quot;Latency Numbers Every Programmer Should Know&quot; and I
think things like this are actually handy, back pocket facts that ease
estimation, s.t. if you find you're parsing a gigabyte of JSON and it's taking
orders of magnitude more than this, you can probably know there are gains to be
made, and also know that there might be a happy limit for optimisation.</p>
<h2><a href="https://charity.wtf/2018/08/19/shipping-software-should-not-be-scary/">Shipping Software Shouldn't be Scary</a></h2>
<p>I have a number female software engineer role models; Julia Evans, Jessie
Frazelle, and Charity Majors, to name a few. Charity's kick is on empowering
devs to be both coding and operations gurus by empowering them with the
superpower of observability. This article by Charity also goes into some core
things about operational concerns that are &quot;everyone's problems&quot;. The hot take
is that devs should be able to take code from cradle to grave; ideation into
production which entails all the messy debugging and reversals after Things Go
Wrong. The aim is to make everyone a software owner rather than a mere
occasional participant.</p>
<p>It may also get the reward for the best work memes of 2018.</p>
<h2><a href="https://gist.github.com/bcantrill/835837a66bcc21b899f501dd794a7d5f">Bryan Cantril's 'internal' doc on hiring</a></h2>
<p>Bryan sums this up nicely in the beginning, I'll add my takeaways for each bit.</p>
<ul>
<li>
<p>Aptitude: can the person actually code?</p>
</li>
<li>
<p>Education: have they persevered through boring lectures and stress?</p>
</li>
<li>
<p>Motivation: are you driven to write code everyday or is it Just Another Job?</p>
</li>
<li>
<p>Values, per the doc itself:</p>
</li>
</ul>
<blockquote>
<p>One observation is that one's values -- and the adherence or divergence from
those values -- will often be reflected in happiness and satisfaction with
work. When work strongly reflects one's values, one is much more likely to
find it satisfying; when values are compromised (even if for a good reason),
work is likely be unsatisfying.</p>
</blockquote>
<ul>
<li>Integrity: The usual verification that people are who they say they are and
not actually crazy, serial axe murders or simply bullshit artists.</li>
</ul>
<h2><a href="https://lethain.com/migrations/">Migrations: the sole scalable fix to tech debt</a></h2>
<p>You have legacy technology and you wish it would just go away, but how are you
going to euthanize it? You could write a new system and hide the other one in
the corner, perhaps? But you end up realising that your business relies quite
heavily to this chunk of code and that it's actually grown into such a mess you
can't fathom now moving away from it entirely and now need to run two things;
this is a fairly common problem and I think the gist of this article was spot
on: if you want to deprecate A, you need to be focused and clear about how B is
going to entirely replace it's functionality to the point where it can be
politely deleted out of existence, and you need to make this a common practice
as an engineering team as every piece of code you write is legacy the moment you
write it. Code rots, and, as such, that shiny new piece of tech you've just
built will one day need a successor.</p>
<p>In general the process is the same for any piece of software: do upfront
planning with documentation and good old fashioned thinking, get people on
board, don't be the machine and automate as much of the migration as possible,
help <em>track</em> the actual shift away from the older platform, and finally, don't
forget to celebrate your achievements! That last one is huge because if you
don't make migrations a big deal, people won't see the value in it!</p>
<h2><a href="https://www.amazon.com/Writing-Without-Bullshit-Career-Saying-ebook/dp/B01A5CEKQM/ref=sr_1_1?ie=UTF8&amp;qid=1544955550&amp;sr=8-1&amp;keywords=writing+without+bullshit">Writing Without Bullshit</a></h2>
<p>I'm actually quite surprised that this book devolved into a description of a
particular process, but I think it's a worthy ally in the fight to cut the fat
out of one's prose (the emphasis is in a workplace setting). Two major things of
note I took from this book were:</p>
<ul>
<li>
<p>Be short and to the point but scrutinize your use of data.</p>
</li>
<li>
<p>It's incredibly useful to get out 'fat outlines' of work before you start
obsessing on details and get lost in the subsequent worrying. I write like
this now in general: large bulk of content upfront and several passes of
refinement afterwards.</p>
</li>
<li>
<p>The 'iron imperative': &quot;Don't waste the readers time&quot;</p>
</li>
</ul>
<p>That last point kept having me think about the book <a href="https://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/ref=sr_1_1?ie=UTF8&amp;qid=1544955670&amp;sr=8-1&amp;keywords=don%27t+make+me+think">Don't Make
Me</a>
Think where the crux of design should be about alleviating cognitive load on the
user of the interface. It's a powerful phrase to have handy when needing to
point out really lengthy bits of code, comments, or documentation, as well as
general communiques.</p>
<h2><a href="https://www.amazon.com/Messy-Power-Disorder-Transform-Lives-ebook/dp/B01BD1SU2E/ref=sr_1_1?ie=UTF8&amp;qid=1544955751&amp;sr=8-1&amp;keywords=messy+the+power">Messy: The Power of Disorder to Transform Our Lives</a></h2>
<p>This book was monumental in getting me comfortable with the idea of doing rough
groundwork upfront. In fact, some people describe this as the <a href="https://en.wikipedia.org/wiki/Pareto_principle">'paeto
principle'</a> where 'eighty
percent of the work comes from twenty percent of the effort'. In my mind I'm
sure Pareto never intended this to be about a specific ratio but rather about
how a small portion of input winds up relating to a large portion of the output.
I actually found no better succinct description of the book except at the start
(the rest is still worth the read):</p>
<blockquote>
<p>The argument of this book is that we often succumb to the temptation of a
tidy-minded approach when we would be better served by embracing a degree of
mess.</p>
</blockquote>
<h2><a href="https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&amp;qid=1544955863&amp;sr=8-1&amp;keywords=programming+rust">Programming Rust</a></h2>
<p>I've not really read a specific programming-language book in awhile, but I love
systems programming and Programming rust (along with several other rust books)
focus heavily on the subject; whether it's talking about error handling, rough
edges with Unicode, or even how different languages handle assignment in order
to explain the borrow checker and moves, I was constantly enthralled to pick
this book up despite it being close to six-hundred pages. It's gained the
classic 'dirty and crushed pages' aesthetic and it'll probably gain a bit more
as I keep trying to double down on rust.</p>
<h2><a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?ie=UTF8&amp;qid=1544955948&amp;sr=8-1&amp;keywords=designing+data-intensive+applications">Designing Data-Intensive Applications</a></h2>
<p>I can't recommend this book enough. I actually finished reading this near the
end of 2017, but I love it so much that I wanted to write about it here. I
personally don't know of any other book that handles the fundamentals of
database internals (along with many modern improvements), database design,
clusters, nodes, schema encodings, discussions around guarantees (CAP, ACID) and
what are valid and invalid points, consensus in distributed systems, and even
modern data processing patterns all while in the context of proposing a credo
that data-intensive applications should uphold, in such an elegant and
ultimately fun way. I still read through the book for knowledge I may have
passed up or forgotten so it's a fantastic resource to have on my shelf.</p>
<p>It's also dawned on me that data processing <em>is</em> programming, and knowing more
and more about data and how to handle it is enlightening and empowering.</p>
<h2><a href="https://www.amazon.com/Thinking-Forth-Leo-Brodie/dp/0976458705/ref=sr_1_1?ie=UTF8&amp;qid=1544956045&amp;sr=8-1&amp;keywords=thinking+forth">Thinking Forth</a></h2>
<p><a href="https://twitter.com/lorentzframe/status/997997523301117953">Brian Beckman wrote a tweet I read awhile
back</a>, stating:</p>
<blockquote>
<p>&quot;SICP&quot; by Abelson &amp; Sussman should be read continuously, ~2 pages a day,
returning to page 1 every year. Ditto &quot;Thinking Forth&quot; by Leo Brodie, tho'
only ~1 page a day. The former teaches how to think, the latter how to
engineer. Both are in unpopular languages, on purpose.</p>
</blockquote>
<p>and having read most of the SICP, even watching the entire lecture series, but
never having heard of <em>Thinking Forth</em> (but having heard of the language), I was
intrigued to hear of a book put on the same page as the SICP! I am actually,
also, cheating here as I've read only half of it, but so far it's definitely a
different way of looking at software construction in the same vein that the SICP
went to lengths to describe.</p>
<p>The heaviest emphasis made so far in the book that I love seems drawn between
Domain Driven Design and traditional functional programming: words are vital to
how we code, the meaning they carry as well as their surrounding semantics; we
group words into lexicons which we can call 'components', and with 'components'
we get all the general benefits we get from composition. There is some forth
specific information nestled between, but <a href="https://twitter.com/lorentzframe/status/999121431559487489">Beckman is wise in a followup
tweet</a>, saying:</p>
<blockquote>
<p>If SICP were in, say, (popular) Python instead of (unpopular) Scheme, people
might be distracted by the Python and miss the thinking. Ditto if &quot;Thinking
Forth&quot; were in (popular) Java instead of (unpopular) Forth, people might miss
the deeper points about software engineering.</p>
</blockquote>
<p>I'm excited to finish it in January, in tandem with <em>Thinking in Systems: A
Primer</em>, which I've started just last week and has so far been an amazing way of
perceiving the structure of various systems from a proper, abstracted point of
view.</p>
<h2>Conclusion</h2>
<p>I originally thought a format involving simple review styled blurbs might be
handy, but it's kind of nice having not thought about some of these various
texts for a bit to really see what would stick. I hope this has been somewhat
useful or fun!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>What makes a good pull request?</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/what_makes_a_good_pr.html</link>
      <guid>https://justanotherdot.com/posts/what_makes_a_good_pr.html</guid>
      <pubDate>Fri, 20 Jul 2018 20:40:08 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Pull Requests (or PRs) are a tango between two parties; the code author and the
code reviewer which I will simply refer to as the 'author' and 'reviewer' in the
remainder of this article. In a pull request, the author has provided code to
solve a particular problem and the reviewer is there to provide a feedback
mechanism to the author.</p>
<h2>Code review is not a gate keeping task</h2>
<p>I once worked for an organisation whose code review process centered around a
total lack of faith in its developers ability to deliver quality product; tech
leads acted as the gate keepers to their respective stacks forcing the average
developer to resort to underhanded tactics in order to get their changes
merged, regardless of quality or prospective bugs. This, in turn, meant the
gate keepers felt justified under the guise of 'keeping things safe'. Thus,
code wasn't being checked properly and wrong or flimsy changes would trickle
into master, making the code review process utterly broken.</p>
<p>As a software engineer, every line of code you ship is code you, or someone
else, will need to maintain, and as such, you should be fighting to deliver the
best quality you can offer, regardless of deadline. <strong>Code reviewers are there
to help people bring their code into the light of day where asking questions is
the chief tool a reviewer employs</strong>. This can include, but is not limited to,
probing to see if:</p>
<ul>
<li>Is the thought fully fleshed out?</li>
<li>Is this implementation correct for the problem it aims to solve?</li>
<li>Are the changes sound and principled?</li>
<li>Are there any performance or semantic concerns?</li>
</ul>
<p>I'm purposefully leaving out stylistic choices here as the discussion often
leads to the argument around adoption of some automated code-formatting tool.</p>
<h2>Raise early and raise often</h2>
<p>In &quot;Debugging Team's&quot;, the authors kick off the start of the book with a simple
analogy between two competing inventors. The one inventor does not want to share
his ideas for fear of them being poached by others, while the rival inventor
gleefully goes to local places where experts might hang out to get more
information about how to build her inventions.</p>
<p>Software development is no different in that knowing things earlier is always
better than knowing things later. Raising PRs even before they are 'complete'
(and appropriately marking them as WIPs or 'works in progress') allows people to
possibly, time permitting, look into your changes and see if there are any major
red flags.</p>
<p>That said, if you are a reviewer and are asked (or not!) to look at a set of
changes that is marked as a WIP, try to hold off on a more in-depth review until
the author changes this status. And to PR raisers, don't keep things in WIP
stage for too long, which brings me to my next point.</p>
<h2>PRs are for small chunks of code to merge often</h2>
<p>A PR should represent up to a day's worth of work. This is beneficial to both
parties in that it facilitates a 'merge often' approach for devs (and devs get
the little adrenaline kick from clicking that green <code>merge</code> button) and
reviewers can much more easily review a smaller hunk of changes. A reviewer
reviewing five PRs in the course of a week has to spend less time grokking
those individual changes than to review five days worth of work in a single PR.</p>
<p>Massive projects poised around scaled tooling and reviews such as the Linux
kernel would flat out reject a patch with, say, 3k additions and 1k removals. If
a project of that size and calibre, and that many international hands involved,
is marking code review of those dimensions as 'unmanageable', what hope does a
startup have at making fast, rapid changes in the same light?</p>
<p>Small PRs are also <em>focused</em> on a clear intent. Asking the author to fix
neighboring code 'just because' or refactoring/formatting several adjacent files
that are not directly tied to the immediate effort of the PR wastes the both
parties time. Opening a PR to refactor changes and another PR to add new
functionality is a much better way to get appropriate attention from reviewers.
As the joke goes:</p>
<blockquote>
<p>10 lines = 10 possible bugs, 100 lines = lgtm</p>
</blockquote>
<p>It's important to remember that a PR is not to encompass a single ticket or
issue. Tickets can have several PRs attached to them and all it takes is
lobbing <code>[FOO-123]</code> on top of one's PR title for Jira or marking <code>#&lt;issue number&gt;</code> in your description in GitHub. I like to call this act 'linking' and
it's useful for stakeholders to track down all the changes that have fed into a
particular ticket.</p>
<h2>Context matters</h2>
<p>Reviewers need to discuss with the author about the purpose of a set of changes
and how close or far off they are from that goal, but if the reviewer is
unclear about this goal, it's difficult for them to strike up a discussion with
the author.</p>
<p>In the context of OSS, raising a patch directly to a project such as the Linux
kernel is poor practice. If you want to make a change in any capacity it's best
first to contact the people who own the code on public channels. This provides
auditing and clear context for others. <strong>Your first instinct should be to raise
a PR unless it's a feature. If you feel uncertain about whether or not your
change is warranted, it's best to raise an issue first, instead.</strong></p>
<p>That said, raising PRs should feel natural; PRs are cheap and can be closed and
their branches pruned as need be, but regardless of the cost of raising a PR,
it's critical to include appropriate information. Some important things to
mention may be:</p>
<ul>
<li>What does this set of changes solve?</li>
<li>Is there a specific task (issue/ticket) that this relates to?</li>
<li>Is this blocked or blocking any other PRs/issues/tickets?</li>
<li>Is there any additional information that will help the reviewer know about my
manual testing of this ticket (screenshots, output from tooling, et. al.)?</li>
<li>Have you updated tests and documentation accordingly? Have you added tests
that the reviewer can skip to first to immediately see how you're proposed
changes are supposed to work and in what cases?</li>
<li>Is there current behaviour to contrast the new behaviour to?</li>
<li>Are there breaking changes present?</li>
</ul>
<p>The traditional approach for this was to include commentary in your actual
commit messages and headers. I don't think times have really changed in this
regard and the more context you sprinkle about the better, so long as you are
clear about your intent and you don't waste the reader's time.</p>
<h2>Check out changes locally when it makes sense</h2>
<p>A really healthy habit for reasonably sized changes is to always check out a PR
and see if it works for you. Some projects have powerful processes for testing
full 'e2e/raw hardware' scenarios such as Intel's <a href="https://01.org/lkp/documentation/0-day-test-service">zero-day testing
bot</a> which actually boots
up machines to test out differing version of the Linux kernel. While continuous
integration can catch a lot, it's important to sometimes get a human eye for
regressions that may not, or cannot, be encoded in automated tests. We all make
mistakes, and reviewers are there to add a layer of sanity checks to our
changes.</p>
<p>Use common sense. If a change is pretty sensible (e.g. a single line change to
update a variable name), you probably don't need to spend the time pulling the
change down, compiling, running the tests, and so forth. As the developer's
mantra goes, &quot;Don't be the machine&quot;!</p>
<h2>Clean up your mess</h2>
<p>Your changes have been merged and you can go on with your life, but before you
reach for beverage of choice, you should prune your dead branches. I actually
have a git bash script for this that I place in my <code>PATH</code> so I can call it as
<code>git wash</code>. The script is <a href="https://gist.github.com/justanotherdot/3e3a16df805d09a37e1c26bbedd23fcc">here</a>. When
run without arguments, this will delete the current branch you are on locally
and remotely so long as the branch specified to <code>git wash</code> is not master and the
remote branch is not protected. If you need other git functionality like this,
any script in your path with the name <code>git-&lt;thing&gt;</code> can be run as <code>git &lt;thing&gt;</code>.</p>
<h2>Conclusion</h2>
<p>Reviewing many small changes is much more manageable for a reviewer than
reviewing large, tangled changes. Decomposition in programming gives us the
ability to stitch together many small, verified solutions that lead up to an
equally trustworthy result and the same is no different in the process of
supplying code changes to a project; breaking up the changes you need to make
into reasonable chunks that can fit in everyone's heads not only helps to
provide code changes faster but it also paves foundations for robust and
resilient code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Trampling Trampolines</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/trampling_trampolines.html</link>
      <guid>https://justanotherdot.com/posts/trampling_trampolines.html</guid>
      <pubDate>Thu, 24 May 2018 19:40:17 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <h2>Continuation Passing Style</h2>
<p>or CPS for short, is a way to ‘continue’ a function call by calling into another function. The simplest example would be:</p>
<pre><code>function cps(x, return) {
  return(x);
}
</code></pre>
<p>The important thing is the callback is passed and it is called at the ‘completion’ of the function, passing things along.</p>
<p>CPS is not a problem in languages where Last Call Optimisation is commonplace. What these languages do (normally of eager evaluation) is collapse the stack frame and call the function call so long as there is nothing else beyond the function. <a href="http://erlang.org/pipermail/erlang-questions/2016-October/090663.html">Here’s a fun rant by Joe Armstrong regarding the implementation and implication of Tail/Last Call Optimisation</a>. Of note is how he is careful not to say this is the same as Tail Call Optimisation, or TCO, as that typically implies a recursive call.</p>
<h2>DIY</h2>
<p>Languages that don’t collapse the stack or represent functions in a form that is conducive to fusion are subject to ‘blowing the stack’ (exceeding it’s maximum size) when writing a recursive function. The function may be correct, but for particular values it may grow too large and too many stack frames will be pushed onto the program’s stack.</p>
<p>There are some <a href="http://chrispenner.ca/posts/python-tail-recursion">interesting ways to mimic TCO</a> in languages that don’t have native support for it. JavaScript does, technically, but implementation seems spotty across engines.  A clever way to mimic TCO in JavaScript is to rewrite your function to return a continuation, of sorts, and then wrapping it in another, generic, function (the trampoline) that knows about this arrangement in order to collapse the stack itself, for example:</p>
<pre><code>let ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`ERROR: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>I’ve chosen the <a href="https://en.wikipedia.org/wiki/Ackermann_function">Ackermann function</a> here as it’s a golden standard for testing recursive functionality in programming languages since its value grows rapidly even for small digits. I’m also using the convention of naming things as <code>go</code> to specify recursive helper functions with arguments we don’t care about. A common counterpart to <code>go</code> you might see (particularly popular in LISPs) is <code>do</code> .</p>
<p>If you play around with this function you should quickly find it exceeding the allotted call stack size. Let’s fix this with a trampoline:</p>
<pre><code>let trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (typeof rv === 'function') {
    rv = rv();
  }
  return rv;
};

ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return () =&gt; ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return () =&gt; ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`urk: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>Note the zero-arity functions we are returning in order to signal that the return value can be applied (specifically lines 13 and 15). What kind of functions will <code>trampoline</code> fail on given it’s current implementation? When you’ve given it some thought, consider this case:</p>
<pre><code>let higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return offset =&gt; acc+offset;
  }
  return () =&gt; higherOrderFunc(n-1, acc+n, offset);
}

let hof = trampoline(higherOrderFunc);
</code></pre>
<p>Although this will terminate, it won’t give us the correct result. We want a closure in the end, but here we will wind up with <code>NaN</code> since that is the result of adding any number in JavaScript to <code>undefined</code>, and since we are calling the result value, <code>rv</code>, with no arguments, we are technically passing <code>undefined</code> to this final value.</p>
<p>A <em>structural</em> type system is one which cares only about the form of given values, whereas a <em>nominal</em> type system cares about the <em>names</em> that values have in the system. In a language like Haskell, we’d use what’s known as a “data constructor” to declare a type. Consider:</p>
<pre><code>data Cont a = Stop a | Cont (() -&gt; Cont a)
</code></pre>
<p>This says “for any given value of <code>a</code>, I can either be a <code>Stop</code> value wrapping some given value of <code>a</code>, or I can be a <code>Cont</code> wrapping a function which takes <code>unit</code> (the <code>()</code>, or, in other words, no arguments that matters) to another <code>Cont</code> value”</p>
<p>We can, again, mimic something similar using a <code>tag</code> field on an object, a la:</p>
<pre><code>higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return {
      tag: 'stop',
      val: offset =&gt; acc+offset
    };
  }
  return {
    tag: 'cont',
    val: () =&gt; higherOrderFunc(n-1, acc+n, offset)
  };
};
</code></pre>
<p>Which we can leverage in a new definition of <code>trampoline</code>:</p>
<pre><code>trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (rv.tag === 'cont') {
    rv = rv.val();
  }
  return rv.val; // Of tag 'stop'.
};
</code></pre>
<p>We only have two cases for our ‘type’, so when we no longer have a <code>cont</code> tag, we must have a <code>stop</code>, and with this we get the correct result: a function!</p>
<h2>Conclusion</h2>
<p>In mathematics it is just as important to reason about the types of objects we are dealing with as it is to reason about their values. The same is no different in programming. Even though you may say “I hack in a dynamically typed language, I don’t need to think about types”, the inverse is actually the truth! Hacking in a dynamically typed environment means juggling these notions around in your head rather than allowing the type checker make sense of the form of things for you.</p>
<p><em>food for thought:</em> What we’ve done by tagging these values is upgrade a <code>union</code> type into a <code>discriminated union</code>. A <code>discriminated unions</code> is also sometimes known as a <code>sum</code> type. The power of passing around sum types is that we can reason about the cases in our code: recursion itself is similar to mathematical induction, and both are forms of breaking down data whereas their duals, co-recursion and co-induction, build up data. This is an important notion because it means that when we write things recursively in this form with inductive-like types (e.g. sums) we can ‘pattern match’ on their values and know something about each case. In the above examples we knew that <code>cont</code> always contained a function that took no arguments and returned another <code>cont</code> or <code>stop</code> tagged object. When we finally got a <code>stop</code> we knew we had our final result we could return, and since there were only two values we could be sure that those were the only two cases worth exploring (show exhaustivity checking as well as case analysis). As is common with FP, some languages give you the power of actually statically checking this; ensuring that you’ve considered all cases, whereas in others you’ll be left to discipline or libraries to replicate this, just like trampolines.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Proficiency is Tiered and other Lies We Tell Ourselves</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/proficiency_is_tiered_and_other_lies_we_tell_ourselves.html</link>
      <guid>https://justanotherdot.com/posts/proficiency_is_tiered_and_other_lies_we_tell_ourselves.html</guid>
      <pubDate>Thu, 01 Mar 2018 15:48:02 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Tiered categorisations of knowledge and proficiency are fundamentally flawed as
they rest on the notion that all knowledge can eventually be obtained,
retained, and divvied up amongst n-many categories. Such categorisations also
ignore the fact that most skills rely on overlapping knowledge from various
domains. In this article I propose a way to evaluate subject matter in the
context of best prioritising <em>what should I learn next?</em> I’ll also offer up an
approach to evaluating others that isn’t based on ‘skill level’ (admittedly
regurgitated from Amy Cuddy).</p>
<h2>A Tagging System</h2>
<p>Instead of suggesting that knowledge from a domain of expertise can be chunked
and tagged in toto, we consider an alternative tagging system where we look at
knowledge from three types of labels and, most importantly, accept that the
fringes are fuzzy:</p>
<ul>
<li>Fundamentals
<ul>
<li>There is usually a corpus of knowledge that everyone can agree upon is
pivotal to ‘being competent/dangerous’ in a particular subject matter.
These may overlap to other skills and it may be unclear <em>which</em> skills they
overlap in, but what matters is that these skills are relatively obvious in
the domain of note.</li>
</ul>
</li>
<li>Nice-to-Haves
<ul>
<li>This is the knowledge that might be good to spend a bit of time on as it
refines and builds on fundamentals to introduce more powerful techniques
and practices. This is where the only clarity is that they are definitely
not fundamentals and they are definitely not esoteric.</li>
</ul>
</li>
<li>Trivia
<ul>
<li>This is the stuff you probably don’t need to know, like that some AIX
machines have a weird bug in certain prompts where inputting uppercase
characters will cause the machine to reboot or that earlier, alternative
architectures supported 7-bit bytes. These tidbits of information
(sometimes not so miniscule!) are probably very costly to pick up and don’t
give you much in return.</li>
</ul>
</li>
</ul>
<p>These tags map very much to the progression of learning a subject: when you
start learning a subject, everything is rough and unclear; you should focus on
exposing yourself to as much as knowledge as possible even if you don’t quite
understand everything. This ’5yo’ view of the world helps build the framework
wherein we can fill in further details as we step towards the nice-to-haves,
but instead of becoming an ‘expert’ by picking up trivia, we try to avoid it,
and if it were important, then it would fall back into the nice-to-haves. This
is the important caveat to learning anything in general I’m trying to make
here; mastering a subject has nothing to do with knowing absolutely everything
there is to know.</p>
<p>The practice of using these tags is simple: whenever you’re faced with a
variety of options, pick fundamentals over nice-to-haves, nice-to-haves over
trivia, and (per that last regard) try to pick more fundamentals and
nice-to-haves in a variety of subject matter than trying to pick up a
collection of specific trivia for a single subject matter.</p>
<p>I liken this to the Pareto principle, which effectively states that input
effort is usually disproportionate to output gains, or, as the common quote
goes, “20% of the effort for 80% of the output” (although it’s perfectly
feasible for the opposite situation to occur). This roughly implies that most
initial upfront work is high leverage and that driving towards ‘expertise’ may
have little return on investment. What I like about this proposal is that it
accepts the fact that knowledge categorisation is messy and that there’s
probably no one in the world that knows everything.</p>
<h2>Evaluation of Others</h2>
<p>One problem with the above proposal is that it doesn’t consider the common
usage of such tiered categorisations: evaluation of others’ sets skills. Amy
Cuddy proposes that most people are judging you on your competency after
they’ve judged you on whether or not they can trust you. I propose we try to
drop the skill-evaluation-at-moment-of-evaluation tactic and focus on
evaluating others in two primary metrics: trust and ability to advance ones
skills over any given period of time. Some people call this “hiring for the
slope rather than the Y axis”.</p>
<p>That said, I’m predominantly a software engineer and in my experience I find
the former usage of this proposal to be the one I care about the most.
Determining what’s appropriate for ourselves rather than trying to divvy people
up into boxes is a far more valuable use of engineering time, and further,
evaluating people on the merits of their enthusiasm, ability and desire to
continually learn, and their capacity to both work away and in teams is more
worth it’s weight in gold than if someone is a self-proclaimed 10x engineer
capable of cranking a lot of (read: complicated, un-maintainable, mal-scoped)
code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Code is Decoding</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/reading_code_is_decoding.html</link>
      <guid>https://justanotherdot.com/posts/reading_code_is_decoding.html</guid>
      <pubDate>Sat, 27 Jan 2018 13:43:09 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Roger Antonsen says in his Ted Talk <a href="https://www.ted.com/talks/roger_antonsen_math_is_the_hidden_secret_to_understanding_the_world"><em>Mathematics is
the Secret to Understanding the
World</em></a>,
mathematics, or rather the act of understanding, is largely about:</p>
<ul>
<li>Discovering patterns</li>
<li>Devising language(s) to express said patterns</li>
<li>Making assumptions</li>
<li>Playing around with all of the above</li>
</ul>
<p>Early this January I finished reading <em>Coders at Work</em> and in each interview
there is a recurring question of “how do you read code?” Here’s a rough summary
of some styles mentioned I found particularly useful:</p>
<ul>
<li>Get the code building early and often and make various changes to study
connections</li>
<li>Read it like literature whether printed out or jumping around</li>
<li>Rewrite the code into a version optimised for legibility</li>
<li>Puzzle through it the same way one would tackle a mathematical problem</li>
</ul>
<p>It turns out I had previously read <a href="http://www.gigamonkeys.com/code-reading/">a post from Peter
Seibel</a>, the book’s author, who had
tried on several occasions to start code reading groups at his places of work,
in which he states:</p>
<blockquote>
<p>It was sometime after that presentation that I finally realized the obvious:
code is not literature. We don’t read code, we decode it. We examine it. A
piece of code is not literature; it is a specimen.</p>
</blockquote>
<p>He goes on to quote a passage (my favourite in the book) of his interview with
Knuth (emphasis added by me):</p>
<blockquote>
<p>Knuth: But it’s really worth it for what it builds in your brain. So how do I
do it? There was a machine called the Bunker Ramo 300 and somebody told me
that the Fortran compiler for this machine was really amazingly fast, but
nobody had any idea why it worked. I got a copy of the source-code listing
for it. I didn’t have a manual for the machine, so I wasn’t even sure what
the machine language was.</p>
<p>But I took it as an interesting challenge. I could figure out <code>BEGIN</code> and
then I would start to decode. The operation codes had some two-letter
mnemonics and so I could start to figure out “This probably was a load
instruction, this probably was a branch.” And I knew it was a Fortran
compiler, so at some point it looked at column seven of a card, and that was
where it would tell if it was a comment or not.</p>
<p>After three hours I had figured out a little bit about the machine. Then I
found these big, branching tables. So it was a puzzle and I kept just making
little charts like I’m working at a security agency trying to decode a secret
code. But I knew it worked and I knew it was a Fortran compiler—it wasn’t
encrypted in the sense that it was intentionally obscure; it was only in code
because I hadn’t gotten the manual for the machine.</p>
<p>Eventually I was able to figure out why this compiler was so fast.
Unfortunately it wasn’t because the algorithms were brilliant; it was just
because they had used unstructured programming and hand optimized the code to
the hilt.</p>
<p>It was just basically the way you solve some kind of an unknown puzzle—make
tables and charts and get a little more information here and make a
hypothesis. In general when I’m reading a technical paper, it’s the same
challenge. I’m trying to get into the author’s mind, trying to figure out
what the concept is. <strong>The more you learn to read other people’s stuff, the
more able you are to invent your own in the future, it seems to me.</strong></p>
</blockquote>
<p>Alas, if we’re to treat literacy in a human language as the combined skills of
writing <em>and</em> reading, why do we place so much emphasis on the former when it
comes to teaching how to code? I now actively seek out code to read for the
same reason Knuth mentions early in his interview; dispelling magic is an
invaluable skill we crucially need to keep improving. Treating things as a
black box may sometimes help reasoning but it doesn’t mean we should keep the
covers on until the end of the universe.</p>
<p>Take my <a href="https://j2kun.svbtle.com/mathematicians-are-chronically-lost-and-confused">favourite mathematical
post</a>
by Jeremy Kun in which he discusses, with a wonderful supporting analogy from
Andrew Wiles about stumbling around a dark house looking for light switches,
that feeling lost is far more common and acceptable than the enlightened state
we assume intelligent role models seem to possess. These role models have
simply learned to live with and accept the discomfort of being lost because
that’s what it means to be in a process of learning and growing!</p>
<p>Simon Peyton Jones is well known for stating how important it is to simply
<em>do</em>, no matter how humble the project in question may be. This is fantastic
advice for coding literacy; Writing this blog post involved an initial sit down
of a roughly one-thousand word brain dump followed thereafter by approximately
two days of refinements, with lots of rereading, simply honing in on the main
theme. It’s important to get fingers moving and code executing, but it’s just
as important to advocate to new starters that reading is something they should
be pouring time and attention into.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fail Fast not Error Out</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/fail_fast_not_error_out.html</link>
      <guid>https://justanotherdot.com/posts/fail_fast_not_error_out.html</guid>
      <pubDate>Sat, 07 Oct 2017 12:50:02 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>tl;dr</strong> Static analysis is a form of 'failing fast' that does not consist of
leaving error based exit strategies (which should be reserved for situations
where the program simply cannot transition to a new state) in code that will
eventually be shipped to production.</p>
<p>The notion of 'failing fast' in programming details finding faults at the
earliest possible time; when the application developer is fitting out the code!
This seems to be sensible, but is often strangely antithetical to the notion of
'the only true test of code is production data'; how can we fail fast and catch
a ton of bugs when the truly icky bugs we want to smash are after we've done
some kind of deployment? Clearly the distinction here is to find bugs, in any
context, as soon as possible, production or otherwise, but that does mean the
concept can be carried over to production, where failing fast could mean major
problems (payments not being processed, account information being leaked, etc).</p>
<p>Ops people have devised all sorts of methods to roll out code in deployment to
handle situations like this; blue-green deployments, canary deployments, et.
al. all focus on testing code on a much smaller subset (on some segment of
traffic) accepting <em>some</em> failure as an acceptable loss to know if the code is
ok enough to push to 100% of the traffic. Percentage deployments put a lot of
focus on monitoring and logging. Essentially, people have to watch the metrics
after the roll out to make sure everything is ok.</p>
<p>A computation does not need to crash the program in order to fail fast:</p>
<ul>
<li>
<p>Errors are for irrecoverable states of program transition; the program
depends on writing to disk for some critical task, and the disk has been ripped
out of the server rack and can no longer be accessed via the kernel drivers.
The kernel tells us something very bad is up, and we die. This is fine, because
there's no sensible state to transition to in this scenario.</p>
</li>
<li>
<p>Exceptions are for situations where something bad happened, but it's not bad
enough to cause us to fail completely, i.e. we can do something to transition
to another sensible step. The general frame of mind is that exceptions can be
problematic when they are not caught, but can be a pain to constantly look out
for (this is the source of the 'checked exceptions' controversy in the Java
community). The primary problem with exceptions is that if an exception is not
'checked' or 'caught', then it will bubble up to the main function (entry
point) of the program and cause it to error out as above. Exceptions are said
to be sensible if they preserve <strong>progress</strong> and <strong>preservation</strong>, meaning that
they are able to move forward and they don't manipulate the types of
expressions where they are thrown. In most languages, however, we can't be sure
if something is going to throw an exception, so many programmers are told to be
defensive and paranoid; hardly the kinds of things you'd want out of people who
need to also be innovative.</p>
</li>
</ul>
<p>In most pure functional programming languages, we know less about lurking
exceptions, and this is of particular importance. When we have a type system,
which is effectively a lightweight proof system that gives us static guarantees
and checks at compile time (a form of 'fail fast' but without the problem of
leaving 'ticking time bombs' in our code base that may still present themselves
in production), then it makes no sense to fail fast in an error-prone way.
Abstractions such as monads and friends allow us to do this elegantly and
tersely.</p>
<p>It is far more ideal to let pure computations transition gracefully to new
states, failures to be found at <em>compile time</em>, and production code to be
robust and resiliant. If we extend this notion of static analysis to property
based testing, formal correctness practices, and even linters, among other
things, there are several smarter alternatives to failing quickly and
validating the correctness of our programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Start</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/hi.html</link>
      <guid>https://justanotherdot.com/posts/hi.html</guid>
      <pubDate>Sun, 17 Sep 2017 16:06:55 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I've managed to hack together this blog using Chris Penner's alternative to
Hakyll, <a href="https://github.com/ChrisPenner/SitePipe">SitePipe</a>, as well as using a
CSS framework I've wanted to try out, <a href="http://bulma.io/">Bulma</a>, and host it on
github pages. My plan is to:</p>
<ol>
<li>Move over to some other hosting platform to do my own infra (either Digital
Ocean or Linode w/ some kind of SSL reverse proxy.)</li>
<li>Tidy up a little bit of the layout around the site.</li>
<li>Write some content!</li>
</ol>
<p>I had written a <a href="https://medium.com/@justanotherdot/sapir-whorf-and-you-f4b45ff2f216">post over at
Medium</a>
and while I liked the overall editing experience, I knew I'd eventually find
some things lacking e.g. Mathjax integration and the like, of which I was able
to sort of hack into my <a href="http://justanotherdot.tumblr.com/">previous tumblr</a>
thanks to having access to the HTML of my blog, and is another thing I'll need
to add here.</p>
<p>When I was originally looking to make my own blog my ideal layout consisted of
Markdown being dumped as HTML which would be sent over some kind of ajax
request to fill in an SPA. That still may happen, but at the moment this
workflow seems fine, and I'm hoping I can fulfill a personal quota of 250 words
per week, but we'll see how that goes.</p>
<p>Also, there's something to say about how generic (and sometimes even bland)
some publishing sites make the blog-to-blog experience. Yes, it is just text
and media, but even a little bit of a personal touch goes a long way, I think.
I worry that, especially in the realm of tech articles, people will feel the
'overal sleek' experience of something like Medium will help carry the weight
of their voices beyond the actual rigor of their articles.</p>

        ]]></content:encoded>
    </item>
    
  </channel>
</rss>
