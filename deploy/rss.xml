<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>justanotherdot</title>
    <link>https://justanotherdot.com</link>
    <atom:link href="https://justanotherdot.com/rss.xml" rel="self" type="application/rss+xml" />
    <description>Personal blog of Ryan James Spencer</description>
    <category>Technology</category>
    <copyright>2018 Ryan James Spencer</copyright>
    <language>en-us</language>
    
    <item>
      <title>An Infinite Barrage of Mountains to Climb</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/an-infinite-barrage-of-mountains-to-climb.html</link>
      <guid>https://justanotherdot.com/posts/an-infinite-barrage-of-mountains-to-climb.html</guid>
      <pubDate>Wed, 13 Nov 2019 19:14:36 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This Tuesday I went to my last therapy session for the year. In that session I
finalized a relapse prevention program for my obsessive compulsive disorder and
recapped strategies I learned to handle various stressors in my life.</p>
<p>I‚Äôm one of those exuberant but actually shy people. I love reading and
programming because I‚Äôd rather be in an internet-equipped mountain cabin away
from the rest of civilization than on a boat in the Caribbean partying. I spend
a great deal of time honing skills through study and experimentation. The
studying has, and sometimes still, makes me anxious and exhausted.</p>
<p>A <em>lot</em> of people experience this; they learn new thing after new thing and when
the mountain seems climbed and truly conquered they look up again only to reveal
a new mountain waiting for them to ascend. If only they climb that next one will
they truly be done. It is a lie. There is never an end to the mountains.</p>
<p>This hurdle of seemingly endless mountains is partly why people feel constant
imposter syndrome. I do. I also feel imposter syndrome when I am full of
self-criticism by attaching my self worth to my productivity. <a href="https://www.google.com/search?q=thought+challenging">Thought
challenging</a> is a
convenient way to question the validity and usefulness of the thoughts we might
encounter that attempt to undermine us. If the thoughts still plague us despite
challenging them, it can also help to try <a href="https://www.google.com/search?q=thought+defusion">thought
defusion</a> where one displaces
the importance of a thought, or thoughts, to something less dominant by various
means. For example I sometimes imagine someone whom I would not trust their
advice as saying the plaguing thoughts to me. This makes it easy to ignore the
thought then. I know it's there, but like a hand on my lap instead of a hand in
front of my face, I barely notice it's presence.</p>
<p>Education matters and in a technical field it is unavoidable. If you don‚Äôt want
to stagnate you need to keep pushing to improve your skills and knowledge.
Pushing towards discomfort is good! It is the essence of growth to push until we
are uncomfortable. But <em>continually</em> pushing towards things until the bucket of
energy is dry withers away at who we are. We can become not only paranoid from
the rampant thoughts of imposter syndrome but also burned out from our desire to
improve. Rest is a crucial part of the process. Get uncomfortable for a bit,
take a breather, repeat.</p>
<p>Full disclaimer: I am not a medical professional of any kind. All I can do is
try to share some things I've learned that were prescribed and work for me. The
articles out there in the tech space for imposter syndrome and the like have not
cut it before and writing is article is more for reminder as it is for sharing.
I give no warranty as to the use of anything I say here in the entirety of this
article. If you are able to attend therapy and think it might benefit you in any
way, shape, or form, by all means you should try to attend. Getting personalised
support is best. With that said, How do I learn and stay healthy these days?
Here's some things I've picked up in my time wandering around:</p>
<ol>
<li>
<p><strong>You are not your work.</strong> Your work is it‚Äôs own thing. If you are your work you
can not have a critical, and therefore healthy, stance towards it.</p>
</li>
<li>
<p><strong>If you don't feel dumb you aren't learning and everyone who is learning
feels dumb.</strong> You need to pursue this feeling all the while reminding
yourself that everyone feels this and it does not signify that they, or you,
are a dumb person. As they say, it's not about being right but knowing the
truth. People who are always right don't go through this process and don't
actually obtain any deep understanding or learning.</p>
</li>
<li>
<p><strong>Compare yourself to yourself.</strong> Your history is yours alone and it isn't
logical to try to cookie-cutter other people's tales onto your own. Track
your improvements not by comparing yourself to others but by comparing
yourself to your past.</p>
</li>
<li>
<p><strong>Life is not work.</strong> Some people might argue that it's OK to dump all your
time into studying or hacking on side projects or whatever but the reality is
we need different entertainment for our brains and we need to give it rest.
Simply &quot;taking a holiday&quot; from your studying or work where you do literally
nothing is a fantastic way to replenish your excitement for the things you
are passionate about.</p>
</li>
<li>
<p><strong>Excitement is healthy and an absence of it is a warning flag.</strong> It helps
clarify what you want to concentrate your efforts on. Yes, sometimes things
will be boring when you learn about a topic but that doesn't mean you have to
read the entirety of Donald Knuth‚Äôs <em>The Art of Computer Programming</em> to make
a program (although it certainly might help). If you can make the task at
hand fun you will probably replenish your store of energy all the while
<a href="https://www.psychologytoday.com/files/attachments/4141/the-neuroscience-joyful-education-judy-willis-md.pdf">retaining information more
effectively</a>.
Doesn't this contradict my argument that we need to get to the state of
discomfort to grow? Not quite.</p>
</li>
<li>
<p><strong>Questions can be fun.</strong> Having a healthy sense to question everything is
what makes people smart. We want to tirelessly get at the truth but the
process of getting there need not be a slog through a muggy swamp on a hot
summer afternoon. You may have had that giddy moment exclaiming &quot;Aha! It
works! But <em>why</em>?!&quot; and whiled away another stream of hours on the problem,
and resulting problems, at hand.</p>
</li>
</ol>
<p>There's always another mountain and it <em>is</em> daunting but hopefully something
I've said here will at least help make part of overcoming the next mountain
easier.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Nothing of Value Will be Lost</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/nothing-of-value-will-be-lost.html</link>
      <guid>https://justanotherdot.com/posts/nothing-of-value-will-be-lost.html</guid>
      <pubDate>Sun, 10 Nov 2019 18:49:53 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>Drop your backlog. Burn all of your tickets. Eject your issues into the sun.
Nothing of value will be lost.</strong> Teams and maintainers alike cling to reminders
of work as if they are the same as the result of the work itself. Backlog
grooming sessions pass and only thin slabs of the gelatinous mass disappear into
the abyss all the while delaying developers from focusing on actual work.</p>
<p>Design and debate need to occur for a project to progress and when those things
happen it's good to record the results. As these records accumulate they age
because parts take priority over others. Ok, maybe you don't want to drop
<em>everything</em> but you definitely want to drop items older than a certain age. I'm
fond of choosing a natural period of time where you, the human, can easily
enumerate key points that have happened. Longer periods of time produce smaller,
less detailed lists. Periods of time that are too small might experience churn
on the issue tracker as items disappear and return repeatedly.</p>
<p>This process might sound crazed. How dare we close valid issues tied to real
people on an open source project or abandon fixes and feature work that could
drive up revenue and delight users purely because of age? Finding what to work
on is not the hard part, despite what you may think. Prioritising, hashing out
ideas, and setting goals has value but <a href="https://en.wikipedia.org/wiki/Sturgeon%27s_law">ninety percent of everything is
crap</a> and issues sitting in the
dark, ignored and unloved, are alike.</p>
<p>Those using todo lists will know the value of scrapping them at the end of a day
or week. Copying a few things over from prior days or periods of time can be
beneficial but usually the gain is marginal. Adopt a process that reflects the
fact that things change rapidly. <strong>Work that needs doing is from problems and
pain points that are being frequently encountered.</strong> It's work that's at the tip
of the tongue. This is the reason we care about 99th percentiles and avoid
one-off optimisations and bug fixes. What if the bug fix is data being
spuriously deleted? I can guarantee that issue won't stagnate and if it does I
think there are deeper issues that need handling. In the same way <a href="https://stackoverflow.com/a/153565/2748415">Kent Beck
told people on Stack Overflow</a> that
he gets &quot;paid for code that works, not for tests&quot;, one also won't (shouldn't?)
get paid writing or pruning issues.</p>
<p>Neglect the fool's gold of issue trackers. <a href="https://www.goodreads.com/book/show/1633.Getting_Things_Done">Your brain isn't a storage
device</a>; enable
your mind to process what it ought to be processing by using these glorified
todo lists to offload information.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Composition</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-composition.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-composition.html</guid>
      <pubDate>Thu, 07 Nov 2019 06:07:08 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Using composition gives you superpowers. It is by far the most practical
experimentation tool I know.</p>
<p>The <a href="http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:.">dot (.)
operator</a>
is my favorite infix operator in Haskell. Statically typed languages help ensure
that <a href="https://en.wikipedia.org/wiki/Function_composition">function composition</a>
is structurally sound before anything is run. Composition of two functions means
the type of the output of the first function must equal the type of the input of
the next function. Many languages now have a pipe operator which is the
composition operator in reverse. Some even use pipe or dot to write flow of
execution top-to-bottom or bottom-to-top, given how you can stack the calls.</p>
<p>This isn't just an article about the usefulness and specifics around function
composition itself. Composition as a concept forms a basis of for problem
solving and systems of proof. By decomposing a system or problem into parts we
can scrutinize and, thus, verify them for use in constructing the same or
potentially different solutions, proofs, and so on. Having solid building blocks
means we can play around with different arrangements. Playing around with these
building blocks and assumptions is how
<a href="https://www.goodreads.com/book/show/192221.How_to_Solve_It">mathematics</a> and
<a href="https://www.justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html">experimentation</a>
works at its core.</p>
<p>Composition also forms part of the basis of a fascinating branch of mathematics
known as <a href="https://github.com/hmemcpy/milewski-ctfp-pdf">category theory</a>.
Envision a type of mathematics that encodes any arbitrary concept as a
graph-like diagram to explore general structures and relationships. Having a
<a href="https://rs.io/why-category-theory-matters/">mechanism for encoding general
topics</a> empowers you with the
ability to play with structure and assumptions and study the structure and
implications of those arrangements. Caveat emptor; I am not saying composition
<em>requires</em> category theory to be useful! In fact, having too complicated a
system defeats the purpose of having a
<a href="https://www.justanotherdot.com/posts/lightweight-is-beautiful.html">lightweight</a>
guide.</p>
<p>Architecturally, the common phrase that &quot;systems are the sum of their parts&quot; is
a farce. If systems were some linear combination then removing individual
elements would merely reduce the size of the system, but removal can mean total
system failure, no change whatsoever, and possibly improvement in the system as
a whole!</p>
<p>It is rare to find a mental tool so broadly applicable and yet so uncomplicated
in nature. I'll reiterate strongly here; you don't necessarily need to be
pedantic about the shape of things to reap these benefits. Nor do you need to
understand category theory to its <a href="http://eugeniacheng.com/wp-content/uploads/2017/02/cheng-lauda-guidebook.pdf">highest levels of
complexity</a>
to piece together solutions. In my mind the <em>broad</em> steps are always the same:</p>
<ol>
<li><strong>Take, or produce, components</strong></li>
<li><strong>Scrutinize the components</strong> as you may be able to
i. break things down further (1)
ii. see how things connect
iii. or verify the parts are sound</li>
<li><strong>Experiment with arrangements of components</strong></li>
</ol>
<p>I see composition as a framework for experimentation with no added consequence
of increased complexity from the use of the framework itself. Experimentation
allows us to explore new connections. Exploring new connections means finding
solutions to problems in any domain. Discoveries are the bedrock of learning.
Rapid experimentation increases rate of knowledge acquisitions as well as
improved retention of knowledge. This is why composition is a superpower.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Roundtrips</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-roundtrips.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-roundtrips.html</guid>
      <pubDate>Sat, 02 Nov 2019 20:37:16 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Data &quot;roundtrips&quot; when it goes from one value, to another, and back to the same
value without any data loss, gain, or corruption. If you write code, you have
probably roundtripped JSON, YAML, TOML, or some other serialization format in
your time. You have also probably written versions of functions that do a
similar 'cycle' of some data. Any time you care about data being the same after
it's gone through the ringer, you want to write a roundtrip test.</p>
<p>Pretend we have a system where data comes in as JSON. We slurp up that JSON into
a type using <code>serde</code> (rust's idiomatic, type-driven serialization +
deserialization library). That data might later go onto being a type unrelated
to JSON, so we might write some <code>From</code> instances. This will be our adaptive
layer so we can keep the shape of the JSON and our core types distinct. I
mention this approach briefly in my post <a href="https://www.justanotherdot.com/posts/safely-shape-code-with-curtains.html">&quot;Safely Shape Code with
Curtains&quot;</a>.
The <code>From</code> instance would normally be trivial, but we don't want the JSON layer
and the core types to look the same, do we? That would make the point of the
JSON types moot:</p>
<pre><code>struct JsonType {
  names: Option&lt;Vec&lt;String&gt;&gt;,
  ids: Vec&lt;i64&gt;,
}

struct CoreType {
  names: Vec&lt;String&gt;,
  ids: Vec&lt;i64&gt;,
}

impl From&lt;JsonType&gt; for CoreType {
  fn from(x: JsonType) -&gt; Self {
    Self {
      names: x.names.unwrap_or(vec![]),
      ids: x.ids,
    }
  }
}

impl From&lt;CoreType&gt; for JsonType {
  fn from(x: CoreType) -&gt; Self {
    Self {
      names: Some(x.unwrap())
      ids: x.ids,
    }
  }
}
</code></pre>
<p>We could test each direction in isolation, but that would mask the actual
mistake here. Can you spot it? The roundtrip test in a property based testing
context would find the failure quite quickly. I'll do it by hand here to
demonstrate the mistake:</p>
<pre><code>let beg = JsonType {
  names: None,
  ids: vec![1,2,3],
};
let roundtrip_fwd: CoreType = expected.into();
let end: JsonType = roundtrip_fwd.into();
assert_eq!(beg, end);
</code></pre>
<p>When the data comes back to the JSON layer, unless we tell <code>serde</code> that empty
vectors are always <code>None</code>s for this field, we've now lost information. Clients
might care a lot that their POST of some JSON for creating an entity in this
make-believe system is non-symmetric. Developers might be going between the core
and the JSON types regularly, and they may even be using the JSON types to write
to disk, too, which would mean what was passed up from the client is now not the
same as what is stored.</p>
<p>We can extrapolate this sort of information loss or corruption to other
conversions. If you author an automatic code formatter, say <code>prettier</code>, <code>gofmt</code>,
<code>mix fmt</code>, <code>rustfmt</code>, and so on, you'd want to make sure that any time you save
a file and the formatter runs that your code is still the same code,
semantically, as it was before saving the file. Although things might possibly
look the same by eye, it could be another program entirely when run.</p>
<h3>Food for thought</h3>
<p>A quick refresher on functions.</p>
<ul>
<li>Functions can be seen as <strong>mappings</strong> from one type of value to another</li>
<li>All possible values that can go into our mapping are known as the <strong>domain</strong> of
a function</li>
<li>The set of all possible values our mapping can produce is called the <strong>codomain</strong></li>
<li>The set of all values the mapping realistically produces is called the
<strong>range</strong> or <strong>image</strong></li>
</ul>
<p>Ok, onto the concepts with fancy names:</p>
<ul>
<li>
<p>An <strong>injective</strong> mapping is when a mapping goes from values in the domain to
<em>unique</em> values in the codomain.</p>
</li>
<li>
<p>A <strong>surjective</strong> mapping is when a mapping goes from values in the domain to
<em>every</em> value in the codomain, even if some mappings overlap.</p>
</li>
<li>
<p>A <strong>bijective</strong> mapping is <strong>simultaneously injective and surjective</strong> which
means every value in the domain maps to every value in the codomain exactly
once.</p>
</li>
</ul>
<p>Why does this matter?</p>
<p>Bijective mappings give you an inverse function for free. If you are a value in
the codomain and you know the mapping is bijective, then you can be sure that
there must be one, and only one, value where you came from in the domain.
One could <a href="https://math.stackexchange.com/a/165440/156419">prove bijections</a>
using classical means but we don't need to for production usage. Instead, it
suffices to simply show the action going forwards and backwards.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Idempotence</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-idempotence.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-idempotence.html</guid>
      <pubDate>Wed, 30 Oct 2019 20:28:07 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Charging someone twice is bad for business; it burns trust with customers and it
involves a lot of unnecessary churn. Payment providers go to <a href="https://stripe.com/au/blog/idempotency">great
efforts</a> to support <em>idempotent</em>
endpoints. When you do something more than a given number of times, and every
time after that, things don't change. In the case of a payment it would be once
and only once, no matter how many times the request was submitted after that.</p>
<p>An
<a href="https://www.justanotherdot.com/posts/the-lowly-assert-involution.html">involutive</a>
function is idempotent modulo a <em>certain</em> number of applications. Involutive:
Driving a car around a square block means after four turns you're back on the
same corner you began on. Idempotent: A volume knob that reaches maximum volume
but still keeps turning. The assertion of idempotence looks suspiciously like
involution, but the concepts aren't quite the same:</p>
<pre><code>-- Involutive

f(x)       != x
f(f(x))    == x
f(f(f(x))) != x

-- Idempotent

g(x)       == x
g(g(x))    == x
g(g(g(x))) == x
</code></pre>
<p>If the function <code>f</code> was a one-hundred and eighty degree turn around a point then
the next part of the series would be another equality and would alternate back
and forth for every other function application. In the case of <code>g</code>, we do
something once, twice, or n-many times and nothing seems to change. Per the
volume example, there might be <em>some</em> changes initially, but <code>g</code> becomes
idempotent at or after a particular value.</p>
<p><a href="https://en.wikipedia.org/wiki/Idempotence">Idempotence</a> can relate to values,
but it can also relate to side effects, such as the payment example we've given
above. A &quot;thunk&quot; is a function that performs a calculation once and then stores
(&quot;memoizes&quot;) that result to return on all future calls: in this case a thunk is
idempotent in its computation: it's lazy <em>and</em> cached.</p>
<p>Things don't always <em>need</em> to be idempotent but can be chosen to be idempotent
for stylistic reasons. One API may force users to use explicit <code>insert</code> and
<code>update</code> calls, managing the housekeeping of keys itself, whereas a different,
but equally effective, API could allow a single endpoint that &quot;saves&quot; the
provided data, inserting at first, overwriting when the data is different, and
idempotent when the data is the same, forcing the tracking of keys on the
client. Both of these are valid options but have different trade offs for
particular applications!</p>
<p>When you think of idempotence, think about the mental model of things &quot;clamping&quot;
into place for a particular subset, or all, of our domain (inputs). And while
you're at it, make sure no one ever gets charged again for smashing the refresh
button for a slowly loading payment submission page!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Involution</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-involution.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-involution.html</guid>
      <pubDate>Tue, 29 Oct 2019 05:43:10 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>As part of <a href="https://www.justanotherdot.com/posts/the-lowly-assert.html">The Lowly
Assert</a> series I
wanted to go over some mathematical patterns. Filling your arsenal of known
properties helps with recognizing common ways functions, systems, etc. are, and
should continue, to behave.</p>
<p>Occasionally you'll write functions that flip-flop: when you call the function
multiple times in a row, chunks seem to cancel out. Mathematics calls these
functions <a href="https://en.wikipedia.org/wiki/Involution_(mathematics)">&quot;involutive&quot;</a>. Negating a number or a boolean twice gets you back
to the original value. Involution is handy to recognize because it's a simple
assertion:</p>
<pre><code>x == f(f(x))
</code></pre>
<p>The classic property based testing example of this is the
<code>reverse(reverse(some_list))</code> you'll see in endless tutorials and getting
started guides on the subject. When you reverse a list you expect to simply flip
the contents one end to the other, but this may not be immediately applicable to
day-to-day affairs. Here's one: a function that opens a dialogue box has two
states, open and closed, and is commonly tested for involutivity; if you didn't
have the toggling action you'd see no feedback after clicking!</p>
<p>But involution doesn't have to be about functions applied exactly twice.
Rotating around a point back to an original direction can be done with various
divisions: two rotations of pi, four rotations of pi/2, and so on. Precisely,
involution requires that the application be a single function, but we can
sometimes be a bit less rigorous and claim that two or more actions that
eventually cancel out are involutive: removing and adding an item in a
collection or returning someone's money in an exchange.</p>
<p>With this we can hopefully see the application isn't on any end of the scale:
from small to big, XORing bits to whole product flows that might 'restart' the
user in a funnel. Properties like these are worth their weight in gold because
they are useful in almost any type of testing and are definitely a shining
example of The Lowly Assert.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Making Plants Thrive</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/making-plants-thrive.html</link>
      <guid>https://justanotherdot.com/posts/making-plants-thrive.html</guid>
      <pubDate>Wed, 23 Oct 2019 19:38:18 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It's often lamented that software projects become dead plants in an unloved
garden: we excitedly keep buying new plants but we don't put in the time to see
them thrive.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Anyone else&#39;s GitHub account literally just a graveyard of good intentions? üôé‚Äç‚ôÄÔ∏èüôã‚Äç‚ôÄÔ∏è</p>&mdash; CaroOooOoOolyn üëª (@carolstran) <a href="https://twitter.com/carolstran/status/1184938790533681152?ref_src=twsrc%5Etfw">October 17, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The appeal of building something new, playing with some fancy dependency or
tool, trying out some new process; if only we could resist the temptation. But
we shouldn't resist the temptation because this is the sign of healthy
experimentation! <strong>It's far better to experiment in your spare time than to use
your career as an excuse to try out the next shiny thing.</strong></p>
<p>I'm a huge fan of &quot;laboratories&quot; where questions you have regarding code are
answered by creating code and committing them to a central repository. Making
them multi-language helps by reducing friction for testing things out. A
graveyard of good intentions becomes a collection of prior discoveries.</p>
<p>This doesn't change the fact that we feel guilty that we can't keep the plant
alive. It takes a little discipline, and maybe for some, a bit of prior
knowledge, but it's not too hard to get things into place. In the same way we
reduced friction by making a project multi-language, introducing automation to
reduce toil is the best way for us to combat bitrot; if we can come back to
projects knowing full-well they build, we are much more willing to continue to
&quot;water the plants&quot;. Making a project thrive comes in a few major parts:</p>
<ol>
<li>Testing and building the code before it reaches trunk/master</li>
<li>Artifacts (library, binary, etc.) are created and published</li>
<li>Said artifacts may be deployed to a server to run</li>
</ol>
<p>Many other automations can be done too: linting, dependency updates, scheduled
builds, et. al. Scheduled builds are cool (and underrated) because they
continuously show projects are building and tests are passing. You now have
extra capacity from all the built artifacts to handle services going down or
security updates having been released. <strong>If you automate away the toil, you can
treat a project less as a chore (by focusing less on the accidental complexity)
and more as a labor of love (by focusing on the inherent complexity of the
problem you are trying to solve).</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Dumping Grounds for Good and Bad</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/dumping-grounds-for-good-and-bad.html</link>
      <guid>https://justanotherdot.com/posts/dumping-grounds-for-good-and-bad.html</guid>
      <pubDate>Mon, 21 Oct 2019 19:04:52 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>A former colleague and friend once referred to modules that grow without any
clear organisation as &quot;The Dumping Grounds&quot;. You probably know this module: it
often is named &quot;utils&quot; and acts as a kitchen sink for anything you are unsure
where to put. It might come in a different name, and there might be several of
them when the last dumping grounds were abandoned. When that happens, it's only
a matter of time until the majority of modules all become dumping grounds.</p>
<p>I do this thing when I'm coding or writing articles sometimes that I <em>also</em> call
&quot;The Dumping Grounds&quot;. It's similar because it's a pit of random junk I think
might be useful but may also just be crap. I have a rule about this space:
whatever is left in there by the time I'm done with the main chunk of work gets
thrown out. No disputes.</p>
<p>I might do this with things like whitespace or certain patterns of characters,
when writing these articles. For example, I'll tend to put a triple-hyphen or
triple-underscore to mark where the dumping ground begins and it (generally)
ends at the end of the file.</p>
<p>With the clear discrepancy of the dumping grounds, you can pick and choose what
you want from it, knowing full well it will get deleted. Other times, like in
code, it can be a bit more subtle.</p>
<p>When people work, there tends to be a bit of mess accumulated in particular
areas of the final piece. If the whole thing is a mess it can be hard to think,
but if the mess is distinct it can be a guiding force. Parts of a sculpture
might clearly be finished and other parts are in the rough. When I code, there
is usually a combination of tactically using whitespace, comments, and sometimes
syntactic/type constructs to keep chunks of mess easily identifiable, and, most
importantly, deletable.</p>
<p>If you <strong>treat dumping grounds as the landfills they are and try to keep them
out of your results</strong> then you'll get closer to the target you want. By using
this form of recognizable mess attached with a rule you can do just that, but
remember that mess is inevitable and that's ok! This approach isn't saying &quot;it
can <em>all</em> be a bit messy <em>just for this one piece</em>&quot;. What it is saying is &quot;this
is what I want and this is the mess I'm using to get there which <em>I will not
keep</em>&quot;.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Safely Shape Code With Curtains</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/safely-shape-code-with-curtains.html</link>
      <guid>https://justanotherdot.com/posts/safely-shape-code-with-curtains.html</guid>
      <pubDate>Sat, 19 Oct 2019 21:29:55 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Once upon a time I studied photography at an art school. It was there that I
learned the importance of separation between tones in an image. If the
separation, tone or color, between objects in my images wasn't quite right I'd
have to redo all my work in order to get a grade. <a href="https://en.wikipedia.org/wiki/Gestalt_psychology">Separation is how we often
define our mental maps of the
world.</a> For this article, I'll
call anything that is distinct from other things an &quot;entity&quot;.</p>
<p>Entities have edges. They may have eddies of communication or arrows of
connection through these edges. Edges may be incidental, e.g. defined by people
you don't know or from natural consequences, or they may be intentional, i.e.
the result of deliberate planning and execution. Entities have non-zero surface
area, otherwise they wouldn't exist, but that doesn't mean they cannot be
relatively invisible.</p>
<p>Clarifying (edges) means simpler mental maps. Simpler mental maps means easier
to reason about systems and programs. Simpler systems and programs means
increased velocity for progress and experimentation. Each of these examples
could be their own posts, but for now it suffices to say that examples of this
type of organization (clarification) are,</p>
<ol>
<li>
<p>Serialization to the wire (network), disk, and internal datatype definitions
<em>individually</em> go into their respective modules</p>
</li>
<li>
<p>Core logic that performs calculations versus reading from disk, e.g.
application level versus storage engine logic, are separated</p>
</li>
<li>
<p>Munging layers, or what some call an adapter, that transform data to the
shape you so desire are not tied into (1). This is bidirectional; it's
equally fair to have the adapting layer work on outbound and inbound
interfaces.</p>
</li>
</ol>
<p>Most of this might feel a bit obvious: things have edges and that's how
we tell they are distinct, but how does this relate to coding?</p>
<p>It's common to think that programming <em>has</em> to be a balancing act between
progress (by accepting breakage) and stability (by leaving things alone). I've
talked a bit before about <a href="https://www.justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html">how vital constant experimentation
is</a>,
but this balancing act is not the <em>only</em> way to go about things. Yes, things
break when they have production data running through their digital veins and
having instrumentation to gain visibility into your running code in production
is crucial to combat this statistic of failure, but let's consider another
approach to the development side of correctness.</p>
<p>Although it may seem strange for me to use the term <em>artificial</em> when all the
boundaries discussed here seem planned by ourselves or by others, I use the term
&quot;curtain&quot; here to denote <em>artificial</em> delineations we establish to avoid working
in <em>slices</em>. A sliced approach to development means we attempt to get all
working functionality, from front to back, one slice at a time. In the following
diagram the red boxes are slices of features whereas non-sliced functionality is
stable:</p>
<figure>
  <img
    src="/assets/images/sliced-development-example.png"
    alt="a diagram depicting 'sliced' development"
    title="An example of 'sliced' development">
  </img>
</figure>
<p>We can define &quot;curtains&quot; (again: artificial edges for the purposes of
development) to retain stability in all areas &quot;exterior&quot; to the curtain.
&quot;Exterior&quot; may very well be &quot;interior&quot; code! Setting up curtains can be done
with <a href="https://www.justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html">feature-flags, parallel
implementations</a>
or even creating new surfaces where interaction will be performed and migrating
after the fact when everything seems settled. As long as the &quot;exterior&quot; to the
curtain go on its life as if nothing is wrong, a curtain serves its purpose. Per
the example above it might look like this:</p>
<figure>
  <img
    src="/assets/images/curtained-development-example.png"
    alt="a diagram depicting 'curtained' development"
    title="An example of 'curtained' development"
  </img>
</figure>
<p>In this diagram, you could be setting up the curtain to keep the core of the
application stable or the client and interfaces the client talks to stable. A
curtain based approach by no means requires having a layered architecture or
thinking in that manner. The fact that a curtain is malleable and artificial
means we can define its boundaries, but a curtain becomes a slice when it
overlaps too many real edges in a system. Why is this any better than a sliced
approach?</p>
<p><strong>Curtains buy you breathing room.</strong></p>
<p>There is always an implicit countdown when you keep things stable but don't make
progress. &quot;Where is the business value they are adding?&quot; squawks the manager. If
you are making a lot of progress but breaking things the countdown timer is time
to completion but in the face of churn, top-down pressure, peer-pressure, and so
on. <strong>Breathing room gives you space to think. Space to think means you can buy
yourself breathing room. This helps build healthy systems with reduced
complexity and healthy systems means higher rates of progress and
experimentation.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold: Time Estimates</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/fools-gold-time-estimates.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold-time-estimates.html</guid>
      <pubDate>Tue, 15 Oct 2019 19:02:35 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It would be fantastic if we knew the future. With that knowledge we could plan
with the utmost precision. But we are not clairvoyant. We actively exercise a
process of guesses we dress up in the fancier name of &quot;estimates&quot;. Enrico Fermi
would feel these guesses are fine so long as you are <a href="https://en.wikipedia.org/wiki/Back-of-the-envelope_calculation">within an order of
magnitude</a>. This
form of educated guess is also known as a &quot;back of the envelope&quot; calculation and
most random guesses are not doing anything near this <a href="https://www.wired.com/story/how-to-get-better-at-back-of-the-envelope-calculations/">level of
rigor</a>.
Back of the envelope calculations take rough approximations, simplified
assumptions, and various tidbits of top-of-the-mind knowledge to calculate a
<a href="https://en.wiktionary.org/wiki/ballpark_figure">ballpark figure</a>.</p>
<p>Put yourself in the shoes of a manager of a team and imagine asking each person
in this team &quot;how long do you think this task will take to finish?&quot; This could
well be <em>any</em> type of task. Estimations may sometimes vary wildly and sometimes
group around a certain value. Any kind of grouping is a coincidence. Asking the
same person about an estimate even the next day may yield different results. How
could you help improve the accuracy of these estimations with such fluctuating
results?</p>
<p>One option is to decide on all the work you expect upfront. If you know how work
is subdivided <em>exactly</em> you'll know, transitively, how long the total task will
take, right? Wrong.</p>
<p>Instead you'll starve people of autonomy around how they can tackle a goal.
Starving autonomy means people stop thinking and now you'll have to work even
harder to keep everyone productive. It also makes people unhappy, whether they
realize it or not, and you'll probably wind up with a great deal of turnover
because of it.</p>
<p>What about using time taken from similarly sized tasks? If it took someone a
certain number of time to complete a task with a rough &quot;t-shirt size&quot; then it
ought to take another, or the same, person roughly the same, right? Wrong.</p>
<p>Even if you had the same person doing the same task there is the possibility
that some spontaneous act can change timings drastically. People get sick. Their
dependents and partners get sick. Trains get delayed and vehicles break down.
The human brain decides to watch a video for an hour instead of fifteen minutes.
A meeting that was scheduled for an two hours only takes one.</p>
<p>We tend to attach degrees of confidence to our guesses but never seem to discuss
those confidence levels openly. We also tend to wrongly consider <a href="https://blog.codinghorror.com/how-good-an-estimator-are-you-part-ii/">tapered
ranges</a> as
the more accurate guess. We feel pressured to pick the most likely range and
make it a promise. <strong>Resist this temptation and try making this range of
possibilities explicit.</strong></p>
<p>Many people focus on what kind of confidence they back on a single range, but
our confidence may differ between slices of respective intervals of time. <strong>Try
to consider that your confidence in your guesses is not a normal distribution
centered around a single range; Confidence may be distributed in any number of
ways.</strong> There may be a high probability that the job gets finished if we focus
on it at the beginning and the end of the year, but a low probability otherwise.
If we have the time to work on this task on the fortnight's the probability goes
up, but the likelihood of completion dips on the weeks in-between.</p>
<p>Does a low probability mean impossibility? No. Nor does a high probability imply
absolute certainty. A range of probabilities discusses the full spectrum of what
might be feasible. It is better to know you think a project might take only a
months work of time all the way up to six months than it is to merely work under
the assumption that the single month would do.</p>
<p><strong>Make back of the envelope calculations by decomposing problems into
constituent parts.</strong>. The refinement of the accuracy of these parts refines the
overall estimation. This isn't to say breaking down a random guess into several
random guesses will improve accuracy. In fact, with the random-guessing approach
you will probably be reluctant to go below a certain lower-bound, which means
that a decomposed guess might far exceed the original guess. Back of the
envelope calculations try to tie you to real facts, although possibly
simplified, without believing the hype that you can estimate down to the minute
based on prior similar scenarios (see above). Machine learning won't save you.
The great thing about back of the envelope calculations is that they work
equally well for both high- and
<a href="http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html">low-level</a>
concerns.</p>
<p><strong>Pretending estimates given on work are guarantees is fool's gold.</strong> Push back
on demands for promises when you know you are only making a guess. We might get
better at making guesses with time by practice and research but a guess is still
a guess, educated or not.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How Fast Can You Take Your Time, Kid?</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/how-fast-can-you-take-your-time-kid.html</link>
      <guid>https://justanotherdot.com/posts/how-fast-can-you-take-your-time-kid.html</guid>
      <pubDate>Wed, 09 Oct 2019 20:38:58 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is nothing wrong with chopping wood and carrying water. Hard work
certainly pays off. Hackers are notorious for <a href="http://threevirtues.com/">revelling in
laziness</a> but laziness has a stigma of the lazy party
not <em>being enough</em>. <a href="https://en.wikipedia.org/wiki/Laziness">Wikipedia</a> states,</p>
<blockquote>
<p>[laziness] may reflect a lack of self-esteem, a lack of positive recognition by
others, a lack of discipline stemming from low self-confidence, or a lack of
interest in the activity or belief in its efficacy.</p>
</blockquote>
<p>I'm not sure I think laziness is the right virtue for what hackers claim it
stands for. I'm not sure I think laziness is right for me.</p>
<p>Taoism has this idea called Wu Wei <a href="https://en.wikipedia.org/wiki/Wu_wei">&quot;a concept literally meaning &quot;inexertion&quot; or
&quot;inaction&quot;&quot;</a>. Wu Wei implies that going
against one's nature is an act of exertion. A tree has Wu Wei as it grows; there
is no thinking or straining towards the new shape but simply an act of <em>being</em>,
of following it's instincts towards the light and water.</p>
<p>William S. Burroughs wrote an essay called 'Do Easy' and Gus van Sant <a href="https://www.youtube.com/watch?v=eoOUBETTyMI">recorded
a video about it under the same
name</a>. The Discipline of Do Easy
embodies much, in my eyes, that is Wu Wei.</p>
<p>There is sometimes an illusion of progress through lots of pull-requests,
updated dependencies, rushing to complete features, and so on. Discomfort and
pain are nature's way of telling us that we are growing. It is the same when
your brain hurts when you study as it is when you exercise at the gym; growth is
a constant cycle of compression and decompression. How do we pair this cycle
with the idea of Wu Wei and Do Easy?</p>
<p><a href="http://www.bopsecrets.org/gateway/passages/chuang-tzu.htm">The Parable of Cook
Ting</a> is my favorite
Taoist fable. In it, an emperor eats a meal that touches him deeply; someone who
cooks a meal like <em>that</em> must know a thing or two about life! And off he goes to
beg the chef to share his secrets. The cook replies:</p>
<blockquote>
<p>What I care about is the Way, which goes beyond skill. When I first began
cutting up oxen, all I could see was the ox itself. After three years I no
longer saw the whole ox. And now ‚Äî now I go at it by spirit and don‚Äôt look
with my eyes. Perception and understanding have come to a stop and spirit
moves where it wants. I go along with the natural makeup, strike in the big
hollows, guide the knife through the big openings, and following things as
they are. So I never touch the smallest ligament or tendon, much less a main
joint.</p>
</blockquote>
<p>he goes on to talk about his tool of the trade,</p>
<blockquote>
<p>A good cook changes his knife once a year ‚Äî because he cuts. A mediocre cook
changes his knife once a month ‚Äî because he hacks. I‚Äôve had this knife of mine
for nineteen years and I‚Äôve cut up thousands of oxen with it, and yet the
blade is as good as though it had just come from the grindstone. There are
spaces between the joints, and the blade of the knife has really no thickness.
If you insert what has no thickness into such spaces, then there‚Äôs plenty of
room ‚Äî more than enough for the blade to play about it. That‚Äôs why after
nineteen years the blade of my knife is still as good as when it first came
from the grindstone. However, whenever I come to a complicated place, I size
up the difficulties, tell myself to watch out and be careful, keep my eyes on
what I‚Äôm doing, work very slowly, and move the knife with the greatest
subtlety, until ‚Äî flop! the whole thing comes apart like a clod of earth
crumbling to the ground. I stand there holding the knife and look all around
me, completely satisfied and reluctant to move on, and then I wipe off the
knife and put it away.‚Äù</p>
</blockquote>
<p><strong>It isn't to say that discomfort or pain won't come. Overexerting yourself when
a task needs far less effort is wasted effort.</strong> It damages you and your tools.
You and your mind are the knife. You are the chef. Many times we jump to hacking
away tirelessly when a no-code solution is right there in front of us. When code
is required finding the toil and automating it away alleviates wasted effort
from repetition you don't need to make.</p>
<p><strong>Work in a way so you never need a grindstone.</strong> Growth is no different. Exercising
without any rest means there is no time for your body to recover and build new
muscles. Sleeping on a subject you've been studying allows your subconscious to
forge new connections which is the essence of learning. <strong>There are healthy ways
to grow as there are healthy ways to work. Don't be the machine.</strong></p>
<p><strong>Keep your eyes on what you're doing, be patient, and move with the greatest
subtlety until the whole thing crumbles before you.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert.html</guid>
      <pubDate>Mon, 07 Oct 2019 21:43:52 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is one thing that ties all forms of testing together; <strong>assertions</strong>. The
lowly assert humbly serves whether it's as types, panics, automated tests, or
any other glorious form. Regardless of how it manifests itself, it allows us to
declare things about our systems or program and automatically check them.</p>
<p>But when people test they don't tend to think about what they are asserting.
I've met a great number of people who are taught testing as a mechanical
practice, one that is simply followed because of the social expectation that a
tested system is a 'correct' system. But what is correctness?</p>
<p>Correctness is not merely the absence of bugs. <strong>Correctness is the assurance
that a system is doing as is intended.</strong> This can be business logic or even
sterile concerns like if a function returns the right value given the right
inputs (forms of unit tests). It can be about output or generated content
looking the way it's supposed to look (snapshot tests). It can be about multiple
systems behaving when coupled (integration tests) or about whole flows of usage
(end-to-end tests or possibly contract tests). The things we are testing <em>for</em>
and the ways to test for them is vast.</p>
<p>It helps to think about blocks of computation as black boxes: inputs go in and
outputs come out. Assertions that need to be upheld,</p>
<ul>
<li>while things are happening inside of the box are called <strong>invariants</strong></li>
<li>before the box starts work are called <strong>preconditions</strong></li>
<li>after the box has finished work are called <strong>postconditions</strong></li>
</ul>
<p>There are also a number of general properties the box can uphold:
<a href="https://en.wikipedia.org/wiki/Involution_(mathematics)">involutivity</a>,
<a href="https://en.wikipedia.org/wiki/Idempotence">idempotence</a>,
<a href="https://en.wikipedia.org/wiki/Partial_function#Total_function">totality</a>, etc.
The specifics of each of these isn't important but the idea is that there are
reusable patterns for guarantees we can wish from our systems and programs.</p>
<p>This article is the start of many to describe how the varying forms of
assertions lines up with their respective forms of testing. There are even
meta-principles at play about asserting facts about systems that we should make
elicit in the hopes they better our testing in general. These explorations
aren't going to be exhaustive but I am hoping they help expand your mind in the
things you can ask your code enforce.</p>
<p>A quick journey and recap, if you will.</p>
<p>When you write a program, you might use a typed programming language. In this
case you can use types to encode facts about your problem domain and structure
of data. <a href="https://blog.janestreet.com/effective-ml-revisited/">With types we can help make illegal states
unrepresentable</a>.</p>
<p>Later, you are writing a program and you want to know it acts the way you are
expecting it to act. Compilation non-withstanding you start to run the program
and check the results manually. <a href="https://landing.google.com/sre/sre-book/chapters/automation-at-google/">But this sort of tedium is easily
automated</a>.
<strong>Toil should infuriate you!</strong> With this sentiment in mind you start writing a
program to run your program in different circumstances, hence automated testing
is born. Now that you have this tool in place, you can run tests on small things
all the way up to big things. When the assertions in question fail, the tests
fail.</p>
<p>When a system misbehaves, you might want to know immediately while you are
coding and what faster way to know than to have your program halt when an
assertion is not met. Perhaps a failure is even one which requires a process to
abort while running in production (a fatal error). The difference between these
two is the subject of recoverable versus unrecoverable errors, which I won't
indulge in here, but it suffices to say catching mistakes and misunderstandings
sooner is always better than later by <a href="https://www.cs.tufts.edu/%7Enr/cs257/archive/jon-bentley/correct-programs.pdf">attaching these sorts of assertions to
forms of
panics</a></p>
<p>Now your test suite tests both small and large. As these tests get more
complicated, assertions can be about <em>models</em> of these systems; as state
machines or even where the inputs are generated randomly. <a href="https://www.youtube.com/watch?v=hXnS_Xjwk2Y">Property based
testing</a> starts joining your
repertoire for this reason. For verifying raw memory access you consider
<a href="https://en.wikipedia.org/wiki/Fuzzing">fuzzing</a>. Perhaps the end-to-end tests
are brittle and always breaking which might lead you into <a href="https://docs.pact.io/">contract
testing</a> two systems to ensure that the pre- and
post-conditions (read: the contract) are being met. Maybe there are extremely
complicated concerns such as concurrency and you write a
<a href="https://en.wikipedia.org/wiki/Formal_specification">specification</a> in something
like TLA+ which can verify the model it describes as part of the tooling.
Specify the system or program abstractly and test that, instead.</p>
<p>Like anything, there are diminishing returns. Finding assertions everywhere
doesn't mean proving your TODO-list single page application with a theorem
prover or dependant types is worth the time, although if those processes were
more lightweight it <em>would</em> probably be worth it! <strong>Think of assertions as bets
that pay off when code is introduced that violates them.</strong></p>
<p>Systems come in all sizes but despite their mixed formats they are all guided by
principles. <strong>Instead of thinking the path to correctness is forged by
mindlessly coding and churning out fixes, try to think about the properties you
want upheld, instead, and work to encode those in every possible assertion you
can leverage within reason.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How I Git</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/how-i-git.html</link>
      <guid>https://justanotherdot.com/posts/how-i-git.html</guid>
      <pubDate>Mon, 07 Oct 2019 20:02:46 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I thought it might be worth having a look at two things <code>git</code> allows I've abused
to remove some warts and toil from my day-to-day flow.</p>
<p>One thing <code>git</code> does is alias support. Anything under the <code>[alias]</code> key in ones
<code>$HOME/.gitconfig</code> is treated as a valid subcommand. This is fine for quick
things, like <code>r</code> as <code>rebase</code> or <code>a</code> for <code>add</code>, but you can also alias one-line
scripts, for example, here's a snippet from my <code>.gitconfig</code>.</p>
<pre><code>  it = &quot;!f() { git fp &amp;&amp; git r origin/master; }; f&quot;
</code></pre>
<p>This demonstrates defining an ad hoc shell function named <code>f</code> and calling it
immediately. What's notable about this is that it is <em>also</em> calling a <code>git</code>
alias. <code>git fp</code> in this case is an alias for <code>git fetch --prune</code> and <code>r</code> we've
already mentioned is <code>rebase</code>, so this, verbosely, is,</p>
<pre><code>$ git fetch --prune &amp;&amp; git rebase origin/master
</code></pre>
<p>Another thing <code>git</code> let's you do is invoke arbitrary scripts that are named in
the format <code>git-name</code>. If the script is on the path, you can call <code>git name</code> and
the script, <code>git-name</code>, will run. My old process for pushing to a branch I had
authored was a bit verbose,</p>
<pre><code># on first push
$ git push -u origin/master current-branch

# afterwards ...
$ git fetch --prune

# and, after hacking, changes both behind + ahead on branch (rewritten history)
$ git push --force-with-lease

# or, if simply, without any `--force*` flag
$ git push
</code></pre>
<p>I wrote a script that does all of this, automatically, called
<a href="https://github.com/justanotherdot/gits/blob/master/scripts/git-p"><code>git-p</code></a>,
which lets me call <code>git p</code>. It's doesn't work for all corner cases, and could be
extended to, but this fits ninety-nine percent of my use case. This worked well
for awhile, but I needed to build on it. I eventually wrote an alias called <code>git up</code>,</p>
<pre><code>  up = &quot;!f() { git it &amp;&amp; git p; }; f&quot;
</code></pre>
<p>The point of <code>up</code> is to ensure my changes are always rebased on master before I
push. This is pretty handy but I've recently added yet another alias called
<code>raise</code> (also aliased as <code>pr</code>),</p>
<pre><code>  raise = &quot;!f() { git up 2&gt;&amp;1 | awk '/http/ { print $2 }' | xargs open; }; f&quot;
</code></pre>
<p>This scrapes out the remote output with the PR creation link that GitHub
provides after a branch is first pushed to the remote repository and funnels it
into <code>open</code>. MacOS X has <code>open</code> as the default way to open mime-type related
files to respective 'default' applications. On linux, where I use the gnome
windows manager, I have the shell alias,</p>
<pre><code>$ which open
open: aliased to xdg-open
</code></pre>
<p>to try to bridge the gap, which just goes to show aliases and scripts that use
this same format can be really handy for hiding away toil! I don't know if it's
ideal for all CLI tooling but I think this approach is certainly an interesting
approach to let people slip in their own functionality and 'rewire' an interface
to better suit their needs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Actually Using Git Worktrees</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/actually-using-worktrees.html</link>
      <guid>https://justanotherdot.com/posts/actually-using-worktrees.html</guid>
      <pubDate>Sat, 05 Oct 2019 13:48:54 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Let's say you are expected to do code review and you are also expected to code.
When you do either a certain set of changes is in place. Switching because you
are blocking someone means you <em>have</em> to do a dance with stashing changes,
checking out a branch, perhaps cleaning temporary files, restarting tooling,
etc. Bar changing your codebase, workflow, and job requirements, here's an
approach that uses <code>git</code> <a href="https://git-scm.com/docs/git-worktree"><code>worktrees</code></a> to
ease the cost of these context-switches.</p>
<p><code>git</code> uses <code>worktree</code>s to track changes in a repository. <code>git</code> gives us the
ability to make more than one <code>worktree</code> at a time that are checked out to
potentially different sets of changes. This means we can effectively split up
our codebase into review and development environments:</p>
<pre><code>$ git worktree add ../foo-review --checkout master # where `foo` is the name of your project
$ cd ../foo-review
$ git clean -fddx
$ git checkout branch-name
</code></pre>
<p>You can tuck the change-directory code into a script if there's a slew of other
steps needed to get into a good-known state. If you are using GitHub, here's an
added bonus for checking out pull requests by number rather than by branch name:</p>
<pre><code>[alias]
  &lt;snip&gt;
  copr = &quot;!f() { git fetch origin pull/$1/head &amp;&amp; git checkout pr/$1; }; f&quot;
  &lt;snip&gt;
</code></pre>
<p><em>N.B. There are alternative ways of <a href="https://gist.github.com/piscisaureus/3342247">fetching all remote pull requests from
GitHub</a> which might be preferable
to the above alias.</em></p>
<p>GitHub assigns this special remote tracking branch to your PR, but it's
read-only so if you want to contribute changes you will need to know the name of
the original branch.</p>
<p>With this setup the context-switch dance is reduced. The workflow could be like
this:</p>
<ol>
<li>Someone asks for a review or perhaps you're done and want to get back to work</li>
<li>Calling <code>work</code> might get you back into your development environment where you
left off</li>
<li><code>review branch-name</code> will go the other direction preparing the pull-request
for inspection</li>
</ol>
<p>Git aliases are a neat way to remap the surface area of <code>git</code>. I actually think
this is a utility for configuration I don't see more CLI tooling using that
probably could to great effect. In the context of <code>git</code> it allows me to get
around some particular ergonomic warts. Also, I don't do this workflow anymore
as I leverage pull-requests largely for communicating changes more than
gate-keeping these days, but I understand not all circumstances are the same.
Worktrees could be used to keep a reference implementation around for quickly
inspecting without having to switch branches, for example. Little things like
this that help reduce toil are worth their weight in platinum so it pays to keep
your eye open to automation opportunities!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold: Code Coverage</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/fools-gold-code-coverage.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold-code-coverage.html</guid>
      <pubDate>Wed, 02 Oct 2019 06:41:51 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you are unfamiliar with code coverage, the idea is simple: you write
accompanying tests to code and a code coverage tool produces reports of lines
covered by tests and the percentage of that coverage to all lines of code. The
hope is that a higher coverage with tests means you'll have a 'correct' system.
I have even heard of some establishments initiating quotas on required coverage
per lines of new code being introduced. &quot;If it doesn't have tests it doesn't
exist&quot; is the usual argument for this requirement; code without tests is
potentially problematic code, but tests are also <em>untested chunks of code</em> in
our codebase. For example, consider this bit of React code:</p>
<pre><code>test('Breadcrumb renders', () =&gt; {
  expect(() =&gt; {
    &lt;Breadcrumb/&gt;
  }).not.toThrow();
});
</code></pre>
<p>What is this testing exactly? Literally any other test, even one without the
<code>toThrow</code> expectation, would mark this as a failure on an exception being
thrown. This will light up code coverage though. People learn to cheat the
system, or please the percentage going up, and focus less on the guarantees that
tests are providing them. <strong>This does not help us deliver better products to end
users.</strong></p>
<p><a href="https://twitter.com/KentBeck/status/812703192437981184">Code coverage percentage is a useless
metric</a>. There is no way
to know what percentage of code written is code your end users actually care
about when that percentage is derived from synthetic traffic. You may write a
continent of code, but only a thousandth of that code may actually be hit by
users. If you are using code coverage to tell you that you have greater than
zero percent code coverage than you have a code organization issue or there is
the possibility that many or all of your tests are false positives.</p>
<p>Here's a different approach: instrument your application to track invocations of
code paths. You can do with this tracing, structured logging, profiling replayed
traffic, etc. The technique employed doesn't matter but what does is determining
what is valuable and what is dead. Regardless of collecting metrics, you will
always need to consider this <a href="https://kentcdodds.com/blog/how-to-know-what-to-test">from a human
perspective</a>.</p>
<p>Detractors may argue that code that doesn't immediately show usage should not be
hastily deleted. They are partly right. If things are early on at your company,
doing eyeball statistics may be fair but eyeball statistics is not real
statistics. Practicing some basic statistical understanding is always in order
for any kind of analysis. It may take time to reach a statistically significant
result and whether one is reached should, more often than not, drive your
decisions. As noted we need to exercise judgement despite what the numbers may
tell us. Perhaps the piece of code in question is a critical piece of error
handling that is rarely executed, for example, or maybe the code is serving a
particularly infrequent, but high-paying, user base.</p>
<p>I'd like to stress that I am <strong>not</strong> saying that testing is a pointless errand.
What I am saying is that <strong>code coverage is fool's gold</strong>. Tests take more
effort to create because developers must check that a test is actually testing
what you think it's testing. <a href="https://stackoverflow.com/questions/153234/how-deep-are-your-unit-tests/153565#153565">Testing is a part of one's confidence
level</a>
in what they ship, and when we assert important properties of a system are
upheld you boost that confidence level. <strong>Don't buy into the idea that coverage
is going to lead to a correct system by default.</strong> Be vigilant with you tests
and instrument your application.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Errors Across a Boundary</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/errors-across-a-boundary.html</link>
      <guid>https://justanotherdot.com/posts/errors-across-a-boundary.html</guid>
      <pubDate>Fri, 27 Sep 2019 21:10:01 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is a line across our systems we shall call the boundary. On one end of the
boundary are the consumers and on the other side are the providers. This
boundary is what we are accustomed to calling an interface. Interfaces are the
embodiment of the dance needed to cross the boundary. The interface may have
adapters on either side whose purpose is to munge details of the internals into
this known language of communication. This way internals can continue working
without the fuss of the protocol driving their decisions.</p>
<p>Things go wrong. But when they do developers tend to clump everything up as a
single form of error. Errors are about reporting mistakes or complications. A
better name compiler writers have been using for years is 'diagnostics'; they
should help diagnose a particular problem by being part of the symptoms an
ill-behaving service might demonstrate. <strong>As such, when an infraction occurs you
want to know who is the offending party. <em>Are we holding it wrong or are you?</em></strong></p>
<p>Borders on your errors make clarify what the fix is by knowing who should be
performing the correction. This might mean the speech of diagnostics changes. A
person hacking on some code is much more accustomed to cryptic messages from a
compiler than an average person using a web interface to access their bank who
doesn't understand how any of this is rigged up.</p>
<p>Clearly delineate your errors and you'll know better if something is a mistake
or a matter of environment, if it is something a maintainer needs to worry about
or a blunder from usage. There are many styles to error handling but this
approach does not impact which style you end up using. You can use this whether
it is error codes, exceptions, or values. This is a matter of organisation.</p>
<p>Systems architecture itself can largely be seen as a form of organisation.
Conceptual partitions, domain concerns, and our pursuit for pieces that compose
drive this organisation. Independent the layer of granularity the focus on
organisatoin is always the same. <strong>When things go wrong, tell everyone which
side of the fence the mistake or complication came from.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold, An Introduction</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/fools-gold.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold.html</guid>
      <pubDate>Sun, 22 Sep 2019 15:37:09 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Developer's are a big target for what I call &quot;fool's gold&quot;. It's the hope that a
piece of tech can solve all of our problems that keeps us going with the bait of
new tech. Solutions tempt despite us knowing better. An experienced software
developer realizes that <em>everything</em> has strengths and weaknesses which we call
&quot;tradeoffs&quot;, but plenty of developer's don't realise this yet or are in denial.
This article is an introduction to the concept that plenty software and services
are sold as panacea but anything sold as panacea should be considered with
caution.</p>
<p>Let us discuss two ends of an argument first. There is the camp of <a href="http://boringtechnology.club/">choosing
boring tech</a> and <a href="https://www.intercom.com/blog/run-less-software/">running less
software</a>. This camp says that
cognitive load and operational costs are distractions for teams whose primary
focus ought to be the product they are building that makes them profit. By
running less software you are curbing the desire to have a bijective mapping
between problems to solutions where the relations are each their own distinct
solutions. Think about it this way, if you have N many devs and M many distinct
solutions for your problems, you have three particular cases to consider</p>
<ol>
<li><code>N &gt; M</code>: devs can't be experts except for some subset of the total pool of
technology. Ditto (and more importantly) maintenance and operations. More
than one dev has to be allocated per tech to make this work (pigeon hole
principle).</li>
<li><code>N = M</code>: every dev can own a particular piece of tech and grow with. Devs may
get bored and want to congregate on other pieces of tech. If this happens you
wind up with <code>N &gt; M</code>, effectively.</li>
<li><code>N &lt; M</code>: Devs can congregate around pieces of tech without much fuss. They
have freedom to pick what they like most (within a certain degree depending
on the delta <code>M-N</code>). Maintenance and operations is bearable as the whole team
can participate and not have to spin plates.</li>
</ol>
<p>Then there is the camp of constant agitation. A company goes under if it's not
constantly pushing to optimise for end users and reducing costs. <a href="https://www.goodreads.com/book/show/28592994-simplify">Some even
argue you can only pick one of these two optimisations</a>. Enterprises claim to
avoid this complication because their golden goose is sitting pretty, but the
reality is that any revenue generating organisation has to constantly push
themselves into the future to compete. Technology is an enabler, it allows teams
to move faster by automating away toil, easing collaboration friction, and a
productive team means they can, hopefully, deliver user experiences that delight
or at a cost that is beats the competition.</p>
<p>All of this shuffling around comes at the cost of churn and bloat. Companies try
to sooth this issue by either by hiring more devs and/or performing lots of
migrations. <a href="https://en.wikipedia.org/wiki/Software_rot">Software rots</a>, which
can mean different things to different people, but I see it as the eventual
ineffectiveness of a piece of software as improvements are found in competing
solutions. That is to say, even though your software may not actually be getting
slower, it will definitely feel slower in the context of all neighboring
solutions getting faster. This is but one example yet other things like security
exploits, support for a particular version of a language or library, and so on
all work to disempower your application or system.</p>
<p>The reality is that we cannot simply pick one camp to be part of as professional
developers. We are paid to help companies continue to live and be better than
they were before we joined, all within the confines of the ethics and legalities
we are bound to. <strong>We cannot sit still but we can't move too much!</strong> Some have
suggested things like a <a href="https://www.shimweasel.com/2018/08/25/novelty-budgets">novelty
budget</a> to support
keeping the platform largely stable while pursuing new ways of handling
constantly arising issues.</p>
<p>This article isn't meant to attack particular companies or pieces of tech or
practices despite those practices that carelessly hold on to the past or
endlessly throw it out for the new. The core intent is to encourage critical
thinking and consideration of tradeoffs when problem solving. Weigh your
options! You cannot make decisions purely by reading and watching, nor can you
make a choice by trying everything as there simply isn't the time. Certain devs
learn to do cheap experiments at work or at home but this can have the pitfall
of comparing a toy project a success that may not handle the scale of an
industrial grade application. This sort of scrutiny is part of the weighing
process.</p>
<p>If anyone tells you they're going to make you rich, they're getting rich off of
you. <strong>Take marketing with a grain of salt and throw out the fool's gold!</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Pushing the Boulder</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/pushing-the-boulder.html</link>
      <guid>https://justanotherdot.com/posts/pushing-the-boulder.html</guid>
      <pubDate>Thu, 19 Sep 2019 20:15:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Imagine you have a giant boulder in front of you. This is the task you want to
undertake. You know you need to push it to get it moving but you also recognise
that the inertia you'll need to overcome at first is substantial. You
unfortunately think that pushing the boulder will <em>always</em> take this much force.
The truth is, if you can get the boulder moving, keeping it in motion only takes
many small, infrequent pushes. The idea behind pushing a boulder is the same as
<a href="https://jsomers.net/blog/speed-matters">fast systems incurring usage</a>. Want to
improve your coding? Code. Want to read more? Read. <strong>Action begets action</strong>.</p>
<p>When you try to acquire knowledge there is always a pool of unknown-unknowns
whose size is, itself, infinite. We are constantly refining things from the
unknown-unknown pool into known-unknowns while we learn. One way to acquire
knowledge is to read. I recently joked on twitter about speed reading:</p>
<blockquote class="twitter-tweet">
  <p lang="en" dir="ltr">
    Here‚Äôs how you speed read: flip through a book and tell people you‚Äôve read it.
  </p>
    &mdash; Ryan James Spencer (@_justanotherdot)
  <a href="https://twitter.com/_justanotherdot/status/1170118831219474433?ref_src=twsrc%5Etfw">
    September 6, 2019
  </a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>But in all fairness, this is a sensible approach. Some relate knowledge
acquisition as panning for gold. I think that's a misguided analogy because
panning for gold is <em>slow</em>. Yes, we want to find the gold and we're not quite
sure where it is, but no modern mining outfit would pan for gold these days.
They would use large machines that could tear, demolish, crush, rip, sift, sort,
detect, and on and on in orders of magnitude time more quickly than someone
holding a pan to a river.</p>
<p>There is a technique involving multiple passes of reading. I learned this first
when I got into <a href="https://www.elsevier.com/connect/infographic-how-to-read-a-scientific-paper">research
papers</a>.
It's also the basic idea behind <a href="https://www.goodreads.com/book/show/567610.How_to_Read_a_Book">How to Read a
Book</a>. You read a
first pass involving the abstract/introduction, the conclusion, and then you
skim, noticing headlines, captions, diagrams, and all the other top-level items.
You follow suit with other passes increasing in detail if you haven't already
dropped interest in the material. Basically, actions get prioritised by cost,
the lower cost stuff coming first. <strong>Don't pan for gold, mine for it with
machines.</strong>.</p>
<p>Take a book you want to get through. Try to just flip through it. Once you're
done skimming, try to concentrate on the primary points you've just become aware
of; your known-unknowns. In your future passes, try to hone in on the things you
care about more. Reading books out of order is actually fine for a lot of
material if the first pass didn't tell you enough of what you need to know. What
this does is change your definition of done.</p>
<p>There is always signal-to-noise and it's largely why there are diminishing
returns which is both true for what we produce as well as what we consume.
Ruthlessly closing browser tabs to reduce distraction, keeping your output high
and frequent, reading books in multiple pass, and on and on. Do things to put
you in an uncomfortable state. Make tons of little changes and skim content
frantically going back for more when you need it instead of sitting around
dreaming or scheming. Be a giant machine. Tinker like a maniac. Push the
boulder.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Lightweight is Beautiful</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/lightweight-is-beautiful.html</link>
      <guid>https://justanotherdot.com/posts/lightweight-is-beautiful.html</guid>
      <pubDate>Sun, 08 Sep 2019 20:03:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>We are all guilty of having done the &quot;edit a little bit, go to another terminal,
hit the up-arrow a number of times, fire off the found command&quot; dance over and
over again at some point in our careers. It's such an easy automation to remove
these steps! IDEs give this to you because they know best about when a buffer or
a file has been saved or modified. Indeed, people go crazy for IDEs because they
provide information directly in the editor.</p>
<p>Even though things like VSCode and the Language Server Protocol have done a
tremendous amount of work in reducing complexity around both the setup and
maintenance of an IDE environment since days of yore, there are still times when
the array of plugins and external tooling 'go wrong'. Bugs or even the nefarious
'opinionated' feature can cripple a dev's workflow. Fixing these issues isn't
necessarily time poorly spent but it's hard to shrug off because the integration
is so tight-knit‚Äînow that you depend so heavily on the plugin, switching to
something different is slow. Here's an approach I think is a bit more
<a href="https://www.goodreads.com/book/show/13530973-antifragile">antifragile</a>, to use
a term coined by author Nassim Taleb. An antifragile approach is distinct from a
fragile approach because</p>
<ul>
<li>a fragile approach will break when encountering an unexpected event and</li>
<li>a robust approach does not change when encountering an unexpected event but</li>
<li>an antifragile approach gets better as it encounters unexpected events</li>
</ul>
<p>I'm a bit spartan when it comes to coding. I do this largely because I've had a
lot of tooling mistreat me and this has taught me that the weight of a tool or
process is a matter of its cost. <strong>Lightweight is beautiful</strong>. By lightweight we
mean cheap to replace not 'small' and 'simple'. Sometimes you do need beastly
machines because you can't bore a hole into the earth to make a tunnel with a
few workers armed with spoons. <strong>Lightweight functionality is preferable to
mindless adherence to a given tool or process.</strong> In other words, it's
antifragile to be prone to lightweight .</p>
<p>So here is the setup; two terminals or windows or whatever you like to use. In
one is your source code and in the other is your tests, linting, typechecking,
you name it. Either they are side-by-side or perhaps there is a dead-simple way
for you to swap between them. You can have several of these going at once and in
fact I recommend it. If they are resilient to files changing from version
control that's even better. <strong>It's important they stay <em>relevant</em> and by that I
mean obvious and up-to-date.</strong> When we talked about
<a href="https://www.justanotherdot.com/posts/stdout_is_forever.html">debugging</a>, this
is the very loop I was referring to. With this in place you can progressively
slap in debugging statements and changes while watching the results come seeping
out.</p>
<p>There are plenty of testing frameworks and tools that support automatically
running tests or commands on file save. <code>jest</code>, <code>PyTest</code>, <code>cargo watch</code>, <code>go watcher</code>, <code>mix watch</code>, you name it. This sets up an automatic link between the
file(s) being edited and the suite of tests to run. Just alleviating the step
where you need to context switch is the small win here and is not the point.</p>
<p>With this approach, if anything like a plugin or even a specific command in the
pipeline you setup goes awry, you can cheaply swap it out for an alternative.
<strong>This is the best kind of feedback loop as it favours tinkering and
experimentation.</strong> Lately because I mostly write Rust at work, I tend to use
<code>cargo watch</code> but one incredibly handy, language agnostic tool is
<a href="http://eradman.com/entrproject/"><code>entr</code></a> which is useful when I foray into the
unknown or uncommon. Let's say I find that I need to run a particular pipeline,
I can do that by running,</p>
<p><code>rg -l . | entr -cs 'cmd1; cmd2; cmd3'</code></p>
<p>Now if <code>cmd2</code> is being a pain, I can take it out of the pipeline quickly or even
choose to replace it. Perhaps it's a new project and you are furiously adding
files, you could set up a governing loop that watches all files and tears down
the loop if that changes some known set.</p>
<p><code>while true; do ls src/* | entr -d cmd; done</code></p>
<p>Most people never even think of doing a <code>git bisect</code> because of the pain of
steering the interaction with the bisect and running the tests to confirm the
first failure in the regression suite. This isn't just the cost of swapping
between terminals. Sometimes it can be tests that are flaky and come up as false
positives or maybe a test suite is slow to run but there is no way to neatly run
a subsection without commenting out code. With this approach, however, we can
focus on the steering and watch what happens in the other window. If flaky or
slow tests show up, we can comment them out and move on (<code>git clean -fdxx</code> is
handy for these sort of tempermental changes if you tack on it on the back of
the pipeline you construct).</p>
<p>If a great style guide favours <a href="https://www.justanotherdot.com/posts/a_plea_for_style_guides.html">deletability and ease of
modification</a>,
this approach is stressing for <strong>replaceability</strong> for producing tinker-friendly,
antifragile feedback loops. If you lower friction you'll always beget action,
and <a href="http://jsomers.net/blog/speed-matters">fast systems incur usage</a>.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Stdout is Forever</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/stdout-is-forever.html</link>
      <guid>https://justanotherdot.com/posts/stdout-is-forever.html</guid>
      <pubDate>Wed, 04 Sep 2019 15:49:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Debuggers are worth their weight in gold but stdout is the diamond in the rough.
All the tools we have to pinpoint problems such as REPLs, automatic tracing,
stacktraces, and even printing to stdout wind up being about two things:
<strong>poking</strong> and <strong>prodding</strong>.</p>
<h2>A useful macro or two</h2>
<p>Rust has the <code>dbg!</code> macro and I love it. It's short enough to type and it shows
you what file you are in, line you are on, and how the code looks plus its
value after evaluation. e.g. <code>dbg!(dbg!(12) == dbg!(1 + 11))</code> will print</p>
<pre><code>[src/main.rs:2] 12 = 12
[src/main.rs:2] 1 + 11 = 12
[src/main.rs:2] dbg!(12) == dbg!(1 + 11) = true
</code></pre>
<p>Two important quirks with this are,</p>
<ol>
<li>No arguments passed means you just get the file and line number</li>
<li>The code still behaves the way it used to except now you have tracing</li>
</ol>
<p>This gives us just enough information to be lethal. This is possible because
this expands at compile time and can be replicated in other languages that have
macro support. This is a source transformation and we can't easily use a
function because our line number will always be the line number of the function,
not the calling site. As such, one option is to write it as some repeated action
in your editor of choice. Imagine you have the following go code in front of
you,</p>
<pre><code>func AddOne(x Int) Int {
  return x + 1
}
</code></pre>
<p>and you want to lay down some tracing so you highlight the <code>x + 1</code> and hit a
keyboard shortcut which transforms the code into the following,</p>
<pre><code>func AddOne(x Int) Int {
  fmt.Printf(&quot;[src/main.go:8] x + 1: %#v&quot;, x + 1)
  return x + 1
}
</code></pre>
<p>We could have also used the
<a href="https://golang.org/pkg/runtime/#Caller"><code>runtime.Caller</code></a> function to get
filename and line number but we can get that spliced in via our editor to avoid
an import. If you are curious what the <code>runtime.Caller</code> code looks like here it
is (and, yes, I'm ignoring error handling here since this is intentionally
throwaway code):</p>
<pre><code>func AddOne(x Int) Int {
  _, file, line, _ := runtime.Caller(0)
  fmt.Printf(&quot;[%v:%v] x + 1: %#v\n&quot;, file, line, x+1)
  return x + 1
}
</code></pre>
<p>The advantage with the above is now we can take our print lines and move them
around at will and we won't have to tweak the filename/lineno combo.</p>
<h2>Poking</h2>
<p>Sometimes the fastest way to get at a problem is by writing test cases that flex
assertions about the functionality in question. Other times that's not as fast
because the logic might rely on other systems, e.g. integration tests. In those
cases, if you have stacktrace support you might find it useful to panic/throw if
particular assertions aren't met. When that fails you are probably interfacing
with code that is covering up exceptions or panics, say a piece of library code
that takes your code as a callback. You could try stubbing in your own forked
version of the code (scripting languages tend to make this easy) or you could
turn to building your own stacktrace. You iteratively apply print statements in
the following fashion,</p>
<pre><code>fn foo() {
  dbg!() # beginning
  &lt;snip&gt;
  dbg!() # middle
  &lt;snip&gt;
  dbg!() # end
}
</code></pre>
<p>With <code>dbg!</code> this is really easy because I don't have to think
about what to pass to the printing function since <code>dbg!()</code> simply
emits the filename and line number. In languages that may not have this I've
done <code>printf(X)</code> where X = &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, and so on.</p>
<p>With this format in place you can use binary search to figure out where you need
to apply more printing statements on each subsequent run. If, however, your
tests or program take a long while to run it can pay to do upfront work but
perhaps limiting yourself to an arbitrary depth to avoid spending too much time
on tracing that won't pay off.</p>
<h2>Prodding</h2>
<p>You can load your <a href="https://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/">core
dumps</a> into
<code>gdb</code> and explore the call stack after a segfault among all sorts of other cool
things that debuggers allow you to do, or you can rig up systems to
automatically provide tracing, such as in <a href="http://erlang.org/doc/man/dbg.html">erlang or
elixir</a> but hopefully this article has shown
that stdout gives you powerful debugging functionality since we already have
access to executing the program and manipulating its source. We can print
assertions to see if they hold up or mess around with alternative solutions that
may work if the problem is clear. Stdout isn't always the fastest but it's
<em>lightweight</em> which makes it invaluable as it can circumvent a lot of
preparatory work. You can pair this approach into a feedback loop, too, to
reduce duplicated work such as running the tests or program over and over again.
In a future article I'll discuss ways to do this in a range of languages and
environments but at least we've set the tone for some thinking about how to
improve what we spit out while you hack to give you a better understanding of
what's going on under the hood.</p>
<h4>Acknowledgements</h4>
<p><em>The name of this post is inspired from <a href="https://twitter.com/bodil/status/878563460233277440?s=20">Bodil
Stokke</a> when
responding to what &quot;What are everyone's fave debugging tools for languages you
write code in?&quot;</em></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Principles</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-principles.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-principles.html</guid>
      <pubDate>Sun, 01 Sep 2019 18:43:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Programmers make assertions all the time with a wide range of utilities: tests,
types, written prose, and so on. Assertions are how we establish some ideal of
the system at any particular scope. Maybe there is a requirement that all
services in an ecosystem do not rely on each-other's internal implementation
details or that exceptions should not be thrown in library code. Perhaps a team
decides that despite correctness being of the utmost importance, it doesn't give
anyone the right to be an asshole.</p>
<p>Ray Dalio <a href="https://www.goodreads.com/book/show/34536488-principles">wrote a whole book on the matter of
principles</a> and feels
they are at the center for decision making and iterative improvement,</p>
<blockquote>
<p>Principles are fundamental truths that serve as the foundations for behavior
that gets you what you want out of life. They can be applied again and again
in similar situations to help you achieve your goals.</p>
</blockquote>
<p>My likening of principles to assertions is superficial. Assertions are, in my
mind, simply the application of principles. We <em>assert</em> that an invariant is
upheld during each iteration of a loop, that some postcondition is still met
after execution of a function, that a system is composable and modular from
applying principles sat around the locus of flexibility. In some cases these are
machine-checked, in others they rely on humans to verify. Regardless of checking
they are still present.</p>
<p>When you have autonomy in a team it is key that you take initiative to achieve
excellence. The problem with initiative is precisely an issue of what to work
on. Principles provide a 'north star' metric that enables you to say &quot;is the
error handling in this code dividing concerns between internal versus external
concerns?&quot; or possibly &quot;do people feel supported and empowered to make the
changes they need ot make across the broader company?&quot; <strong>With principles we can
scrutinize, but also work towards, some ideal. Hence our application of these
ideals is the work we undertake.</strong></p>
<p>One could easily take principles from elsewhere but that would leave us with
little to no understanding of how to form our own principles that best suit our
needs and desires. What is the trick? The process I'm about to describe is
probably not the only way to go about carving up principles but it's one that
has worked for me and others.</p>
<p><a href="https://www.justanotherdot.com/posts/a_love_letter_to_feedback_loops.html">Feedback
loops</a>
are how we take risks and reflect. Experiencing this cycle for long enough means
we start noticing
<a href="https://www.justanotherdot.com/posts/a_love_letter_to_patterns.html">patterns</a>
that can take the form of principles. A principle is really just a pattern that
can describe some ideal clearly. You make failures (by way of decisions), you
learn from failures (by way of reflection), you get principles.</p>
<p>I've been thoroughly enjoying <a href="https://www.goodreads.com/book/show/16158601-turn-the-ship-around">Turn that Ship
Around!</a> and
one of the mentioned mechanisms towards getting a &quot;leader-leader&quot; culture is
changing the focus from avoiding errors to achieving excellence. In this same
manner I think the best principles are formed. When I talked about <a href="https://www.justanotherdot.com/posts/make_a_home.html">treating
your codebase as a home you want to
build</a>, this 'idealised
life' that a home represents is the same as these principles I've talked about
here. In that sense, from a programmer's perspective, excellence is really just
the code and the culture we wish to embody.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Patterns</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-patterns.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-patterns.html</guid>
      <pubDate>Fri, 30 Aug 2019 21:47:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Patterns are how we tend to carve up the world around us. Like <a href="https://www.justanotherdot.com/posts/a_love_letter_to_feedback_loops.html">feedback
loops</a>,
they are everywhere once we start noticing them. Roger Antonsen <a href="https://www.ted.com/talks/roger_antonsen_math_is_the_hidden_secret_to_understanding_the_world">feels that
mathematics is all about discovering
patterns</a>.
When the pattern is discovered, we come up with a language to describe that
pattern and give us a handle on playing with it. When we have both of those we
can start tinkering and playing with assumptions.</p>
<p>Patterns are usually globbed together with abstractions. Abstractions are
usually misunderstood. What people tend to call abstractions are actually
indirections. Abstractions are actually the rough structure or form of an idea
or concept. The abstraction of a program is a specification for the program
itself. I don't think patterns and abstractions are the same.</p>
<p>So how are they useful in your day to day? You may or may not realise you do
what I'm about to describe but I'm fairly certain most of us do this regardless
of us being conscious of it. Becoming more conscious of this process makes it
more powerful, so let's shed some light.</p>
<p>When you are working, there is generally a discovery of patterns. You build a
trust framework around these patterns; if a pattern is useful, you keep using
it. Perhaps a pattern betrays you. Maybe you sit and dabble with it to make it
work again at the same capacity it used to serve you. If you're ruthless enough,
you ditch the pattern entirely and move onto new ones.</p>
<p>Consider a scenario where you are rigging up some code that needs to walk across
a directory and perform an action on every file. You see a third party library
is more ergonomic than the standard library so you use it, instead. If you're
conscious about this as a pattern, you'll trust it and reuse it tirelessly until
it stabs you in the back. It <em>doesn't</em> pay for you to go fiddle around with the
standard library functionality or some other third party library because you'll
be wasting time. This is also why trying to 'save' a forsaken pattern can come
with a high cost. It <em>does</em> pay to generally treat patterns as throwaway in the
spirit of speed. There's tons of these every day, in every language, and even
beyond coding, so you will not run out if you decide a pattern isn't delivering.</p>
<p>As patterns age and you spend some time thinking about them you'll find you can
increasingly discover ways to generalise them further. Patterns are amazing
because they let you turn all the boilerplate into habits which means you'll
have the mental room to solve other problems with deliberate attention.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Feedback Loops</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-feedback-loops.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-feedback-loops.html</guid>
      <pubDate>Thu, 29 Aug 2019 21:28:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Feedback loops are everywhere and they're <strong>awesome</strong>. In essence, whenever
there is some system with inputs, outputs, and some readjustment based on the
outputs of the system, that's a feedback loop. The analysis of the output 'feeds
back' to the input and the system is now different, and hopefully better,
because of it.</p>
<p>what about refinement of the feedback loops themselves? Truly productive people
not only recognise feedback loops but relentlessly modify them so they reach
their full potential. We tend to call this act of refinement as making the
feedback loop 'tighter', although that might not mean making the delay between
some analysis and the adjustment shorter, <em>per se</em>.</p>
<p>For example, you might code and run something like <code>cargo watch</code>, <code>jest</code>,
<a href="http://entrproject.org/"><code>entr</code></a>, et. al. to avoid having to switch to your
terminal, find the test command, and hit enter. Dropping those steps makes the
loop 'tighter' and means you get feedback (the state of the tests) far sooner
(and automatically).</p>
<p>Or perhaps you are a car manufacturer who gets reports from the dealerships
about sales to help you better gauge how many to make next. If you react too
quickly you might wind up producing a surplus right before a lull in the market.
<a href="https://www.goodreads.com/book/show/3828902-thinking-in-systems">Thinking in Systems: A
Primer</a>
discusses this same case.</p>
<p>Refining a feedback loop is really a matter of finding the sweet spot where it
delivers the most value.</p>
<p>If you're mathematically inclined or curious, there is <a href="https://en.wikipedia.org/wiki/Control_theory">control
theory</a> for designing feedback
loops in mechanical and dynamical systems. This <a href="https://www.youtube.com/watch?v=O8xLxNje30M">AWS
re:invent</a> talk gives a good
practical application of the control theory to software with respect to the
creation of S3.</p>
<p>When it comes to feedback loops for humans, we need to take risks to make
failures to learn. We learn more from our failures than our successes, but we
can't make huge failures all the time. <a href="https://www.goodreads.com/book/show/34536488-principles">Ray Dalio's
Principles</a> describes
the process as reaching for goals (taking risk), experiencing failure
(inevitably), learning from the failures (growth), and then upping the audacity
of said goals. This is pretty cool because it means it's not only OK to
experience failure but it's crucial to achieve what we truly want in life. The
sweet spot is experiencing enough failure so as to allow one to come back with
big dreams.</p>
<p><a href="https://increment.com/testing/i-test-in-production/">Charity Majors reminds
us,</a></p>
<blockquote>
<p>It‚Äôs better to practice risky things often and in small chunks, with a limited
blast radius, than to avoid risky things altogether.</p>
</blockquote>
<p>You can't think through everything. Sometimes you'll need to tinker; take bold
risks, make mistakes, refine, and repeat.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Plea For Style Guides</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/a-plea-for-style-guides.html</link>
      <guid>https://justanotherdot.com/posts/a-plea-for-style-guides.html</guid>
      <pubDate>Wed, 28 Aug 2019 20:16:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You commonly hear two particular attributes that drive style guides, and,
subsequently, automatic formatting tools: 'consistency' and 'readability'. The
argument goes that a developer reads a codebase far more than any other
interaction.</p>
<p>Now, you could always take a codebase that has a style and use <a href="https://github.com/antlr/codebuff">machine learning
to generate a formatter</a> to keep things
'consistent' and 'readable'. This would get around the subjective definition of
readability because it's what the team picked through usage. Some feel that a
community driven style guide is ideal because then the codebase's 'readability'
is 'consistent' with the larger ecosystem, so formatting tools should simply be
blindly adopted.</p>
<p>Unfortunately, they are focusing on the wrong thing.</p>
<p>I have read a lot of code. I care about it as a practice and I like teaching
others how to do it, but I don't think it's the right metric for a style guide.
Bar things like the <a href="https://www.ioccc.org/">obfuscated C contest</a>, minified
markup and javascript, and many other mind-melting formats , most code I see is
actually quite 'readable'. Consistency is no better because you can have a style
that is consistently spaghetti.</p>
<p>In truth, <strong>developers change code far more often than they read and write new
code and they sure as hell should be <em>deleting</em> code with a frantically high
frequency, as well, if they aren't already.</strong></p>
<p>About two years back someone mentioned <a href="https://elm-lang.org/docs/style-guide">the elm style
guide</a> to me. The focus on
ease-of-modification <em>for a human</em> was eye-opening. With this mindset, alignment
was pointless. What good would it do a developer to re-align things after making
a change than to simply let them make the change by itself, communicate it
simply to their peers, and get it into master ASAP?</p>
<p>Then, later, another practice I adopted was adding newlines to assignments/let
bindings, taken from a team of brilliant software engineers. Every time I wrote
an <code>=</code> I would hit enter, allowing the name of the thing and its guts to be
distinct. The contents of the variable could be expanded, shrunk, removed
entirely, turned into an error, whatever. The name could be changed to better
suit constantly shifting needs and not highlight the guts in code review. It was
a handy pair.</p>
<p>I found sometimes having stuff as modifiable or deletable meant you would get
the other for free, similar to what people claim about consistency and
readability. In the end, the specific practices aren't important here. What is
important is that gaining momentum and keeping up against inertia is pivotal
when keeping a project relevant and rot-free. And, if you do the same thing
everywhere, you'll inevitably get a 'consistent' codebase, anyways!</p>
<p>Next time you write a style guide, try to think about the sea of changes that
will need to take place and the stuff that will get old and need to die before
you consider your codebase as another magnum opus.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>May You Be The Author of 2^N Programs</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html</link>
      <guid>https://justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html</guid>
      <pubDate>Mon, 26 Aug 2019 20:00:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>The sheer propensity of articles detailing productivity tips for software
developers under the auspices of them becoming better employees is alarming.
What about our mental health? Why not more articles about our productivity from
the point of view of improving our ability to deal with discomfort, burn out,
imposter syndrome, and so on?</p>
<p><strong>Progress is key.</strong></p>
<p><a href="http://jsomers.net/blog/speed-matters">Speed matters</a> because it principally
enables us to make progress with our work and our life. Half-finished projects
weigh on us like mangled fruit on a dying tree. Project deadlines sneer at us
right around the corner and we put unfair stress on ourselves to both reach a
deadline <em>and</em> achieve excellence. <strong>We cannot be happy and, therefore, productive
if we are unable to be flexible. The only way is to accept mess.</strong></p>
<p>The book <a href="https://www.goodreads.com/book/show/187633.Art_and_Fear">Art and Fear</a>
is chiefly about two things:</p>
<ul>
<li>Proliferation and practice are the way to improvement</li>
<li>Successes can only be determined based on one's own history</li>
</ul>
<p>One analogy that is given in the book is a pottery instructor who divides his
class in two. Half of the class will be judged by quantity and the other half by
quality. In the end, it is the quantity group that has the best work through the
simple act of constantly learning from their failures. Reflection and planning
<em>are</em> crucial but the quantity-based group's ability to accept the mess that
comes with failures is what drives their progress. Fantastic! All we need to do
is just constantly crank things out and we'll be masters of our medium in no
time.</p>
<p>Except for some of us mess instills great discomfort.</p>
<p>At the beginning of the year I went to therapy. I was burnt out from work and
other stressors in life. Many sessions later I am diagnosed with Obsessive
Compulsive Disorder, which in some ways was a shock and in others not a
surprise. Eventually, exposure therapy comes up as a tool to help tackle the
anxiety and stress from parts of my affliction. There I was, sitting with my
discomfort, watching it, not trying to solve it nor run away from it, and sure
enough, the discomfort would melt away with time. I started convincing some part
of myself that the discomfort I was dredging up was <em>not</em> based on fact and I
found myself able to do things I had been so slow or entirely unable to do for
ages.</p>
<p><strong>It has dawned on me that sitting with the discomfort of what you produce is part
of learning from the failures.</strong></p>
<p>You <em>can</em> defer some decisions for later and put in rudimentary solutions in the
meantime. You <em>will</em> probably save time by making progress in this manner that
will allow you to revisit those same problems you deferred. Sure, this may not
always be the case for those trapped in <a href="https://cutle.fish/blog/12-signs-youre-working-in-a-feature-factory">feature
factories</a>,
but that's an organisational issue rather than a personal one. You owe it to
your mental health to grow more flexible by accepting mess.</p>
<p>May you the author of 2^N programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Move Fast and Tuck Code Into the Shadows</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html</link>
      <guid>https://justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html</guid>
      <pubDate>Fri, 23 Aug 2019 21:51:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Migrations are a part of life as a dev. They help <a href="https://lethain.com/migrations/">cut down tech
debt</a> but they can be risky. It's always less
risky merging in <em>new</em> and <em>different</em> sets of changes rather than changing code
in-place. This buys you time. <em>You</em> gain the control over the switch granted
switching doesn't adversely affect some shared, mutable store of data.</p>
<p>The <a href="http://sevangelatos.com/john-carmack-on-parallel-implementations/">parallel implementation
approach</a> is
brilliance incarnate; you keep a functional reference implementation and you
copy it as your 'experimental' version whose sole aim is to eventually replace
(and hence become) the new reference. However, Carmack hits a good point,</p>
<blockquote>
<p>It is often tempting to shortcut this by passing in some kind of option flag
to existing code, rather than enabling a full parallel implementation. It is
a grey area, but I have been tending to find the extra path complexity with
the flag approach often leads to messing up both versions as you work, and
you usually compromise both implementations to some degree.</p>
</blockquote>
<p>I am keen to start experimenting more with the Carmack approach, though. Some
things I've already thought about:</p>
<ul>
<li>Having a duplicated directories messes up navigation for a lot of editors and
is unnecessary bloat</li>
<li><code>git flow</code> styled approaches and any vcs-based approach will never work
because it lends into the 'change in place' idea by merging the reference with
the experiment</li>
</ul>
<p>Otherwise, there are many ways to define clear boundaries between the reference
and experimental implementation. The most popular solution out of many is
feature flag services but I recommend switching between whole modules rather
than having a lot of logic caked into modules to check flags. Keeping flags
macro and mutually exclusive is important because it means changes are kept
cohesive and conflict free. One thing people tend to forget about is the
original feature-flag: versioning. In the end it doesn't matter which technique
you employ so long as you can 1. <strong>toggle between changes</strong> and 2. <strong>keep
differences clear</strong>.</p>
<p>You can start something similar to this approach by focusing on leaving written
code in a disconnected state but being aggressive about it finding its way to
master. This is healthy because it will change the incumbent attitude of
&quot;production means done&quot; to &quot;production means refinement&quot;. I like this approach
and do it as often as I can remember to because it means PRs are kept small
(great for code review) and when I finally do want to rig everything up I can
focus squarely on the plumbing, rather than juggling the correctness of the core
implementation <em>and</em> the coupling to the rest of the system.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Make a home</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/make-a-home.html</link>
      <guid>https://justanotherdot.com/posts/make-a-home.html</guid>
      <pubDate>Thu, 22 Aug 2019 20:49:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I'd like to preface this article that analogies are rough comparisons. That is
why, after all, they are analogies. We say that one thing is <em>like</em> another, but
it is not to say they are the same, point-for-point.</p>
<p>The analogies to construction and architecture in software are abundant. We say
that we are 'building' a codebase. We assign people the role of 'architect'. One
camp of people regale <a href="http://www.laputan.org/mud/">The Big Ball of Mud</a>,
esteeming the progress they make as their rationale for constructing a wobblying
shantytown. Another camp sit high in <a href="https://blog.codinghorror.com/ivory-tower-development/">The Ivory
Tower</a>, planning the
sanctuaries that may give their inhabitants stamina as they rest in its glory.</p>
<p>In Architecture of Happiness, Alain de Boton waxes,</p>
<blockquote>
<p>Beneath the pleasure generated by the juxtaposition of order and complexity,
we can identify the subsidiary architectural virtue of <em>balance</em>. Beauty is a
likely outcome whenever architects skillfully mediate between any number of
oppositions, including the old and the new, the natural and the man-made, the
luxurious and the modest, and the masculine and the feminine.</p>
</blockquote>
<p>What analogy sits between these two extremes? Where is the balance between
progress and stamina?</p>
<p>I feel like it's building a home.</p>
<p>Recently, I watched <a href="https://www.youtube.com/watch?v=AxM9FYSs8V4">a man building his own log
cabin</a>, slowly outfitting large
portions while also doing his chores from day to day. You can tell the focus
placed on making it a lovely, warm place people can relax but also the
willingness to accept the mess where it need to be accepted. As Jonathan Blow
eloquently puts it in a <a href="https://www.youtube.com/watch?v=6XAu4EPQRmY">portion of one of his live
streams</a>,</p>
<blockquote>
<p>We are ignoring [a pointer problem] for now and we are making a note that that
problem needs to be solved. You don't [sic] so here's the thing, in a big
project you just don't have to solve every problem at once in fact if you try
you will not get very far at all. You'll just get crushed under the load under
all the things you have to do and of never getting anything done ...</p>
</blockquote>
<p>Jonathan goes on to state how he subdivides problems into ones he wants to
seriously tackle now, and ones where he is putting in a rudimentary solution so
long as it gains him progress. All of this, to me, is the same sort of balance
that comes in the mental idea of having one's own house.</p>
<p>Most likely several things are demanding ones attention for fixing; you might
need to get a wall repaired, perhaps a door is more important since it's
front-facing, and you need to rake out front and put out the trash. Despite all
this effort, you spend many relaxing moments in your home, recharging and
growing with the environment built up around you. Alain de Boton feels homes
represent our ideal lifestyles, and experienced programmers do the very same on
their codebases; they make it home not only for themselves but for others.</p>
<p>Maybe we all ought to make homes so we can grow and feel happy in them, without
sacrificing progress and stamina.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Custom Search Functionality for Coding</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/custom-search-functionality-for-coding.html</link>
      <guid>https://justanotherdot.com/posts/custom-search-functionality-for-coding.html</guid>
      <pubDate>Mon, 19 Aug 2019 16:10:36 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This may not be revelatory to some, but it's a cool trick I use daily and I
thought I'd write about since it's managed to surprise enough colleagues and
friends when I've used it. Credit where credit is due, I was taught this trick
two years ago by Charles O'Farrell.</p>
<p>Firefox and Chrome both support this functionality but are setup differently.
Let's say you have a github codebase with a particular org (which are also,
confusingly, demarcated as users in github search). You want to find a repo
quickly; you can quickly go to your search bar and hit <code>repos a_project</code> (or in
the case of Chrome, <code>repos&lt;tab&gt; a_project</code>), hammer the enter key and you wind
up at <code>https://github.com/search?q=user%3Aorg+a_project</code>. How?</p>
<p>In both Chrome and Firefox, you can add a custom search engine by right-clicking
on the search 'bar' (form) you'd like to add, except in Firefox the mechanism
works via bookmarks and Chrome has the functionality as it's own thing (seems
like a first class citizen). There <em>are</em> custom search engines for Firefox you
can add (I've noticed I can add them for things like crates.io, docs.rs, amazon,
et. al.) out of the box given a specific version (I'm not sure which) of Firefox
but you'll still need to bookmark approach for most cases. Once you have the
search URL you care about just replace the term you searched for with <code>%s</code> and
all is well, e.g. <code>https://github.com/search?q=user%3Aorg+%s</code>.</p>
<p>Some other examples of uses are:</p>
<ul>
<li><code>code term</code> - similar to the <code>repos</code> keyword above but searches across all
repositories of an org for <code>term</code></li>
<li><code>(docs.rs|crates.rs|younameit) &lt;module&gt;</code> - looks for module documentation,
listing in some large store of knowledge</li>
<li><code>rstd term</code> - search for <code>term</code> in the rust std lib (handy when paired with
something like rust-tags so you can jump to definition inside of std in your
editor)</li>
</ul>
<p>The above are rust centric because it's what I've been in the headspace of but
you could easily set this up for things like hoogle, amazon, shortening things
like youtube to <code>y</code>, <code>hex.pm</code>, and so on. Personally, it's been empowering to
gain a handle on parameterizing search functionality with your address bar.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Review for 2018</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/reading-review-2018.html</link>
      <guid>https://justanotherdot.com/posts/reading-review-2018.html</guid>
      <pubDate>Wed, 12 Dec 2018 20:09:20 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Being a voracious reader, one thing that helps bring the mountain of things I
want to read down to something manageable is being able to read things faster.
Reading <em>faster</em> itself probably hurts comprehension and digestion of core
concepts, but getting digested content from others who have already read the
main body of work can drastically reduce the amount of fluff you'll have to wade
through yourself. If you've ever read a book review that basically told you
everything you needed to know before you've read the book, you'll know what I'm
talking about.</p>
<p>I've personally felt this way a lot towards reviews and have found it
invaluable. As such, I feel it's necessary for me to pay this back. It's not
worthwhile giving rundowns for <em>everything</em> I've read so the purpose of this
article is to focus on the things I've found fascinating. This is by no means a
comprehensive list and you'll probably find the details per body of work a tad
thin as time has waned on since I've read them. However, I do wish to make this
more of a regular habit and, in the end, hopefully I can at least express the
delight I've had getting through some of these wonderful wading pools of words.</p>
<h2><a href="https://www.amazon.com/Coders-Work-Reflections-Craft-Programming/dp/1430219483/ref=sr_1_1?ie=UTF8&amp;qid=1544954623&amp;sr=8-1&amp;keywords=coders+at+work">Coders at Work</a></h2>
<p>This had so many fantastic insights into programming in the large as well as the
small for some of my heroes as well as some people I had never heard of before.
Of particular note was the recurring question Peter Seibel kept bringing up:
&quot;How do you read code?&quot;. In fact, Seibel even did a follow-up blog post on the
subject, but my particular take was the specifics that people actually replied
with. In fact, the subject matter drove me to write an article about reading
code I dubbed <a href="https://justanotherdot.com/posts/Reading_Code_is_Decoding.html">'Reading Code is
Decoding'</a></p>
<p>One other thing I thought fascinating was that one of the leaders of the Haskell
language, Simon Peyton Jones, had feelings towards types as proof systems that
were less intense as I had originally assumed. In fact, SPJ repeatedly mentions
that the idea is about <em>confidence</em> and not about mathematical fact.</p>
<p>Lastly, guys like Brad Fitzpatrick and Jamie Zawinski 'got things done', but
they also had a lot of sensibility towards quality instead of simply being
<a href="https://www.joelonsoftware.com/2009/09/23/the-duct-tape-programmer/">'duct tape
programmers'.</a>
In my mind, the book does a phenomenal job of showing how several people can
think of simplicity from different angles, whether that's avoiding the barbarism
of C++ in the face of Netscape's pre-existing C codebase, avoiding unnecessary
enterprise software in exchange for possibly less-than-ideal open-source
solutions to keep Live Journal up and running, or sussing out obtuse assembly
language for an unknown system by mathematically dissecting hunks of code.</p>
<h2><a href="https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth-ebook/dp/B06XP3GJ7F/ref=sr_1_2?ie=UTF8&amp;qid=1544954644&amp;sr=8-2&amp;keywords=the+manager%27s+path">The Manager's Path</a></h2>
<p>Soft skills are hard, and we'll see in a few other reviews that they are even
harder to write about and absolutely harder to put into practice properly! But
that still doesn't mean people can't try. Things of note:</p>
<ul>
<li>
<p>You will sometimes need to do hard things and tell people hard news, but it's
better you do that than try to pretend everything is amazing. In other words,
It's better to be 'kind' (honest but considerate) than simply 'nice' (always
accommodating)</p>
</li>
<li>
<p>As you may find yourself with more managerial duties, you're ability to bridge
the gap between technical and non-technical members of the business increases;
this can be with metrics and even how you handle explaining or using
alternative jargon</p>
</li>
<li>
<p>A healthy codebase is an active one, but the rate of change needs to be kept
in check with the rate of errors. There is a subtle hint here about developing
a devops culture so that people are aware of how they can best help ship
software and know if they are within a predefined error budget. If you impede
the error budget, it's time to start focusing more on fixing things, which
means having flexible scheduling percentages.</p>
</li>
<li>
<p>Learning is the most vital aspect of a business and orienting it's processes
around it is pivotal in it's success</p>
</li>
<li>
<p>Having technical chops before you wind up in any managerial capacity is
crucial but it's also important to know that things like leadership and
empathy takes just as much effort to refine and perfect</p>
</li>
</ul>
<p>I feel like a good follow up to this was reading John Allspaw's article <a href="https://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/"><em>On Being a Senior Engineer</em></a>.
In it, Allspaw hits on several soft skills that 'senior' software engineers
absolutely must uphold on a day to day basis, and I really don't see them being
that different from soft skills that others should be upholding as well as they
increase in seniority.</p>
<h2><a href="http://mcfunley.com/choose-boring-technology">Choose Boring Tech</a>, <a href="https://www.intercom.com/blog/run-less-software/">Run Less Software</a>, and [You Need a Novelty</h2>
<p>Budget](https://www.shimweasel.com/2018/08/25/novelty-budgets)</p>
<p>I mention these three articles in tandem because they were thoughts I had
faintly felt being in various organisations but could not pinpoint with words
quite as well as these three articles do. The core ideas are:</p>
<ul>
<li>
<p>You don't always need to map every problem to an ideal solution, in fact,
doing so is problematic because you will wind up with too much tech to
maintain, and maintenance cost must weight in considering adoption</p>
</li>
<li>
<p>Every new piece of tech you acquire into your stack means more things people
need to know about and the more people need to know means the less they can be
experts.</p>
</li>
<li>
<p>Choosing or building swanky libraries, technology, and services may be
exhilarating but it's also problematic for the above reasons. Having a
'novelty budget' can help prevent 'fancy tech creep' into the codebase and
infrastructure. In fact, it may help to run as many crazy projects 'on the
side' (i.e. outside of work) as possible to help reduce the urge of
introducing shiny-new-things</p>
</li>
</ul>
<p>I kept going back to these articles at least every month or two; they serve as a
basis of what I think is productive coding: write boring code that <a href="https://www.youtube.com/watch?v=4Y0tOi7QWqM">fits in your
head</a>, is heavily tested, focuses
on some clear specification (the proper abstraction), and satisfies &quot;small is
beautiful/less is more&quot;. I can't imagine that being the end of the proper
characteristics and that list keeps morphing as I keep coding, but it seems
sensible enough to mention those characteristics in relation to these posts.</p>
<h2><a href="https://www.amazon.com/Twelve-Steps-Compassionate-Karen-Armstrong-ebook/dp/B003WUYPBA/ref=sr_1_1?ie=UTF8&amp;qid=1544954743&amp;sr=8-1&amp;keywords=12+steps+to+a+compassionate+life">12 Steps to a Compassionate Life</a></h2>
<p>For trust to cultivate between members working together to build impossible
programs and systems you need respect and you can't build respect until you have
empathy for others. You don't need to feel the same thing they feel, per se, but
compassion itself is, in my mind and shaped from the sentiments in the book, the
difficult but extremely rewarding practice of cultivating concern for others,
their suffering and feelings.</p>
<p>In the book, Armstrong equips us with a particular process of better developing
empathy for others via religious, sociological, and psychological references.
Since this is a heavy process-oriented book, I can't simply give a quick tl;dr,
but I do want to recommend this book heavily. It and <a href="https://www.amazon.com/How-Talk-Kids-Will-Listen-ebook/dp/B005GG0MXI/ref=sr_1_4?ie=UTF8&amp;qid=1544953942&amp;sr=8-4&amp;keywords=how+to+talk">How to Talk so Kids Will
Listen and Listen so Kids Will
Talk</a>
are both fantastic resources on this subject, although I've read the latter a
number of years before this post.</p>
<h2><a href="https://blog.acolyer.org/2018/08/20/filter-before-you-parse-faster-analytics-on-raw-data-with-sparser/">Filter Before you Parse</a></h2>
<p>The Morning Paper is probably <em>the</em> best resource for mind blowing information
on the internet for software engineers, and I could ramble off many articles
that absolutely twisted my brain this year. If you don't have a subscription to
the The Morning Paper, you need to go subscribe this instance! Despite it not
being the most mind-bending of articles this year, I wanted to point out this
particular piece and associated paper because it's such a &quot;oh <strong>of course</strong>&quot;
kind of moment.</p>
<p>The gist? Do a fast search on the JSON payload <em>before</em> transforming it into
it's in-memory representation.</p>
<h2><a href="https://lemire.me/blog/2018/05/03/how-fast-can-you-parse-json/">How Fast Can You Parse JSON?</a></h2>
<p>Daniel Lemire is a performance and database nut (and so I'm easily a fan). I
wanted to include this since I had also included the JSON parsing related paper
from The Morning Paper. In this short article, Daniel explores a few industrial
grade parser implementations and tries to get at how many cycles it would take
to parse per byte, eventually coming to the closing line of:</p>
<blockquote>
<p>So you should expect to spend 2 or 3 seconds parsing one gigabyte of JSON data.</p>
</blockquote>
<p>I am personally a fan of &quot;Latency Numbers Every Programmer Should Know&quot; and I
think things like this are actually handy, back pocket facts that ease
estimation, s.t. if you find you're parsing a gigabyte of JSON and it's taking
orders of magnitude more than this, you can probably know there are gains to be
made, and also know that there might be a happy limit for optimisation.</p>
<h2><a href="https://charity.wtf/2018/08/19/shipping-software-should-not-be-scary/">Shipping Software Shouldn't be Scary</a></h2>
<p>I have a number female software engineer role models; Julia Evans, Jessie
Frazelle, and Charity Majors, to name a few. Charity's kick is on empowering
devs to be both coding and operations gurus by empowering them with the
superpower of observability. This article by Charity also goes into some core
things about operational concerns that are &quot;everyone's problems&quot;. The hot take
is that devs should be able to take code from cradle to grave; ideation into
production which entails all the messy debugging and reversals after Things Go
Wrong. The aim is to make everyone a software owner rather than a mere
occasional participant.</p>
<p>It may also get the reward for the best work memes of 2018.</p>
<h2><a href="https://gist.github.com/bcantrill/835837a66bcc21b899f501dd794a7d5f">Bryan Cantril's 'internal' doc on hiring</a></h2>
<p>Bryan sums this up nicely in the beginning, I'll add my takeaways for each bit.</p>
<ul>
<li>
<p>Aptitude: can the person actually code?</p>
</li>
<li>
<p>Education: have they persevered through boring lectures and stress?</p>
</li>
<li>
<p>Motivation: are you driven to write code everyday or is it Just Another Job?</p>
</li>
<li>
<p>Values, per the doc itself:</p>
</li>
</ul>
<blockquote>
<p>One observation is that one's values -- and the adherence or divergence from
those values -- will often be reflected in happiness and satisfaction with
work. When work strongly reflects one's values, one is much more likely to
find it satisfying; when values are compromised (even if for a good reason),
work is likely be unsatisfying.</p>
</blockquote>
<ul>
<li>Integrity: The usual verification that people are who they say they are and
not actually crazy, serial axe murders or simply bullshit artists.</li>
</ul>
<h2><a href="https://lethain.com/migrations/">Migrations: the sole scalable fix to tech debt</a></h2>
<p>You have legacy technology and you wish it would just go away, but how are you
going to euthanize it? You could write a new system and hide the other one in
the corner, perhaps? But you end up realising that your business relies quite
heavily to this chunk of code and that it's actually grown into such a mess you
can't fathom now moving away from it entirely and now need to run two things;
this is a fairly common problem and I think the gist of this article was spot
on: if you want to deprecate A, you need to be focused and clear about how B is
going to entirely replace it's functionality to the point where it can be
politely deleted out of existence, and you need to make this a common practice
as an engineering team as every piece of code you write is legacy the moment you
write it. Code rots, and, as such, that shiny new piece of tech you've just
built will one day need a successor.</p>
<p>In general the process is the same for any piece of software: do upfront
planning with documentation and good old fashioned thinking, get people on
board, don't be the machine and automate as much of the migration as possible,
help <em>track</em> the actual shift away from the older platform, and finally, don't
forget to celebrate your achievements! That last one is huge because if you
don't make migrations a big deal, people won't see the value in it!</p>
<h2><a href="https://www.amazon.com/Writing-Without-Bullshit-Career-Saying-ebook/dp/B01A5CEKQM/ref=sr_1_1?ie=UTF8&amp;qid=1544955550&amp;sr=8-1&amp;keywords=writing+without+bullshit">Writing Without Bullshit</a></h2>
<p>I'm actually quite surprised that this book devolved into a description of a
particular process, but I think it's a worthy ally in the fight to cut the fat
out of one's prose (the emphasis is in a workplace setting). Two major things of
note I took from this book were:</p>
<ul>
<li>
<p>Be short and to the point but scrutinize your use of data.</p>
</li>
<li>
<p>It's incredibly useful to get out 'fat outlines' of work before you start
obsessing on details and get lost in the subsequent worrying. I write like
this now in general: large bulk of content upfront and several passes of
refinement afterwards.</p>
</li>
<li>
<p>The 'iron imperative': &quot;Don't waste the readers time&quot;</p>
</li>
</ul>
<p>That last point kept having me think about the book <a href="https://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/ref=sr_1_1?ie=UTF8&amp;qid=1544955670&amp;sr=8-1&amp;keywords=don%27t+make+me+think">Don't Make
Me</a>
Think where the crux of design should be about alleviating cognitive load on the
user of the interface. It's a powerful phrase to have handy when needing to
point out really lengthy bits of code, comments, or documentation, as well as
general communiques.</p>
<h2><a href="https://www.amazon.com/Messy-Power-Disorder-Transform-Lives-ebook/dp/B01BD1SU2E/ref=sr_1_1?ie=UTF8&amp;qid=1544955751&amp;sr=8-1&amp;keywords=messy+the+power">Messy: The Power of Disorder to Transform Our Lives</a></h2>
<p>This book was monumental in getting me comfortable with the idea of doing rough
groundwork upfront. In fact, some people describe this as the <a href="https://en.wikipedia.org/wiki/Pareto_principle">'paeto
principle'</a> where 'eighty
percent of the work comes from twenty percent of the effort'. In my mind I'm
sure Pareto never intended this to be about a specific ratio but rather about
how a small portion of input winds up relating to a large portion of the output.
I actually found no better succinct description of the book except at the start
(the rest is still worth the read):</p>
<blockquote>
<p>The argument of this book is that we often succumb to the temptation of a
tidy-minded approach when we would be better served by embracing a degree of
mess.</p>
</blockquote>
<h2><a href="https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&amp;qid=1544955863&amp;sr=8-1&amp;keywords=programming+rust">Programming Rust</a></h2>
<p>I've not really read a specific programming-language book in awhile, but I love
systems programming and Programming rust (along with several other rust books)
focus heavily on the subject; whether it's talking about error handling, rough
edges with Unicode, or even how different languages handle assignment in order
to explain the borrow checker and moves, I was constantly enthralled to pick
this book up despite it being close to six-hundred pages. It's gained the
classic 'dirty and crushed pages' aesthetic and it'll probably gain a bit more
as I keep trying to double down on rust.</p>
<h2><a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?ie=UTF8&amp;qid=1544955948&amp;sr=8-1&amp;keywords=designing+data-intensive+applications">Designing Data-Intensive Applications</a></h2>
<p>I can't recommend this book enough. I actually finished reading this near the
end of 2017, but I love it so much that I wanted to write about it here. I
personally don't know of any other book that handles the fundamentals of
database internals (along with many modern improvements), database design,
clusters, nodes, schema encodings, discussions around guarantees (CAP, ACID) and
what are valid and invalid points, consensus in distributed systems, and even
modern data processing patterns all while in the context of proposing a credo
that data-intensive applications should uphold, in such an elegant and
ultimately fun way. I still read through the book for knowledge I may have
passed up or forgotten so it's a fantastic resource to have on my shelf.</p>
<p>It's also dawned on me that data processing <em>is</em> programming, and knowing more
and more about data and how to handle it is enlightening and empowering.</p>
<h2><a href="https://www.amazon.com/Thinking-Forth-Leo-Brodie/dp/0976458705/ref=sr_1_1?ie=UTF8&amp;qid=1544956045&amp;sr=8-1&amp;keywords=thinking+forth">Thinking Forth</a></h2>
<p><a href="https://twitter.com/lorentzframe/status/997997523301117953">Brian Beckman wrote a tweet I read awhile
back</a>, stating:</p>
<blockquote>
<p>&quot;SICP&quot; by Abelson &amp; Sussman should be read continuously, ~2 pages a day,
returning to page 1 every year. Ditto &quot;Thinking Forth&quot; by Leo Brodie, tho'
only ~1 page a day. The former teaches how to think, the latter how to
engineer. Both are in unpopular languages, on purpose.</p>
</blockquote>
<p>and having read most of the SICP, even watching the entire lecture series, but
never having heard of <em>Thinking Forth</em> (but having heard of the language), I was
intrigued to hear of a book put on the same page as the SICP! I am actually,
also, cheating here as I've read only half of it, but so far it's definitely a
different way of looking at software construction in the same vein that the SICP
went to lengths to describe.</p>
<p>The heaviest emphasis made so far in the book that I love seems drawn between
Domain Driven Design and traditional functional programming: words are vital to
how we code, the meaning they carry as well as their surrounding semantics; we
group words into lexicons which we can call 'components', and with 'components'
we get all the general benefits we get from composition. There is some forth
specific information nestled between, but <a href="https://twitter.com/lorentzframe/status/999121431559487489">Beckman is wise in a followup
tweet</a>, saying:</p>
<blockquote>
<p>If SICP were in, say, (popular) Python instead of (unpopular) Scheme, people
might be distracted by the Python and miss the thinking. Ditto if &quot;Thinking
Forth&quot; were in (popular) Java instead of (unpopular) Forth, people might miss
the deeper points about software engineering.</p>
</blockquote>
<p>I'm excited to finish it in January, in tandem with <em>Thinking in Systems: A
Primer</em>, which I've started just last week and has so far been an amazing way of
perceiving the structure of various systems from a proper, abstracted point of
view.</p>
<h2>Conclusion</h2>
<p>I originally thought a format involving simple review styled blurbs might be
handy, but it's kind of nice having not thought about some of these various
texts for a bit to really see what would stick. I hope this has been somewhat
useful or fun!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>What makes a good pull request?</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/what-makes-a-good-pr.html</link>
      <guid>https://justanotherdot.com/posts/what-makes-a-good-pr.html</guid>
      <pubDate>Fri, 20 Jul 2018 20:40:08 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Pull Requests (or PRs) are a tango between two parties; the code author and the
code reviewer which I will simply refer to as the 'author' and 'reviewer' in the
remainder of this article. In a pull request, the author has provided code to
solve a particular problem and the reviewer is there to provide a feedback
mechanism to the author.</p>
<h2>Code review is not a gate keeping task</h2>
<p>I once worked for an organisation whose code review process centered around a
total lack of faith in its developers ability to deliver quality product; tech
leads acted as the gate keepers to their respective stacks forcing the average
developer to resort to underhanded tactics in order to get their changes
merged, regardless of quality or prospective bugs. This, in turn, meant the
gate keepers felt justified under the guise of 'keeping things safe'. Thus,
code wasn't being checked properly and wrong or flimsy changes would trickle
into master, making the code review process utterly broken.</p>
<p>As a software engineer, every line of code you ship is code you, or someone
else, will need to maintain, and as such, you should be fighting to deliver the
best quality you can offer, regardless of deadline. <strong>Code reviewers are there
to help people bring their code into the light of day where asking questions is
the chief tool a reviewer employs</strong>. This can include, but is not limited to,
probing to see if:</p>
<ul>
<li>Is the thought fully fleshed out?</li>
<li>Is this implementation correct for the problem it aims to solve?</li>
<li>Are the changes sound and principled?</li>
<li>Are there any performance or semantic concerns?</li>
</ul>
<p>I'm purposefully leaving out stylistic choices here as the discussion often
leads to the argument around adoption of some automated code-formatting tool.</p>
<h2>Raise early and raise often</h2>
<p>In &quot;Debugging Team's&quot;, the authors kick off the start of the book with a simple
analogy between two competing inventors. The one inventor does not want to share
his ideas for fear of them being poached by others, while the rival inventor
gleefully goes to local places where experts might hang out to get more
information about how to build her inventions.</p>
<p>Software development is no different in that knowing things earlier is always
better than knowing things later. Raising PRs even before they are 'complete'
(and appropriately marking them as WIPs or 'works in progress') allows people to
possibly, time permitting, look into your changes and see if there are any major
red flags.</p>
<p>That said, if you are a reviewer and are asked (or not!) to look at a set of
changes that is marked as a WIP, try to hold off on a more in-depth review until
the author changes this status. And to PR raisers, don't keep things in WIP
stage for too long, which brings me to my next point.</p>
<h2>PRs are for small chunks of code to merge often</h2>
<p>A PR should represent up to a day's worth of work. This is beneficial to both
parties in that it facilitates a 'merge often' approach for devs (and devs get
the little adrenaline kick from clicking that green <code>merge</code> button) and
reviewers can much more easily review a smaller hunk of changes. A reviewer
reviewing five PRs in the course of a week has to spend less time grokking
those individual changes than to review five days worth of work in a single PR.</p>
<p>Massive projects poised around scaled tooling and reviews such as the Linux
kernel would flat out reject a patch with, say, 3k additions and 1k removals. If
a project of that size and calibre, and that many international hands involved,
is marking code review of those dimensions as 'unmanageable', what hope does a
startup have at making fast, rapid changes in the same light?</p>
<p>Small PRs are also <em>focused</em> on a clear intent. Asking the author to fix
neighboring code 'just because' or refactoring/formatting several adjacent files
that are not directly tied to the immediate effort of the PR wastes the both
parties time. Opening a PR to refactor changes and another PR to add new
functionality is a much better way to get appropriate attention from reviewers.
As the joke goes:</p>
<blockquote>
<p>10 lines = 10 possible bugs, 100 lines = lgtm</p>
</blockquote>
<p>It's important to remember that a PR is not to encompass a single ticket or
issue. Tickets can have several PRs attached to them and all it takes is
lobbing <code>[FOO-123]</code> on top of one's PR title for Jira or marking <code>#&lt;issue number&gt;</code> in your description in GitHub. I like to call this act 'linking' and
it's useful for stakeholders to track down all the changes that have fed into a
particular ticket.</p>
<h2>Context matters</h2>
<p>Reviewers need to discuss with the author about the purpose of a set of changes
and how close or far off they are from that goal, but if the reviewer is
unclear about this goal, it's difficult for them to strike up a discussion with
the author.</p>
<p>In the context of OSS, raising a patch directly to a project such as the Linux
kernel is poor practice. If you want to make a change in any capacity it's best
first to contact the people who own the code on public channels. This provides
auditing and clear context for others. <strong>Your first instinct should be to raise
a PR unless it's a feature. If you feel uncertain about whether or not your
change is warranted, it's best to raise an issue first, instead.</strong></p>
<p>That said, raising PRs should feel natural; PRs are cheap and can be closed and
their branches pruned as need be, but regardless of the cost of raising a PR,
it's critical to include appropriate information. Some important things to
mention may be:</p>
<ul>
<li>What does this set of changes solve?</li>
<li>Is there a specific task (issue/ticket) that this relates to?</li>
<li>Is this blocked or blocking any other PRs/issues/tickets?</li>
<li>Is there any additional information that will help the reviewer know about my
manual testing of this ticket (screenshots, output from tooling, et. al.)?</li>
<li>Have you updated tests and documentation accordingly? Have you added tests
that the reviewer can skip to first to immediately see how you're proposed
changes are supposed to work and in what cases?</li>
<li>Is there current behaviour to contrast the new behaviour to?</li>
<li>Are there breaking changes present?</li>
</ul>
<p>The traditional approach for this was to include commentary in your actual
commit messages and headers. I don't think times have really changed in this
regard and the more context you sprinkle about the better, so long as you are
clear about your intent and you don't waste the reader's time.</p>
<h2>Check out changes locally when it makes sense</h2>
<p>A really healthy habit for reasonably sized changes is to always check out a PR
and see if it works for you. Some projects have powerful processes for testing
full 'e2e/raw hardware' scenarios such as Intel's <a href="https://01.org/lkp/documentation/0-day-test-service">zero-day testing
bot</a> which actually boots
up machines to test out differing version of the Linux kernel. While continuous
integration can catch a lot, it's important to sometimes get a human eye for
regressions that may not, or cannot, be encoded in automated tests. We all make
mistakes, and reviewers are there to add a layer of sanity checks to our
changes.</p>
<p>Use common sense. If a change is pretty sensible (e.g. a single line change to
update a variable name), you probably don't need to spend the time pulling the
change down, compiling, running the tests, and so forth. As the developer's
mantra goes, &quot;Don't be the machine&quot;!</p>
<h2>Clean up your mess</h2>
<p>Your changes have been merged and you can go on with your life, but before you
reach for beverage of choice, you should prune your dead branches. I actually
have a git bash script for this that I place in my <code>PATH</code> so I can call it as
<code>git wash</code>. The script is <a href="https://gist.github.com/justanotherdot/3e3a16df805d09a37e1c26bbedd23fcc">here</a>. When
run without arguments, this will delete the current branch you are on locally
and remotely so long as the branch specified to <code>git wash</code> is not master and the
remote branch is not protected. If you need other git functionality like this,
any script in your path with the name <code>git-&lt;thing&gt;</code> can be run as <code>git &lt;thing&gt;</code>.</p>
<h2>Conclusion</h2>
<p>Reviewing many small changes is much more manageable for a reviewer than
reviewing large, tangled changes. Decomposition in programming gives us the
ability to stitch together many small, verified solutions that lead up to an
equally trustworthy result and the same is no different in the process of
supplying code changes to a project; breaking up the changes you need to make
into reasonable chunks that can fit in everyone's heads not only helps to
provide code changes faster but it also paves foundations for robust and
resilient code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Trampling Trampolines</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/trampling-trampolines.html</link>
      <guid>https://justanotherdot.com/posts/trampling-trampolines.html</guid>
      <pubDate>Thu, 24 May 2018 19:40:17 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <h2>Continuation Passing Style</h2>
<p>or CPS for short, is a way to ‚Äòcontinue‚Äô a function call by calling into another function. The simplest example would be:</p>
<pre><code>function cps(x, return) {
  return(x);
}
</code></pre>
<p>The important thing is the callback is passed and it is called at the ‚Äòcompletion‚Äô of the function, passing things along.</p>
<p>CPS is not a problem in languages where Last Call Optimisation is commonplace. What these languages do (normally of eager evaluation) is collapse the stack frame and call the function call so long as there is nothing else beyond the function. <a href="http://erlang.org/pipermail/erlang-questions/2016-October/090663.html">Here‚Äôs a fun rant by Joe Armstrong regarding the implementation and implication of Tail/Last Call Optimisation</a>. Of note is how he is careful not to say this is the same as Tail Call Optimisation, or TCO, as that typically implies a recursive call.</p>
<h2>DIY</h2>
<p>Languages that don‚Äôt collapse the stack or represent functions in a form that is conducive to fusion are subject to ‚Äòblowing the stack‚Äô (exceeding it‚Äôs maximum size) when writing a recursive function. The function may be correct, but for particular values it may grow too large and too many stack frames will be pushed onto the program‚Äôs stack.</p>
<p>There are some <a href="http://chrispenner.ca/posts/python-tail-recursion">interesting ways to mimic TCO</a> in languages that don‚Äôt have native support for it. JavaScript does, technically, but implementation seems spotty across engines.  A clever way to mimic TCO in JavaScript is to rewrite your function to return a continuation, of sorts, and then wrapping it in another, generic, function (the trampoline) that knows about this arrangement in order to collapse the stack itself, for example:</p>
<pre><code>let ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`ERROR: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>I‚Äôve chosen the <a href="https://en.wikipedia.org/wiki/Ackermann_function">Ackermann function</a> here as it‚Äôs a golden standard for testing recursive functionality in programming languages since its value grows rapidly even for small digits. I‚Äôm also using the convention of naming things as <code>go</code> to specify recursive helper functions with arguments we don‚Äôt care about. A common counterpart to <code>go</code> you might see (particularly popular in LISPs) is <code>do</code> .</p>
<p>If you play around with this function you should quickly find it exceeding the allotted call stack size. Let‚Äôs fix this with a trampoline:</p>
<pre><code>let trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (typeof rv === 'function') {
    rv = rv();
  }
  return rv;
};

ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return () =&gt; ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return () =&gt; ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`urk: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>Note the zero-arity functions we are returning in order to signal that the return value can be applied (specifically lines 13 and 15). What kind of functions will <code>trampoline</code> fail on given it‚Äôs current implementation? When you‚Äôve given it some thought, consider this case:</p>
<pre><code>let higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return offset =&gt; acc+offset;
  }
  return () =&gt; higherOrderFunc(n-1, acc+n, offset);
}

let hof = trampoline(higherOrderFunc);
</code></pre>
<p>Although this will terminate, it won‚Äôt give us the correct result. We want a closure in the end, but here we will wind up with <code>NaN</code> since that is the result of adding any number in JavaScript to <code>undefined</code>, and since we are calling the result value, <code>rv</code>, with no arguments, we are technically passing <code>undefined</code> to this final value.</p>
<p>A <em>structural</em> type system is one which cares only about the form of given values, whereas a <em>nominal</em> type system cares about the <em>names</em> that values have in the system. In a language like Haskell, we‚Äôd use what‚Äôs known as a ‚Äúdata constructor‚Äù to declare a type. Consider:</p>
<pre><code>data Cont a = Stop a | Cont (() -&gt; Cont a)
</code></pre>
<p>This says ‚Äúfor any given value of <code>a</code>, I can either be a <code>Stop</code> value wrapping some given value of <code>a</code>, or I can be a <code>Cont</code> wrapping a function which takes <code>unit</code> (the <code>()</code>, or, in other words, no arguments that matters) to another <code>Cont</code> value‚Äù</p>
<p>We can, again, mimic something similar using a <code>tag</code> field on an object, a la:</p>
<pre><code>higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return {
      tag: 'stop',
      val: offset =&gt; acc+offset
    };
  }
  return {
    tag: 'cont',
    val: () =&gt; higherOrderFunc(n-1, acc+n, offset)
  };
};
</code></pre>
<p>Which we can leverage in a new definition of <code>trampoline</code>:</p>
<pre><code>trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (rv.tag === 'cont') {
    rv = rv.val();
  }
  return rv.val; // Of tag 'stop'.
};
</code></pre>
<p>We only have two cases for our ‚Äòtype‚Äô, so when we no longer have a <code>cont</code> tag, we must have a <code>stop</code>, and with this we get the correct result: a function!</p>
<h2>Conclusion</h2>
<p>In mathematics it is just as important to reason about the types of objects we are dealing with as it is to reason about their values. The same is no different in programming. Even though you may say ‚ÄúI hack in a dynamically typed language, I don‚Äôt need to think about types‚Äù, the inverse is actually the truth! Hacking in a dynamically typed environment means juggling these notions around in your head rather than allowing the type checker make sense of the form of things for you.</p>
<p><em>food for thought:</em> What we‚Äôve done by tagging these values is upgrade a <code>union</code> type into a <code>discriminated union</code>. A <code>discriminated unions</code> is also sometimes known as a <code>sum</code> type. The power of passing around sum types is that we can reason about the cases in our code: recursion itself is similar to mathematical induction, and both are forms of breaking down data whereas their duals, co-recursion and co-induction, build up data. This is an important notion because it means that when we write things recursively in this form with inductive-like types (e.g. sums) we can ‚Äòpattern match‚Äô on their values and know something about each case. In the above examples we knew that <code>cont</code> always contained a function that took no arguments and returned another <code>cont</code> or <code>stop</code> tagged object. When we finally got a <code>stop</code> we knew we had our final result we could return, and since there were only two values we could be sure that those were the only two cases worth exploring (show exhaustivity checking as well as case analysis). As is common with FP, some languages give you the power of actually statically checking this; ensuring that you‚Äôve considered all cases, whereas in others you‚Äôll be left to discipline or libraries to replicate this, just like trampolines.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Proficiency is Tiered and other Lies We Tell Ourselves</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/proficiency-is-tiered-and-other-lies-we-tell-ourselves.html</link>
      <guid>https://justanotherdot.com/posts/proficiency-is-tiered-and-other-lies-we-tell-ourselves.html</guid>
      <pubDate>Thu, 01 Mar 2018 15:48:02 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Tiered categorisations of knowledge and proficiency are fundamentally flawed as
they rest on the notion that all knowledge can eventually be obtained,
retained, and divvied up amongst n-many categories. Such categorisations also
ignore the fact that most skills rely on overlapping knowledge from various
domains. In this article I propose a way to evaluate subject matter in the
context of best prioritising <em>what should I learn next?</em> I‚Äôll also offer up an
approach to evaluating others that isn‚Äôt based on ‚Äòskill level‚Äô (admittedly
regurgitated from Amy Cuddy).</p>
<h2>A Tagging System</h2>
<p>Instead of suggesting that knowledge from a domain of expertise can be chunked
and tagged in toto, we consider an alternative tagging system where we look at
knowledge from three types of labels and, most importantly, accept that the
fringes are fuzzy:</p>
<ul>
<li>Fundamentals
<ul>
<li>There is usually a corpus of knowledge that everyone can agree upon is
pivotal to ‚Äòbeing competent/dangerous‚Äô in a particular subject matter.
These may overlap to other skills and it may be unclear <em>which</em> skills they
overlap in, but what matters is that these skills are relatively obvious in
the domain of note.</li>
</ul>
</li>
<li>Nice-to-Haves
<ul>
<li>This is the knowledge that might be good to spend a bit of time on as it
refines and builds on fundamentals to introduce more powerful techniques
and practices. This is where the only clarity is that they are definitely
not fundamentals and they are definitely not esoteric.</li>
</ul>
</li>
<li>Trivia
<ul>
<li>This is the stuff you probably don‚Äôt need to know, like that some AIX
machines have a weird bug in certain prompts where inputting uppercase
characters will cause the machine to reboot or that earlier, alternative
architectures supported 7-bit bytes. These tidbits of information
(sometimes not so miniscule!) are probably very costly to pick up and don‚Äôt
give you much in return.</li>
</ul>
</li>
</ul>
<p>These tags map very much to the progression of learning a subject: when you
start learning a subject, everything is rough and unclear; you should focus on
exposing yourself to as much as knowledge as possible even if you don‚Äôt quite
understand everything. This ‚Äô5yo‚Äô view of the world helps build the framework
wherein we can fill in further details as we step towards the nice-to-haves,
but instead of becoming an ‚Äòexpert‚Äô by picking up trivia, we try to avoid it,
and if it were important, then it would fall back into the nice-to-haves. This
is the important caveat to learning anything in general I‚Äôm trying to make
here; mastering a subject has nothing to do with knowing absolutely everything
there is to know.</p>
<p>The practice of using these tags is simple: whenever you‚Äôre faced with a
variety of options, pick fundamentals over nice-to-haves, nice-to-haves over
trivia, and (per that last regard) try to pick more fundamentals and
nice-to-haves in a variety of subject matter than trying to pick up a
collection of specific trivia for a single subject matter.</p>
<p>I liken this to the Pareto principle, which effectively states that input
effort is usually disproportionate to output gains, or, as the common quote
goes, ‚Äú20% of the effort for 80% of the output‚Äù (although it‚Äôs perfectly
feasible for the opposite situation to occur). This roughly implies that most
initial upfront work is high leverage and that driving towards ‚Äòexpertise‚Äô may
have little return on investment. What I like about this proposal is that it
accepts the fact that knowledge categorisation is messy and that there‚Äôs
probably no one in the world that knows everything.</p>
<h2>Evaluation of Others</h2>
<p>One problem with the above proposal is that it doesn‚Äôt consider the common
usage of such tiered categorisations: evaluation of others‚Äô sets skills. Amy
Cuddy proposes that most people are judging you on your competency after
they‚Äôve judged you on whether or not they can trust you. I propose we try to
drop the skill-evaluation-at-moment-of-evaluation tactic and focus on
evaluating others in two primary metrics: trust and ability to advance ones
skills over any given period of time. Some people call this ‚Äúhiring for the
slope rather than the Y axis‚Äù.</p>
<p>That said, I‚Äôm predominantly a software engineer and in my experience I find
the former usage of this proposal to be the one I care about the most.
Determining what‚Äôs appropriate for ourselves rather than trying to divvy people
up into boxes is a far more valuable use of engineering time, and further,
evaluating people on the merits of their enthusiasm, ability and desire to
continually learn, and their capacity to both work away and in teams is more
worth it‚Äôs weight in gold than if someone is a self-proclaimed 10x engineer
capable of cranking a lot of (read: complicated, un-maintainable, mal-scoped)
code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Code is Decoding</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/reading-code-is-decoding.html</link>
      <guid>https://justanotherdot.com/posts/reading-code-is-decoding.html</guid>
      <pubDate>Sat, 27 Jan 2018 13:43:09 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Roger Antonsen says in his Ted Talk <a href="https://www.ted.com/talks/roger_antonsen_math_is_the_hidden_secret_to_understanding_the_world"><em>Mathematics is
the Secret to Understanding the
World</em></a>,
mathematics, or rather the act of understanding, is largely about:</p>
<ul>
<li>Discovering patterns</li>
<li>Devising language(s) to express said patterns</li>
<li>Making assumptions</li>
<li>Playing around with all of the above</li>
</ul>
<p>Early this January I finished reading <em>Coders at Work</em> and in each interview
there is a recurring question of ‚Äúhow do you read code?‚Äù Here‚Äôs a rough summary
of some styles mentioned I found particularly useful:</p>
<ul>
<li>Get the code building early and often and make various changes to study
connections</li>
<li>Read it like literature whether printed out or jumping around</li>
<li>Rewrite the code into a version optimised for legibility</li>
<li>Puzzle through it the same way one would tackle a mathematical problem</li>
</ul>
<p>It turns out I had previously read <a href="http://www.gigamonkeys.com/code-reading/">a post from Peter
Seibel</a>, the book‚Äôs author, who had
tried on several occasions to start code reading groups at his places of work,
in which he states:</p>
<blockquote>
<p>It was sometime after that presentation that I finally realized the obvious:
code is not literature. We don‚Äôt read code, we decode it. We examine it. A
piece of code is not literature; it is a specimen.</p>
</blockquote>
<p>He goes on to quote a passage (my favourite in the book) of his interview with
Knuth (emphasis added by me):</p>
<blockquote>
<p>Knuth: But it‚Äôs really worth it for what it builds in your brain. So how do I
do it? There was a machine called the Bunker Ramo 300 and somebody told me
that the Fortran compiler for this machine was really amazingly fast, but
nobody had any idea why it worked. I got a copy of the source-code listing
for it. I didn‚Äôt have a manual for the machine, so I wasn‚Äôt even sure what
the machine language was.</p>
<p>But I took it as an interesting challenge. I could figure out <code>BEGIN</code> and
then I would start to decode. The operation codes had some two-letter
mnemonics and so I could start to figure out ‚ÄúThis probably was a load
instruction, this probably was a branch.‚Äù And I knew it was a Fortran
compiler, so at some point it looked at column seven of a card, and that was
where it would tell if it was a comment or not.</p>
<p>After three hours I had figured out a little bit about the machine. Then I
found these big, branching tables. So it was a puzzle and I kept just making
little charts like I‚Äôm working at a security agency trying to decode a secret
code. But I knew it worked and I knew it was a Fortran compiler‚Äîit wasn‚Äôt
encrypted in the sense that it was intentionally obscure; it was only in code
because I hadn‚Äôt gotten the manual for the machine.</p>
<p>Eventually I was able to figure out why this compiler was so fast.
Unfortunately it wasn‚Äôt because the algorithms were brilliant; it was just
because they had used unstructured programming and hand optimized the code to
the hilt.</p>
<p>It was just basically the way you solve some kind of an unknown puzzle‚Äîmake
tables and charts and get a little more information here and make a
hypothesis. In general when I‚Äôm reading a technical paper, it‚Äôs the same
challenge. I‚Äôm trying to get into the author‚Äôs mind, trying to figure out
what the concept is. <strong>The more you learn to read other people‚Äôs stuff, the
more able you are to invent your own in the future, it seems to me.</strong></p>
</blockquote>
<p>Alas, if we‚Äôre to treat literacy in a human language as the combined skills of
writing <em>and</em> reading, why do we place so much emphasis on the former when it
comes to teaching how to code? I now actively seek out code to read for the
same reason Knuth mentions early in his interview; dispelling magic is an
invaluable skill we crucially need to keep improving. Treating things as a
black box may sometimes help reasoning but it doesn‚Äôt mean we should keep the
covers on until the end of the universe.</p>
<p>Take my <a href="https://j2kun.svbtle.com/mathematicians-are-chronically-lost-and-confused">favourite mathematical
post</a>
by Jeremy Kun in which he discusses, with a wonderful supporting analogy from
Andrew Wiles about stumbling around a dark house looking for light switches,
that feeling lost is far more common and acceptable than the enlightened state
we assume intelligent role models seem to possess. These role models have
simply learned to live with and accept the discomfort of being lost because
that‚Äôs what it means to be in a process of learning and growing!</p>
<p>Simon Peyton Jones is well known for stating how important it is to simply
<em>do</em>, no matter how humble the project in question may be. This is fantastic
advice for coding literacy; Writing this blog post involved an initial sit down
of a roughly one-thousand word brain dump followed thereafter by approximately
two days of refinements, with lots of rereading, simply honing in on the main
theme. It‚Äôs important to get fingers moving and code executing, but it‚Äôs just
as important to advocate to new starters that reading is something they should
be pouring time and attention into.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fail Fast not Error Out</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/fail-fast-not-error-out.html</link>
      <guid>https://justanotherdot.com/posts/fail-fast-not-error-out.html</guid>
      <pubDate>Sat, 07 Oct 2017 12:50:02 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>tl;dr</strong> Static analysis is a form of 'failing fast' that does not consist of
leaving error based exit strategies (which should be reserved for situations
where the program simply cannot transition to a new state) in code that will
eventually be shipped to production.</p>
<p>The notion of 'failing fast' in programming details finding faults at the
earliest possible time; when the application developer is fitting out the code!
This seems to be sensible, but is often strangely antithetical to the notion of
'the only true test of code is production data'; how can we fail fast and catch
a ton of bugs when the truly icky bugs we want to smash are after we've done
some kind of deployment? Clearly the distinction here is to find bugs, in any
context, as soon as possible, production or otherwise, but that does mean the
concept can be carried over to production, where failing fast could mean major
problems (payments not being processed, account information being leaked, etc).</p>
<p>Ops people have devised all sorts of methods to roll out code in deployment to
handle situations like this; blue-green deployments, canary deployments, et.
al. all focus on testing code on a much smaller subset (on some segment of
traffic) accepting <em>some</em> failure as an acceptable loss to know if the code is
ok enough to push to 100% of the traffic. Percentage deployments put a lot of
focus on monitoring and logging. Essentially, people have to watch the metrics
after the roll out to make sure everything is ok.</p>
<p>A computation does not need to crash the program in order to fail fast:</p>
<ul>
<li>
<p>Errors are for irrecoverable states of program transition; the program
depends on writing to disk for some critical task, and the disk has been ripped
out of the server rack and can no longer be accessed via the kernel drivers.
The kernel tells us something very bad is up, and we die. This is fine, because
there's no sensible state to transition to in this scenario.</p>
</li>
<li>
<p>Exceptions are for situations where something bad happened, but it's not bad
enough to cause us to fail completely, i.e. we can do something to transition
to another sensible step. The general frame of mind is that exceptions can be
problematic when they are not caught, but can be a pain to constantly look out
for (this is the source of the 'checked exceptions' controversy in the Java
community). The primary problem with exceptions is that if an exception is not
'checked' or 'caught', then it will bubble up to the main function (entry
point) of the program and cause it to error out as above. Exceptions are said
to be sensible if they preserve <strong>progress</strong> and <strong>preservation</strong>, meaning that
they are able to move forward and they don't manipulate the types of
expressions where they are thrown. In most languages, however, we can't be sure
if something is going to throw an exception, so many programmers are told to be
defensive and paranoid; hardly the kinds of things you'd want out of people who
need to also be innovative.</p>
</li>
</ul>
<p>In most pure functional programming languages, we know less about lurking
exceptions, and this is of particular importance. When we have a type system,
which is effectively a lightweight proof system that gives us static guarantees
and checks at compile time (a form of 'fail fast' but without the problem of
leaving 'ticking time bombs' in our code base that may still present themselves
in production), then it makes no sense to fail fast in an error-prone way.
Abstractions such as monads and friends allow us to do this elegantly and
tersely.</p>
<p>It is far more ideal to let pure computations transition gracefully to new
states, failures to be found at <em>compile time</em>, and production code to be
robust and resiliant. If we extend this notion of static analysis to property
based testing, formal correctness practices, and even linters, among other
things, there are several smarter alternatives to failing quickly and
validating the correctness of our programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Start</title>
      <author>spencer.ryanjames@gmail.com (Ryan James Spencer)</author>
      <link>https://justanotherdot.com/posts/hi.html</link>
      <guid>https://justanotherdot.com/posts/hi.html</guid>
      <pubDate>Sun, 17 Sep 2017 16:06:55 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I've managed to hack together this blog using Chris Penner's alternative to
Hakyll, <a href="https://github.com/ChrisPenner/SitePipe">SitePipe</a>, as well as using a
CSS framework I've wanted to try out, <a href="http://bulma.io/">Bulma</a>, and host it on
github pages. My plan is to:</p>
<ol>
<li>Move over to some other hosting platform to do my own infra (either Digital
Ocean or Linode w/ some kind of SSL reverse proxy.)</li>
<li>Tidy up a little bit of the layout around the site.</li>
<li>Write some content!</li>
</ol>
<p>I had written a <a href="https://medium.com/@justanotherdot/sapir-whorf-and-you-f4b45ff2f216">post over at
Medium</a>
and while I liked the overall editing experience, I knew I'd eventually find
some things lacking e.g. Mathjax integration and the like, of which I was able
to sort of hack into my <a href="http://justanotherdot.tumblr.com/">previous tumblr</a>
thanks to having access to the HTML of my blog, and is another thing I'll need
to add here.</p>
<p>When I was originally looking to make my own blog my ideal layout consisted of
Markdown being dumped as HTML which would be sent over some kind of ajax
request to fill in an SPA. That still may happen, but at the moment this
workflow seems fine, and I'm hoping I can fulfill a personal quota of 250 words
per week, but we'll see how that goes.</p>
<p>Also, there's something to say about how generic (and sometimes even bland)
some publishing sites make the blog-to-blog experience. Yes, it is just text
and media, but even a little bit of a personal touch goes a long way, I think.
I worry that, especially in the realm of tech articles, people will feel the
'overal sleek' experience of something like Medium will help carry the weight
of their voices beyond the actual rigor of their articles.</p>

        ]]></content:encoded>
    </item>
    
  </channel>
</rss>
