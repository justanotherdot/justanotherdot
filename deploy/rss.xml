<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>justanotherdot</title>
    <link>https://justanotherdot.com</link>
    <atom:link href="https://justanotherdot.com/rss.xml" rel="self" type="application/rss+xml" />
    <description>Personal blog of Ryan James Spencer</description>
    <category>Technology</category>
    <copyright>2018 Ryan James Spencer</copyright>
    <language>en-us</language>
    
    <item>
      <title>Mental cartography</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/mental-cartography.html</link>
      <guid>https://justanotherdot.com/posts/mental-cartography.html</guid>
      <pubDate>Thu, 24 Jul 2025 09:27:49 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>Organizations that learn are organizations that, luck granted, succeed.</strong> While success requires sharp teams, healthy cultures, sound strategies, and adequate resources, what separates thriving organizations from stagnant ones is their ability to adapt and learn from experience.</p>
<p><strong>But excellence requires action.</strong> Competitors are constantly innovating and improving. This is physics: systems naturally grow in entropy towards equilibrium. In other words, a system without intentional changes stabilizes, which is ultimately decline in a competitive environment.</p>
<h2>The framework</h2>
<p>The framework works through two complementary approaches:</p>
<ul>
<li>
<p><strong>Mental agility and cartography allows us to thrive in an uncertain world.</strong></p>
<ul>
<li>Failure is inevitable, but that shouldn't stop us from observing, deciding, and acting</li>
<li>We can be conscious of our feedback loops, and tighten them to rapidly learn</li>
<li>As we learn, we are building up, throwing out, and changing all kinds of maps to help guide us</li>
<li>Every round of a feedback loop is an experiment, and it is possible to design experiments to maximize learning</li>
<li>Every action we make has costs and risks we should be mindful of, but the most important cost is opportunity cost, and the most important risk is stagnation</li>
<li>Quitting and pivoting early is important to keep moving in the best direction</li>
</ul>
</li>
<li>
<p><strong>Keeping focus on outcomes and impact allows us to build products users love</strong></p>
<ul>
<li>Asking excellent questions is an art that leads to rigorous thinking, and the most important question is always &quot;why?&quot;</li>
<li>We focus on the work that matters, identifying opportunities that will give us an order of magnitude or more returns</li>
<li>High performing teams support one another, building each other up with true curiosity and kindness</li>
<li>We need to design to make quitting and pivoting trivial through concepts and systems thinking</li>
<li>To avoid getting lost in design, we need to make things tangible as quickly as possible</li>
<li>High performing teams know how to radiate intent, spread information, and realign continuously</li>
</ul>
</li>
</ul>
<p>Together, these approaches give you the tools to experiment with confidence rather than guess and hope.</p>
<h2>Mental agility and cartography</h2>
<blockquote>
<p><strong>The only man who never makes a mistake is the man who never does anything.</strong>
-Theodore Roosevelt</p>
</blockquote>
<p>You are going to fail. The sooner you can embrace this fact, the sooner we can get to the real work. No one has perfect predictive power, but we can sharpen our predictive abilities by being clear about our mental models and feedback loops, and ruthlessly updating each. The only true failure is failing to learn from the failure or mistake.</p>
<p><strong>None of us can tell the future, thus every decision we make is a bet.</strong> And whenever you are heading into the unknown, it helps to have a map. Whether we can find existing maps or have to make them ourselves matters little, as maps are tangible artifacts that we can share, increasing our collective knowledge rather than pooling that knowledge in a single person's head. A mental map is anything that helps you understand the system you're playing with or in.</p>
<p>Maps can be information dense and come in various types and sizes. There is no one true map, and &quot;the map is not the territory,&quot; as Alfred Korzybski put it, which is to say that maps can never exactly capture reality. Nonetheless, <strong>maps help tell us if we are on track and what alternative paths we can explore</strong>.</p>
<blockquote>
<p><strong>All models are wrong, but some are useful.</strong>
-George Box</p>
</blockquote>
<p>The good news is that all of us are fundamentally learning systems. We are always picking up new information that we can use to drive more accurate outcomes. We do this through identifying and optimizing feedback loops around us, which gives us richer, higher quality, and fundamentally more data we can choose to integrate with our maps. <strong>Consult your maps, make predictions and decisions, act, update your maps.</strong></p>
<p>Sound familiar? That's because it's effectively the scientific process. We take available data, make a hypothesis, run an experiment, and adjust our available knowledge based on the result. Each feedback loop provides different input or varying levels of fidelity. The tighter these loops, the more we are acting and, by extension, learning.</p>
<h3>Designing effective experiments</h3>
<p>In this sense, everything is an experiment. Experiments, and by extension bets, should be <strong>safe</strong>, <strong>small</strong>, <strong>cheap</strong>, and <strong>quick</strong>:</p>
<ul>
<li><em>Safe</em> means avoiding ruin, allowing us to see another day of experimentation</li>
<li><em>Small</em> means being minimal as can be to provide valuable insights, where size is a judgment call where time and resources are used wisely</li>
<li><em>Quick</em> means tightening the feedback loops such that we get information before the surrounding context changes altogether</li>
<li><em>Cheap</em> means using minimal resources (time, money, people) to gain valuable insights</li>
</ul>
<p>To maximize our learning, we should design experiments that:</p>
<ul>
<li>Take place in new territory with credible opportunity to advance toward desired goals</li>
<li>Are informed by available knowledge and hypothesis-driven</li>
<li>Exist in contexts that present opportunities to learn something meaningful</li>
<li>Are small, cheap, safe, and fast to execute</li>
</ul>
<p>And for any bet we make, it is imperative we ask &quot;why?&quot; If we aren't sure why we are doing something, we are most likely reacting, and if we are reacting we won't be learning as much.</p>
<h3>When to quit and pivot</h3>
<p>The only mindsets worth keeping are the <em>useful</em> kind. Making steady progress means being quick to quit or pivot from our efforts when they are no longer serving the intended purpose or achieving the predicted outcomes we set out to achieve. One way to know when to quit, pivot, or discard is to decide on &quot;kill criteria&quot;—chosen signals that, if identified, would allow us to move onto other more critical tasks. Good kill criteria are straightforward to measure, have high signal-to-noise, and are fundamentally tied to explicit decisions that help change course.</p>
<p>In due time, with more bets hitting the mark, we make steady progress like a ratchet, snapping into place as it is wound, never unwinding unless the tension is released on purpose. This requires being vigilant with our subconscious and being willing to make change.</p>
<p>Being exceptional at running experiments and making changes means being exceptional at considering the costs and returns of our efforts. We must consider hidden costs such as opportunity cost—what we give up by spending time on the wrong things. By taking calculated risks and making intentional decisions, we avoid the biggest risk of all: <strong>stagnation and irrelevance in a changing world</strong>.</p>
<h2>Outcomes and impact</h2>
<blockquote>
<p><strong>It is possible to commit no mistakes and still lose. That is not a weakness; that is life.</strong>
-Jean-Luc Picard</p>
</blockquote>
<p>As we make steady progress, we need to be mindful of the outcomes and impact of our work. Success requires mental flexibility—the ability to discard and rebuild our mental models, test our claims, and adapt no matter how much uncertainty or ambiguity we face.</p>
<h3>The art of asking why</h3>
<p>It all starts with the <em>why</em>. To do our best work we must understand the motivating drivers deeply, which will allow us to work <em>intentionally</em>. Deliberate action means we aren't spinning in circles, doing work that doesn't push us further toward being an elite team with exceptional outcomes.</p>
<p>To discover these motivations, we must practice the art of asking incredible questions. Part of this art is possessing genuine curiosity and wonder for the world around us. To be exceptional, we must deeply want to support one another, and that means helping each other by being <em>kind</em> as opposed to nice, candidly expressing our thoughts to help build up and refine the thinking and work of our teammates.</p>
<p>This rigor can also be applied to what outcomes we want to pursue, pruning the outcomes that don't lead to an order of magnitude or more improvement. We focus on the work that matters, identifying opportunities that will give us outsized returns and starting with the critical tasks first.</p>
<h3>Systems thinking and design</h3>
<p>We need design to make quitting and pivoting trivial. But how do we actually do this without creating chaos or wasted effort?</p>
<p>This is where systems thinking becomes powerful. A system is a collection of elements with relations between them. This simple but powerful idea lets us view the world as interacting systems, cutting through complexity to see the essential patterns underneath.</p>
<p>Given this understanding, we can approach designing differently. By designing, we mean strategic architecting of your approach to encourage rapid creation and iteration. The emphasis is on designing for flexibility and learning—you're designing the way you'll figure things out as much as the final outcome. This means building modular, adaptable solutions where you can create, delete, and change things freely as you learn. This applies whether you're designing your approach, your solutions, or both.</p>
<p>Once we have motivations and rigorous thinking, we can start designing. The crux of good design is understanding the underlying concepts, and from there we can begin sketching out several paths to achieving the outcome we want. Having many options available gives us optionality—the characteristic of considering and paying for many options upfront, again allowing us to quit or pivot on a moment's notice.</p>
<p>By viewing our work as systems—collections of elements with relations between them—we can create modular, adaptable solutions by treating the parts as building blocks we can reconfigure and reuse. Once we have modular pieces, we can lean heavily into composing elements together. A small number of powerful patterns, technologies, and ideas used more heavily, rather than a large number of specialized tools, establishes a means for us to build on our prior work rather than building from scratch every time.</p>
<h3>Making things tangible</h3>
<p>To avoid getting lost in design, we need to make things tangible as early as we can, linking up initial functionality for ourselves to continually iterate. This isn't to say focus on the <em>easy</em> work upfront; sometimes we need to roll up our sleeves and focus on the hardest work first to get the largest gains. <strong>Do the hard things up front; time will come for the easier things, but they will distract you early on.</strong></p>
<h3>Communication and alignment</h3>
<p>High performing teams know how to radiate intent, spread around information, and repeat their messages enough to continually realign over and over again. Be clear about what you're doing, collaborate a lot with people, and don't be reserved in what you communicate. <strong>More information means more alignment, and more alignment means more autonomy.</strong></p>
<h2>Practical implementation</h2>
<p>To make this actionable, consider these steps:</p>
<p><strong>Map creation and maintenance:</strong></p>
<ul>
<li>Document your current understanding of the problem space</li>
<li>Identify what you know, what you don't know, and what you think you know</li>
<li>Share maps with your team to build collective intelligence</li>
<li>Schedule regular map reviews and updates</li>
</ul>
<p><strong>Designing effective experiments:</strong></p>
<ul>
<li>Define clear hypotheses before testing</li>
<li>Establish measurable success criteria upfront</li>
<li>Set kill criteria—specific conditions that would indicate it's time to pivot</li>
<li>Choose the smallest viable test that provides meaningful data</li>
</ul>
<p><strong>Building feedback loops:</strong></p>
<ul>
<li>Identify key metrics that indicate progress toward your goals</li>
<li>Create regular check-ins to review results and update predictions</li>
<li>Build systems that surface problems early</li>
<li>Cultivate environments where honest feedback is encouraged</li>
</ul>
<p><strong>Learning from outcomes:</strong></p>
<ul>
<li>Conduct post-mortems on both successes and failures</li>
<li>Document what worked, what didn't, and why</li>
<li>Update your mental models based on new evidence</li>
<li>Share learnings across the organization</li>
</ul>
<p><strong>Focusing on impact:</strong></p>
<ul>
<li>Start every initiative by clearly articulating the &quot;why&quot;</li>
<li>Prioritize work that has potential for order-of-magnitude improvements</li>
<li>Design systems for easy pivoting and iteration</li>
<li>Make abstract concepts tangible as quickly as possible</li>
</ul>
<p>In summary, to be exceptional we must take calculated risks in the form of decisions. To make exceptional decisions we need to thrive in ambiguity. To thrive in ambiguity we need to construct mental maps, identify and optimize feedback loops, and make and reflect on our predictions. Feedback loops are set in motion by deciding and acting: <em><strong>consult your maps, make predictions and decisions, act, update your maps.</strong></em></p>
<p><strong>By holding these mindsets and principles, we can move mountains—achieving the kind of transformational progress that separates good organizations from truly exceptional ones.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Record your demos</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/record-your-demos.html</link>
      <guid>https://justanotherdot.com/posts/record-your-demos.html</guid>
      <pubDate>Thu, 01 Feb 2024 20:40:20 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you are on a team and are tasked with delivering code, record or document
your deliverables. </p>
<p>Do this when you finish whole initiatives as part of closing bookends,
<em>but also</em> as you make progress at stepping stones along the way.</p>
<p>Every kind of artifact is fair game for a demo: design documents, diagrams, videos, CSVs of
data, dashboards, and so on. Be creative, but be mindful that the outcome here
is to align others against a sprawling, oft changing technical and product landscape;</p>
<p>Demoing work is about sharing knowledge, not
only about showing completed work with a bow on it.</p>
<p>But why <em>document</em> your demos?</p>
<p>Documenting your demos removes the pressure of the &quot;big bang&quot; deliverables and
alleviates pressure on your team mates to synchronise to get up to speed.
Leaving artifacts allows others to consume in the time that suits them, and
picking various mediums, whether that be writing, video, or visuals, allows
people to ingest the way that suits them. Experiment and find what best suits
your team.</p>
<p>Demos are also useful to regularly present persuasive content and architectural
decisions to others. We make decisions every day, and sometimes exhaustion from
messaging tools can mean your attempts at overcommunication become white noise
to others. </p>
<p>When picking up work it can help to contemplate &quot;how can I demo this at the end?
How can I demo this after each chunk?&quot;. This is not dissimilar from asking how
you might make a system or code easier to test, observe, extend, or modify.
Perhaps it's easier to showcase your API by writing a quick CLI, or cobbling
together a frontend may give an inlet with your data.  Favor lightweight tools;
If making a frontend sounds too heavyweight, use something like retool to demo a
&quot;live view&quot; of a database table, for example, or draw a diagram during a video.
Remember the aim is to convey a message, not build a masterpiece.</p>
<p>In the end, value thy readers time. Whatever your decide to present should be
quick, concise, and distilled. Think &quot;tiktok&quot; not &quot;The Hobbit&quot;. You are looking
to convey the kernel of knowledge you have at hand, not the comprehensive
breadth of everything that's been accomplished. A good writeup or video should
be paired with a space for Q&amp;A, allowing people to ask followup questions. Or,
if asynchronously shared, make sure there is a similar place others feel
questionable to participate around the material. Engagement means further
alignment and a greater likelihood that your audience will remember your
message.</p>
<p>Overcommunication is the essence of good advertising. When people complain that
they do not (yet) understand the architecture or the conceptual mapping to
software and so on, it is because they have not had it drilled into them yet. An
essential caveat to this advice is to avoid yanking around the attention of your
peers. Take the advice of good MVPs and aim to create &quot;SLCs&quot;; Simple, Lovable,
and Complete.</p>
<p>I'll leave you with some practical tools to use:</p>
<p><strong>For recording video and audio of your screen, as well as screenshots:</strong></p>
<ul>
<li>cleanshotx</li>
<li>loom</li>
<li>screen studio</li>
</ul>
<p><strong>For sharing articles:</strong></p>
<ul>
<li>notion</li>
<li>coda</li>
<li>substack</li>
<li>markdown gists</li>
<li>markdown in repos</li>
<li>google docs</li>
</ul>
<p><strong>For diagrams and live drawing:</strong></p>
<ul>
<li>whimsical</li>
<li>miro</li>
<li>asciiflow</li>
<li><a href="https://gist.github.com/justanotherdot/b049f9e607543225ac7c383c64b8f2ef">unicode to make boxes</a></li>
</ul>
<p>Be kind to your peers, record your demos, and share your gold with your team.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Split over join</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/split-over-join.html</link>
      <guid>https://justanotherdot.com/posts/split-over-join.html</guid>
      <pubDate>Fri, 08 Sep 2023 20:19:06 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you're a software developer, you've likely been there; the manic, not to
mention frantic, working through complex input cases and systems interactions,
all to fix or extend some potentially tiny bit of code. This sort of &quot;global
reasoning&quot; is often considered business as usual for software systems at a
certain size, and I'm genuinely surprised there isn't more active discussion
around the tenants of local reasoning. In fact, some time back I was part of a
meeting discussing what we, software developers, each felt was important for
software quality. When it came my turn, I said &quot;local reasoning&quot; only to
confused faces. </p>
<p>All that local reasoning means is that you can infer your code is correct using
local information, without having to think about every input permutation or
complex ordering of system processes or interfaces all fitting together. I find
this to be a huge part of my toolkit of writing software of reasonable quality,
and I believe in it enough that I started writing some articles and a talk on
it.</p>
<p>Part of local reasoning is being able to do what's called <em>case analysis</em> where
we can implement and reason about individual situations; for example, if we had
a vending machine, we might want to process the steps individually, such that if
we're in a &quot;vendoring&quot; stage, we wouldn't want to think about the &quot;payment
processing&quot; stage sets, or vice versa. </p>
<p>And while I love local reasoning, this article isn't where I go over those
foundations. Instead, I wanted to talk about the software design pattern of
favoring splitting out cases and over trying to prematurely generalize. I like
to call this &quot;joining&quot; code as generalized cases are better thought as trying to
join together several special cases by finding the common points, but when we
rush into a general implementation, we may not understand enough of that common
surface area, and mistake the one or two cases we have as being representative
of the larger population. </p>
<p>Why do we generalize in the first place? Many developers are told &quot;don't repeat
yourself&quot; but forget this advice is for refactoring <em>after</em> the code is in
place; identifying the places where we've split things out, where the same thing
is largely being said, but in insiduously different ways that can lead to bugs.
We are also ushered to consider joint abstractions early on, where we can model
multiple things under a canonical notion or concept, but, as with software,
splitting and joining is cheaper than prematurely joining and having to tease
apart ex post facto. Teasing apart is a sharp corner that leads us down
complicated, highly orchestrated migrations, the kind that take months, if not
years, to finalise, if and only if they manage to not get stalled! This is
partly why, although joining is cheap, joining is not always a reversible
decision.</p>
<p>Summarily:</p>
<ul>
<li>Splitting is cheaper than teasing apart premature general abstractions</li>
<li>Joining is cheap and easy once we have enough specific cases to identify the
common points between many cases. <a href="https://www.oreilly.com/library/view/the-rules-of/9781098133108/">Some suggest three as a good number for
when to trigger this
conversion.</a></li>
<li>Local reasoning is awesome, and case analysis is but one part of the
foundations that which allow us to reason about code <em>sanely</em>
without having to tear our hair out about every imaginable configuration of
state, inputs, or system interactions. Each specific case we reason about is 
isolated from other cases. Splitting out is similarly about identifying the
concepts that can be isolated and, well, isolating them.</li>
</ul>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Advertise work interests and ignore proficiencies</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/interests-not-experience.html</link>
      <guid>https://justanotherdot.com/posts/interests-not-experience.html</guid>
      <pubDate>Sun, 02 Jul 2023 20:06:56 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>People working in tech often, but perhaps not always, want to show off their proficiencies. Advertising these proficiencies is a tool used to demonstrate capability to those with promotion power, usually in hopes that the advertisement will lead to involvement in activities that they can bundle in a brag document of some kind. The more &quot;altruistic&quot; are those who advertise so that they can feel useful to the business and others, sharing their expertise in the hopes that they can inadvertantly help mentor their &quot;lesser experienced&quot; peers, and also incidentally create an environment where they can get thick with the subject matters that they find most interesting, and, therefore, have the most experience in.</p>
<p>Therein lies the crux of what is preferable to advertising proficiencies; interests. Whereas advertising proficiencies is veiled in an aura of &quot;look at me&quot;, advertising interests is more about openess and curiosity, seeking a richer, interconnected experience where we know ourselves, and others, better. More importantly, in a work environment, making clear to others about your interests levels the playing field for <em>everyone</em> on a team, no matter their proficiencies, to continue to improve and excel.</p>
<p>Consider this: a junior developer is hired onto a team. She has some minor experiences, but no major proficiencies. If she gets practice, she <em>will</em> develop proficiencies, but she may not <em>want</em> to haphazardly be thrown onto various projects and may have some sense of what she wants to work on. Perhaps she is keen on becoming a better frontend engineer, or she wants to build CLI tooling, or work on distributed systems. And most importantly, these interests are not static, they may shift over time, or change depending on the things that she ends up working on with the initial interests she had. She can practically advertise her interests by saying &quot;hey, I'd love to be tagged on pull requests for this language!&quot; or &quot;can I be a fly on the wall for meetings about technical designs?&quot; Conversely, it is essential to foster an environment where she <em>can</em> advertise these interests and <em>not get shut down or ignored</em>. </p>
<p>Teams that are supported in working in this way are less prone to building up walls between each other. This is an important infraction of Conway's Law, especially if you have cognisantly built an organisational structure around specific communication flows; there is always an underlying fabric of The One Team, and that one team should share and delight in each other's learnings, no matter how seemingly trivial they are. </p>
<p>Leaders can foster this environment by modeling this behavior themselves, role modeling that insights are always welcome on their own pull requests, design docs, and so on. A little it of sunlight, a sprinkle of dissent, a dash of curiosity, all from colleagues who have an equal interest on a subject, will enrich the work being done. Perhaps the largest resistance to this will be the time it may consume or lack of precision it may involve, but at that stage there are other practical methodologies to explore from professional toolkits: <a href="https://mollyg.substack.com/p/decision-making">timeboxing feedback, clarifying key decision makers versus contextual advisors, and urgency to autonomy grids come to mind</a>. Critically, the aim of this is to foster an acceptance to sharing information, and an acceptance to receiving information, as a sharing of ideas, breaking down entrenched walled gardens built up around experience, tenure, or profeciencies. </p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Simulate</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/simulate.html</link>
      <guid>https://justanotherdot.com/posts/simulate.html</guid>
      <pubDate>Tue, 06 Jun 2023 22:29:55 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Software (professionally) involves a lot of theorizing; doing X and Y will roll
up to a feature we expect users to love and, thus, will want to pay us gobs of
money to use to alleviate their pains and/or make them feel like a superstar. </p>
<p>Sometimes the theorizing feels airy, and we try to tether it to real customers,
real (anonymized) user stories, real data of some kind. </p>
<p>There’s another way we, at work, started trying that I quite like; run
simulations (i also call these “drills”). A simulation is a bit like dogfooding
but it is executing the full funnel from start and finish yourself, whether or
not parts are manual or automatic. It sharpens the narrative and is highly
guided. I read Build by Tony Fadel last year and loved it, in particular with
the mantas of making things tangible and being crystal clear about the narrative
for a customer (what others may call “the customer journey”) without excluding
parts such as product awareness or pre-sales,  all the way to being furious with
the product and wanting to ring up to demand a refund or throw the damn thing
out a window. </p>
<p>The <em>whole</em> story means we get a holistic picture, whereby we identify failures
on our (product development) end, as well as opportunities for improvement, and
we better get into the shoes of the user, rather than it being hypothetical. </p>
<p>Does every product fit this simulation shoe? Maybe not. I am hopeful it does,
because it makes so much work realistic, tied to real outcomes that impact this
journey, and improving a <em>shared</em> understanding of empathy of what the user is
feeling. </p>
<p>On the note of tangibility, you can’t go on this journey as a thought
experiment, but you need to fill in the gaps to make it something that involves
actual interactions. Running simulations exposes what is and isn’t tangible, and
what parts of the interfaces need to be shored up. </p>
<p>A cadence of simulation to development might make sense, too, where the
development works on identified tasks from the simulation, and the process
repeats for the next iteration of essentials or improvements.</p>
<p>The one part I am unsure of is whether this is a long term process, or if it’s
only needed <em>now</em> while we are continuing to build up product market fit. But
alas, simulations are cool, and maybe you can give them a try, too, and let me
know what worked or didn’t. </p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Cleanliness through bankruptcy and tethers</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/bankruptcy.html</link>
      <guid>https://justanotherdot.com/posts/bankruptcy.html</guid>
      <pubDate>Tue, 24 Jan 2023 20:26:06 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Cleaning makes my mind nimble. Untethered by the visual distractions of my
environment, I am free to focus on the tasks that I set out to accomplish.
Cleanliness is easier to acheive if you've identified a safe way to give up on
the mess.</p>
<p>One way I clean digital clutter is to concoct lists, but then my time is sunk in
curating lists. This is no good; I want to spend my time savoring the things I
love, not acting as a digital packrat. &quot;Getting things done&quot; was popular because
it provided a system centered around psychological safety. In practice, the
attention to queues taxes the attention on task, and we wind up in the same
place. This doesn't mean that psychological safety is the problem, but it does
indicate that that the &quot;getting things done&quot; process doesn't accomplish that
aim. I prefer no frills processes that I can trivially remember and rally around
the notion of checkpointing. Checkpointing serves the basis for incremental
progress. The point of checkpointing is to record details that we want to keep,
but it also serves as a threshold for rejecting additional weight, as well. In
this way, checkpointing is about regular cleanliness, through determining the 1%
or less you want to keep, and declaring bankruptcy on the rest. Blow away tabs,
bulk quit programs, throw it all in the bin; minimalism calls for a touch of
sophisticated savagery.</p>
<p>Another example of employing psychological safety in this way comes in the form
of reading. I will try to start books by reading cover to cover, but I may get
bored, or the outcomes I set out to acheive for this reading experience aren't
materialising. As such, I have to accept that I am going to possibly be left to
sludging through this, but I don't have to; instead, I can explore ahead,
seeking out if things improve, gleaning any extra details before I give up, but
something in me wants to stay, wants to take the scenic route. At this stage
leave a marker, usually a post-it note, after which I move forward and employ
all the common tricks: I skim, I read first and random sentences seeing if
anything leads me in, I try to discover anything that &quot;catches&quot; my attention
like a fish in an exciting way. Effectively, I turn the knob on my pace up,
inceasing it unless something slows me down. I call this process &quot;the towline&quot;
or &quot;tether&quot;.</p>
<p>I look back on some of these experiences and realise that if I had not done
this, I would have spent an entire week, month, or year on the book in question.
My focus is on outcomes from what I'm reading, not on the number of books I'm
reading. As a community of readers, we need to dispel this myth that books can't
be read in odd ways: reverse, randomly, evenly spaced. There are other tricks to
build psychological safety, but the principle is the same in each; establish the
courage to throw it all away by giving yourself enough that you can keep to make
it feel like you aren't losing everything.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Things I love</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/things-i-love.html</link>
      <guid>https://justanotherdot.com/posts/things-i-love.html</guid>
      <pubDate>Mon, 16 Jan 2023 20:06:13 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It has taken me awhile to summarise 2022. I intended to finish writing my book,
&quot;Just Perf&quot;, but family took precedence, as it always should. Instead, I
drastically improved my curation skills, and did get a lot of writing practice
in, while also honing my craft at work. </p>
<p>Admittedly, I was feeling a bit burnt-out, as one does from time to time working
in tech, and I decided at the start of 2022 that I would commit to making the
things I love <em>explicit</em>. Every time I found something I loved, into a note it
went. Bear has this neat functionality on mobile I use a lot, where the <code>share</code>
functionality can be chosen to append to the end of a note. This is handy for
general link dumping, but in this case it it allowed me to focus media to a
particular bucket, as you can edit the text before dumping if you want. Doing
this sharpened my sensibilities, and I've since taken the practice for other
things in my life that I want to be sure of what is uniquely <em>me</em> and not simply
me copying others templates. I also had read The Practice by Seth Godin in which
he has a small section dedicated to adopting an abundance mindset; I love the
idea of holding onto an abundance mindset now, and I think hoarding a treasure
trove of what I love is the wrong thing to do, hence I am here sharing it with
you, dear reader.</p>
<p>It is now nearing the end of of the first month of 2023, and I felt it
appropriate to get on with it and share this list. Things will inevitably
change, and this list may contian things I will fall out of love with, but
that's ok; I'm human and I'm growing. I will gradually refine this list to more
than a link dump as a I revisit the content and reflect. I will be repeating
this process again, and, indeed, there are some links at the tail end of this
list that were thrown in this January. Hopefully you can mind a bit of my mess
as I clean things up in public, but feel free to come back and see how things
have changed. This foreword will likely adapt as time goes on, too. We need more
living documents.</p>
<p><strong>NB.</strong> <em>With the inevitable collapse of twitter on its way, many of these twitter
links will be converted into screenshots for posterity. I also am keen on the
idea that if it doesn't fit in a screenshot or two, I don't think it's small
enough to love.</em></p>
<p>Without further ado:</p>
<h2>2022</h2>
<h3>The isle of twitter</h3>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;ve found that variations on &quot;what are we trying to do here?&quot; work amazingly well in a bunch of situations<br><br>As long as I keep context and tone in mind, people don&#39;t take it the wrong way. I want it to feel like I&#39;m asking somebody to help solve a problem, not accusing them.</p>&mdash; Tikhon Jelvis (@tikhonjelvis) <a href="https://twitter.com/tikhonjelvis/status/1481118756491304962?ref_src=twsrc%5Etfw">January 12, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;ve also started just asking people &quot;what would you like me to do?&quot;<br><br>It&#39;s a great question when I think the answer is &quot;nothing&quot;: if I&#39;m right, people will convince themselves that&#39;s the case; if I&#39;m wrong, I&#39;ll learn about what they&#39;d like me to do :).</p>&mdash; Tikhon Jelvis (@tikhonjelvis) <a href="https://twitter.com/tikhonjelvis/status/1481119833051713538?ref_src=twsrc%5Etfw">January 12, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">when someone makes you feel safe. safe to express. safe to feel. safe to speak. safe to think. just safe. i love that. i love an emotionally validating human. i love an active listener. i love reciprocity.</p>&mdash; gaia. (@gaialect) <a href="https://twitter.com/gaialect/status/1485498360954068993?ref_src=twsrc%5Etfw">January 24, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">As a senior engineer, you should learn how to be less reactionary and more strategic. Reactionary behavior creates tactical solutions. Tactical solutions create more problems. Learning to think big and fix foundational issues sometimes makes hard problems disappear.</p>&mdash; Jaana Dogan ヤナ ドガン (@rakyll) <a href="https://twitter.com/rakyll/status/1483892728828071936?ref_src=twsrc%5Etfw">January 19, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I wish more people isolated their business logic from their asynchronous interactions through a more explicit partially ordered local inbox that can be trivially scrambled and dropped during tests for getting sooooo many of the bugs in distributed systems to quickly pop out</p>&mdash; @Spacejam@mastodon.social (@sadisticsystems) <a href="https://twitter.com/sadisticsystems/status/1493164859747360769?ref_src=twsrc%5Etfw">February 14, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">.<a href="https://twitter.com/paarsec?ref_src=twsrc%5Etfw">@paarsec</a> and I made an experimental short film: Softly, the current may bend our bodies. It looks at the connection between people and places, nature, water, touch. Watch it for free here: <a href="https://t.co/zz4rvnJGtf">https://t.co/zz4rvnJGtf</a></p>&mdash; Sy Brand (@TartanLlama) <a href="https://twitter.com/TartanLlama/status/1495428416312598535?ref_src=twsrc%5Etfw">February 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">35. Antiroutine:<br>To create original output, consume unusual input. Avoid trending videos, NYT bestsellers, widely cited papers. Instead. read ignored texts, plumb the past for forgotten ideas. Step outside the zeitgeist so you can see it with fresh eyes.<br><br>h/t: <a href="https://twitter.com/mmay3r?ref_src=twsrc%5Etfw">@mmay3r</a></p>&mdash; Gurwinder (@G_S_Bhogal) <a href="https://twitter.com/G_S_Bhogal/status/1492256289149366274?ref_src=twsrc%5Etfw">February 11, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The goal of Calvinball isn&#39;t to &quot;win&quot; the goal is to play. It&#39;s an infinite game, proper. You &quot;win&quot; by advancing the game. You advance the game by making a good move. A good move is surprising, insightful, unexpected, delights the audience, delights the players.</p>&mdash; Rival Voices (@nosilverv) <a href="https://twitter.com/nosilverv/status/1508969954451861515?ref_src=twsrc%5Etfw">March 30, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The wind&#39;s song 🍃<a href="https://twitter.com/hashtag/Pleinairpril?src=hash&amp;ref_src=twsrc%5Etfw">#Pleinairpril</a> day 02 <a href="https://t.co/WA5TCI89Q0">pic.twitter.com/WA5TCI89Q0</a></p>&mdash; Claire 🐇☀️ 🍰🍰🍰 (@Lilblueorchid) <a href="https://twitter.com/Lilblueorchid/status/1510685183413993481?ref_src=twsrc%5Etfw">April 3, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">your life is not a thing to do something with, it&#39;s not a resource to be allocated efficiently (this itself is known to induce despair), your life is the domain in which you exist<br><br>it&#39;s not a mathematical equation to be solved with any sort of finality, but a tension to be danced</p>&mdash; Visakan Veerasamy ⛵️ (@visakanv) <a href="https://twitter.com/visakanv/status/1518867434329239552?ref_src=twsrc%5Etfw">April 26, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">FOUR Productivity FEYNMAN- strategies: <br>i) Stop trying to know-it-all. <br><br>ii) Don&#39;t worry about what others are thinking. <br><br>iii) Don&#39;t think about what you want to be, but what you want to do. <br><br>iv) Have a sense of humor and talk honestly. <a href="https://t.co/COQU9HHkdx">pic.twitter.com/COQU9HHkdx</a></p>&mdash; Prof. Feynman (@ProfFeynman) <a href="https://twitter.com/ProfFeynman/status/1093534576679489536?ref_src=twsrc%5Etfw">February 7, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">My Rust database projects will continue. The next up: merging the <a href="https://twitter.com/komora_io?ref_src=twsrc%5Etfw">@komora_io</a> fragments into sled :) My recent fixation on rewriting things until I can write them in about a day (or break them into pieces that are small enough to do so) really seems to be working well w/ low time.</p>&mdash; @Spacejam@mastodon.social (@sadisticsystems) <a href="https://twitter.com/sadisticsystems/status/1527576338328174592?ref_src=twsrc%5Etfw">May 20, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Fascinating HN comment from someone who&#39;s company built a custom distributed data warehouse using compressed SQLite DB files in S3 that were queried using Lambda functions orchestrated by PostgreSQL running a custom foreign data wrapper <a href="https://t.co/gvRviN8D9R">https://t.co/gvRviN8D9R</a> <a href="https://t.co/YC8iNAyFxB">pic.twitter.com/YC8iNAyFxB</a></p>&mdash; Simon Willison (@simonw) <a href="https://twitter.com/simonw/status/1529134311806410752?ref_src=twsrc%5Etfw">May 24, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Compass computer for GRiD Systems. Designed by Bill Moggridge. Palo Alto, CA, 1982 <a href="https://t.co/fIlBxFKesB">pic.twitter.com/fIlBxFKesB</a></p>&mdash; femb✦t (@__femb0t) <a href="https://twitter.com/__femb0t/status/1527884733614784512?ref_src=twsrc%5Etfw">May 21, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Brian Eno on creating the Windows 95 startup sound <a href="https://t.co/u64NsnkPhm">pic.twitter.com/u64NsnkPhm</a></p>&mdash; 𝗗𝗘𝗔𝗗𝗦𝗢𝗨𝗡𝗗 💀 (@DeadsoundApp) <a href="https://twitter.com/DeadsoundApp/status/1530662088829452288?ref_src=twsrc%5Etfw">May 28, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">These structures can get big! But even zoomed out, seeing the general shape of something can help you when debugging. When you&#39;re familiar<br>with how things look, you can spot patterns in what changes. Our brains are good for that! <a href="https://t.co/2AJWViF1Pk">pic.twitter.com/2AJWViF1Pk</a></p>&mdash; Kate (@thingskatedid) <a href="https://twitter.com/thingskatedid/status/1386084600879804416?ref_src=twsrc%5Etfw">April 24, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">🧵 Make yours and everybody else&#39;s lives slightly less terrible by having all your programs print out their internal stuff as pictures; ✨ a thread ✨ <a href="https://t.co/NjQ42bXN2E">pic.twitter.com/NjQ42bXN2E</a></p>&mdash; Kate (@thingskatedid) <a href="https://twitter.com/thingskatedid/status/1386077306381242371?ref_src=twsrc%5Etfw">April 24, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I have deep respect for people in tech who have a sense of _taste_ and use it to guide judgement (whether in design/engineering/product/whatever). It’s a rare quality, and it’s very few environments that allow such people to thrive, but it’s pretty marvellous when it does happen.</p>&mdash; sunil pai, inc. (@threepointone) <a href="https://twitter.com/threepointone/status/1532130941359083525?ref_src=twsrc%5Etfw">June 1, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I used to have a strict plan for what I wanted to do in my career and how perfect it would look.<br><br>Now I proudly don&#39;t have a 5 or 10 year plan.<br><br>I know the type of impact I want to make, doing work I&#39;m great at, with people I respect and who bring me joy.<br><br>That&#39;s enough. 💜</p>&mdash; Wes Kao 🏛 (@wes_kao) <a href="https://twitter.com/wes_kao/status/1533116325412761601?ref_src=twsrc%5Etfw">June 4, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">NORMALIZE THIS. <a href="https://t.co/BRZLzKM0lc">pic.twitter.com/BRZLzKM0lc</a></p>&mdash; Jorge Castro ❤️ 🇺🇦 (@castrojo) <a href="https://twitter.com/castrojo/status/1537613916418760705?ref_src=twsrc%5Etfw">June 17, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">These &#39;Beach Animals&#39; were created by Theo Jansen as a fusion of art and engineering. The kinetic structures walk on their own and get all their energy from the wind.<a href="https://t.co/1m2JvPXUSB">pic.twitter.com/1m2JvPXUSB</a></p>&mdash; Wonder of Science (@wonderofscience) <a href="https://twitter.com/wonderofscience/status/1541057441663688706?ref_src=twsrc%5Etfw">June 26, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In my constant scouring of Pinterest for fantasy fashion inspiration, I found the account of the Islamic Fashion Institute (it&#39;s a Muslim fashion design school in Indonesia) and it&#39;s full of images of absolute jaw dropping beauty and coolness and incredible design <a href="https://t.co/p3n5ZR3GSb">pic.twitter.com/p3n5ZR3GSb</a></p>&mdash; TREZ MAKES ART AND RIFFS 🥟 (@GameResTrez) <a href="https://twitter.com/GameResTrez/status/1567707956292251648?ref_src=twsrc%5Etfw">September 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Confident leaders acknowledge what they don’t know, work hard to acquire that knowledge, and believe in their ability to learn. This is how I ultimately unlocked my career.<br><br>Honesty engenders trust.<br><br>Humility opens the door to learning.<br><br>Curiosity leads to understanding.</p>&mdash; Dan Rose (@DanRose999) <a href="https://twitter.com/DanRose999/status/1568022680951201792?ref_src=twsrc%5Etfw">September 8, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Programming isn&#39;t so much about conveying to machines instructions for how to do a thing. It&#39;s more about building clear mental models of the thing. That&#39;s the hard part. Writing the code and running it can be a tool to shape your own thinking.</p>&mdash; François Chollet (@fchollet) <a href="https://twitter.com/fchollet/status/1578048460204675075?ref_src=twsrc%5Etfw">October 6, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">There is an art to replying and commenting, and probably like 60-70% of people I’ve seen on the internet fail at it. The important thing is not to speak your mind, but to “support” the OP. You can support them by disagreeing well &amp; you can “mis-support” them by agreeing stupidly</p>&mdash; Visakan Veerasamy ⛵️ (@visakanv) <a href="https://twitter.com/visakanv/status/1039420186586038273?ref_src=twsrc%5Etfw">September 11, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">People underestimate how much a “I believe in you” or “I will support you” means to someone who has uncertainty at the job or in life.</p>&mdash; Bryan Liles (@bryanl) <a href="https://twitter.com/bryanl/status/1605699998657777664?ref_src=twsrc%5Etfw">December 21, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">every moment is an opportunity to choose differently</p>&mdash; Visakan Veerasamy ⛵️ (@visakanv) <a href="https://twitter.com/visakanv/status/1580463851438342144?ref_src=twsrc%5Etfw">October 13, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Small peek at the unnecessarily complex and very bespoke soup shader I made for this <a href="https://t.co/xrIYcELF4W">pic.twitter.com/xrIYcELF4W</a></p>&mdash; Harry 💬 (@HarryAlisavakis) <a href="https://twitter.com/HarryAlisavakis/status/1545734124656279553?ref_src=twsrc%5Etfw">July 9, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
<h3>Quotes</h3>
<blockquote>
<p>We’re only as strong as the people around us let us be. Which is why it
matters that we can now shape our power, by shaping our community and context.
Where is it you can be you? Who does that for you? When you realize identity
is fluid, contextual and shapes what you can do, you can also put yourself
into a context that enables all of you. - Nilofer Merchant</p>
</blockquote>
<blockquote>
<p>yeh, also we're blind to abuse from people we admire -Max Rozen</p>
</blockquote>
<blockquote>
<p>A key engineering skill is noticing what’s blocked, and figuring out why,
following a thread of hints from person to person until you can see what’s not
moving. You might end up asking the same question again and again. You might
get handwavy responses that seem like answers but don’t actually give you
extra information. That’s how these games go.<br />
-from <a href="https://noidea.dog/blog/surviving-the-organisational-side-quest">Surviving the Organisational Side Quest — No Idea Blog</a></p>
</blockquote>
<blockquote>
<p>Base your priorities on company priorities and be relentless about revisiting
them. A clear understanding of priority will help clarify which projects to
start, stop, abandon or finish. And it’ll help avoid the worst scenario where
your team does great work on an ultimately meaningless task.<br />
-from <a href="https://blog.danielna.com/understanding-project-management-will-improve-your-developer-job/">Understanding Project Management Will Improve Your Developer Job | blog.danielna.com</a></p>
</blockquote>
<blockquote>
<p>At Google, one of our favorite mottos is that “Failure is an option.” It’s
widely recognized that if you’re not failing now and then, you’re not being
innovative enough or taking enough risks. Failure is viewed as a golden
opportunity to learn and improve for the next go-around. 7 In fact, Thomas
Edison is often quoted as saying, “If I find 10,000 ways something won’t work,
I haven’t failed. I am not discouraged, because every wrong attempt discarded
is another step forward.” - SWE At Google (Book)</p>
</blockquote>
<blockquote>
<p>So one effective thing you can do if you want to think better is to <strong>become
better at probing other people’s thinking</strong>. Ask questions. Simple ones are
better. “Why” is the best. If you ask that three or four times you get to a
place where you’re going to understand more and you’ll be able to tell who
really knows what they are talking about. Shortcuts in thinking are easy, and
this is how you tease them out. Not to make the other person look bad – don’t
do this maliciously – but to avoid mistakes, air assumptions, and discuss
conclusions. 
from <a href="https://observer.com/2015/11/youve-been-thinking-all-wrong-heres-how-to-think-better/">here</a></p>
</blockquote>
<blockquote>
<p>To create the strategic framework, start with the company vision—where is it
you want to go? Then, identify the obstacles standing in the way of getting
there, and experiment around ways of tackling them. Keep doing this until the
vision is reached. A mission explains why the company exists; a vision
explains where the company is going, based on that mission. 
from <a href="https://youexec.com/book-summaries/escaping-the-build-trap#:%7E:text=The%20build%20trap%20causes%20companies,and%20are%20vulnerable%20to%20disruption.">here</a></p>
</blockquote>
<blockquote>
<ul>
<li>Will the customer buy It? — proof value</li>
<li>Will users figure out how to use it? — check usability</li>
<li>Will we be able to build it? — ask about the feasibility</li>
<li>Will our business benefit from it and will we cover the costs? — verify viability
from <a href="https://t-ziegelbecker.medium.com/a-summary-of-inspired-by-marty-cagan-9d94e1eeb4bd">here</a></li>
</ul>
</blockquote>
<blockquote>
<p>It’s easy for imposter syndrome to kick in at this point. One technique for
fighting the feeling that you don’t know what you’re doing is to simply
pretend that some expert out there knows exactly what to do, and that they’re
simply on vacation and you’re temporarily subbing in for them. It’s a great
way to remove the personal stakes and give yourself permission to fail and
learn. -Google's SWE book</p>
</blockquote>
<blockquote>
<p>“All models are wrong, but some are useful” is a famous quote often attributed
to the British statistician George E. P. Box.</p>
</blockquote>
<blockquote>
<p>It is possible to commit no mistakes and still lose. That is not a weakness. That is life. -Jean-Luc Picard</p>
</blockquote>
<blockquote>
<p>A concept is a structure that is invented to give a coherent account of the
immediate consequences of actions in a complex system. Thus concepts are
rarely ends in themselves, but are means to other ends.  - Daniel Jackson's paper on concepts?</p>
</blockquote>
<blockquote>
<p>A coward dies a thousand times before his death, but the valiant taste of
death but once. It seems to me most strange that men should fear, seeing that
death, a necessary end, will come when it will come. -julius ceaser, shakespeare </p>
</blockquote>
<h3>Notable links</h3>
<p><em>This is currently only a fraction of what is stored in the larger document. It
will take a bit of time to narrow down to what's chaff and what's of value.</em></p>
<ul>
<li><a href="https://ploum.net/the-computer-built-to-last-50-years/">The computer built to last 50 years</a></li>
<li><a href="https://www.ursulakleguin.com/postmodern-anarchism">Postmodern Anarchism in the Novels of Ursula K. Le Guin</a></li>
<li><a href="https://www.dannyguo.com/blog/my-seatbelt-rule-for-judgment/">My Seatbelt Rule for Judgment</a> and <a href="http://fs.blog/chestertons-fence/">Chesterton’s Fence: A Lesson in Second Order Thinking</a> for being wary of prior solutions before dismantling them.</li>
<li><a href="https://arxiv.org/abs/2201.12689">Hyperbolic band theory through Higgs bundles</a>, which has some exquisite illustrations in a maths paper.</li>
<li><a href="https://codewords.recurse.com/issues/three/algebra-and-calculus-of-algebraic-data-types">The algebra (and calculus!) of algebraic data types</a></li>
<li><a href="https://www.youtube.com/watch?v=CpCg-2wyBeQ">Jez Humble - The Secrets of High Performing Organizations</a></li>
<li><a href="http://venge.net/graydon/talks/CompilerTalk-2019.pdf">21 compilers and 3 orders of magnitude in 60 minutes</a></li>
<li><a href="http://cliffle.com/blog/rust-mutexes/">Why Rust mutexes look like they do</a></li>
<li><a href="https://www.inkandswitch.com/crosscut/#design-principles">Crosscut: Drawing Dynamic Models, design principles</a></li>
<li><a href="https://www.media.mit.edu/projects/2d-an-exploration-of-drawing-as-programming-language-featuring-ideas-from-lambda-calculus/overview/">λ-2D: An Exploration of Drawing as Programming Language, Featuring Ideas from Lambda Calculus</a></li>
<li><a href="https://blog.nelhage.com/post/computers-can-be-understood/">Computers can be understood - Made of Bugs</a></li>
<li><a href="https://copyconstruct.medium.com/effective-mental-models-for-code-and-systems-7c55918f1b3e">Effective Mental Models for Code and Systems</a></li>
<li><a href="https://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/">Kitchen Soap  –  On Being A Senior Engineer</a></li>
<li><a href="https://keavy.com/work/where-to-start/">Where to Start - Keavy McMinn</a></li>
<li><a href="https://borretti.me/article/lessons-writing-compiler">Lessons from Writing a Compiler</a></li>
<li><a href="https://github.com/andrejbauer/plzoo">The Programming Languages Zoo</a></li>
<li><a href="http://sled.rs/errors.html">Error Handling in a Correctness-Critical Rust Project</a></li>
<li><a href="https://onlineornot.com/unreasonable-effectiveness-shipping-daily">The unreasonable effectiveness of shipping every day</a></li>
<li><a href="https://sive.rs/relax">Relax for the same result</a></li>
<li><a href="https://evaparish.com/blog/how-i-edit">What I think about when I edit — Eva Parish</a></li>
<li><a href="https://medium.com/@jamesacowling/stepping-stones-not-milestones-e6be0073563f">Stepping Stones not Milestones</a></li>
<li><a href="https://ciechanow.ski/mechanical-watch/">Mechanical Watch</a></li>
<li><a href="https://www.infoq.com/presentations/go-locks/?itm_source=infoq&amp;itm_campaign=user_page&amp;itm_medium=link">Let’s Talk Locks!</a></li>
<li><a href="https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">Parse, don’t validate</a></li>
<li><a href="https://sha256algorithm.com/">A visualisation of the SHA-256 algorithm.</a></li>
<li><a href="http://fs.blog/first-thought-not-best-thought/">Your First Thought Is Rarely Your Best Thought: Lessons on Thinking</a></li>
<li><a href="https://en.wikipedia.org/wiki/All_models_are_wrong">All models are wrong</a></li>
<li><a href="http://sled.rs/simulation">sled simulation guide (jepsen-proof engineering)</a></li>
<li><a href="https://slyflourish.com/throwing_away_secrets.html">Throwing Away Secrets</a></li>
<li><a href="https://cheats.rs/">Rust Language Cheat Sheet</a></li>
<li><a href="https://friendorfoe.com/d/Old%20School%20Primer.pdf">A Quick Primer for Old School Gaming</a></li>
<li><a href="https://shreevatsa.net/post/programming-pearls/">Jon Bentley's &quot;Programming Pearls&quot; columns, and Van Wyk's &quot;Literate Programming&quot; columns in the CACM</a></li>
<li><a href="https://expandingawareness.org/blog/awareness-is-a-user-interface/">Awareness is a User Interface</a></li>
<li><a href="https://brooker.co.za/blog/2022/06/02/formal.html">Formal Methods Only Solve Half My Problems</a></li>
<li><a href="https://pnx.tf/files/x86_opcode_structure_and_instruction_overview.pdf">x86 Opcode Structure and Instruction Overview</a></li>
<li><a href="https://kernel.dk/axboe-kr2022.pdf">What’s new with io_uring</a></li>
<li><a href="https://www.quantamagazine.org/researchers-achieve-absurdly-fast-algorithm-for-network-flow-20220608/">Researchers Achieve ‘Absurdly Fast’ Algorithm for Network Flow</a></li>
<li><a href="https://blog.regehr.org/archives/1653">Explaining Code using ASCII Art</a></li>
<li><a href="https://www.seriouseats.com/how-to-cook-pasta-salt-water-boiling-tips-the-food-lab">A New Way to Cook Pasta?</a></li>
<li><a href="http://mth.io/m/spbanter.mp3">On naming things ...</a></li>
<li><a href="https://philosophy.stackexchange.com/questions/74389/how-do-we-know-certain-things-to-be-obvious-in-general">How do we know certain things to be &quot;obvious&quot;, in general?</a></li>
</ul>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Time management is time sensitivity</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/time-management-is-time-sensitivity.html</link>
      <guid>https://justanotherdot.com/posts/time-management-is-time-sensitivity.html</guid>
      <pubDate>Mon, 16 Jan 2023 18:45:50 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Time management advice tends to devolve into the treating time as an expendable
resource (it is) that must be cost-analysed (it should), but when people look up
&quot;time management tips&quot; I want to propose that they aren't looking for the how or
what of time expenditure, but rather are looking to become better <em>aware</em> of
time. I base this argument on the fact that those working in jobs that
are conducive to &quot;flow&quot; state can lead to time loss. Sure, losing four hours to
solving a problem is good, right? But what if those four hours could have been
compressed to one, or if the pursuit lost value after a particular threshold?</p>
<p><a href="https://embeddedartistry.com/blog/2020/03/02/why-we-estimate/">Philip Johnston</a>
talks about the driver for estimation; we estimate because it <em>aligns</em> team
members by exposing information in our heads in a public forum, allowing us to
discuss risks, possibilities, and so on. But if we are terribly insensitive to
time, our estimations won't be of much help. Of course, we can always <em>improve</em>
our estimations through practice, requiring us to record and reflect on our
estimates, determining the factors that went out and how things shifted from our
original guess. </p>
<p>In the last couple of years I've come in close contact to two ways of developing
better time sensitivity. Albeit far from perfect, it has helped me refine
estimates, improve my ability to focus on things that matter, and avoid a
certain level of strategic myopia that crops us. </p>
<p>The first way is known as time boxing. You determine a task, set a period of
time you suspect it will take, or that you are willing to allow, and commit to
abandoning the task within that period. Doing this <em>properly</em> means learning to
include the ramp down in the time frame, otherwise one is left with work strewn
everywhere. This is helpful in revisiting purpose or change. The intent isn't to
merely abandon tasks where the outcome may no longer be relevant or as high a
priority as initially determined, but moreso to get a higher standing and
evaluate lessons learned. At a microscopic scale, programming can be broken into
30-60secs feedback loops which, after the box edge is hit, involves reverting
back to a prior state, only to restart again, often with new knowledge that can
fuel simpler approaches. It is said that Joe Armstrong of Erlang fame told many
of this approach at conventions, whereby he would recommend throwing out your
work at the end of the day that wasn't finished. I'm not sure if he intended
only the parts that were incomplete, or if he meant the whole design, and if
someone could clarify that to me I would be grateful. The best I can find is
<a href="https://twitter.com/sadisticsystems/status/1119614274538823687?s=20">various
anectdotes</a>
about Joe from <a href="https://github.com/lukego/blog/issues/32#issue-435504246">around the
internet.</a>. To splice
in a quote from sadisticsystems (spacejam):</p>
<blockquote>
<p>#rememberingjoe Once I had an opportunity to ask Joe a few questions in-person
about workflow and managing complexity. He said he would throw code away that
he couldn't complete in a day of work. This felt wrong to me then, but over
time I've grown to appreciate it</p>
</blockquote>
<blockquote>
<p>It's a razor that forces rewrites of code likely to have been warped to adapt
to incorrect guesses when starting the task. It also forces a limit on the
complexity of the implementation. You also notice and avoid many bugs that
crept in during the failed first pass</p>
</blockquote>
<blockquote>
<p>Since then I've met people who have taken similar ideas to the extreme.
@yoshuawuyts said that when writing http://choo.io he would sometimes do a
full-rewrite in one sitting. The result was craftsmanship and not just a code
dump</p>
</blockquote>
<p>This is excellent advice, as it means you are consistently exploring alternative
paths, which sharpens your divergent thinking and simplification skills. Can you
summarise your problem, as well as curate the research that went into it? What
variations were explored, either before you set to writing code, but as well as
when you started smashing the keys?</p>
<p>The second way is simply being highly conscious of time through simple analogue
means. A practical means of accomplishing this is wearing a non-smart watch. I
have a collection of casio watches I've collected over the years that I've taken
to wearing again, as the context switch and distraction-inducing nature of
phones means I'm more focused on the act of handling my phone than I am of
becoming conscious of how much time has elapsed since I last made note. As I
mentioned before, when lost in a flow state, we can lose awareness of external
stimuli. Although this can be fantastic for compressing work into smaller
spaces, it can also mean flow states where you are ignorant to the fact that
four hours has been lost solving the wrong problem. Part of the process of
becoming &quot;more senior&quot; in software engineering is bigger picture thinking; big
picture thinking involves understanding long-term and knock-on consequences but
it is also about understanding the context and specifics of a problem at hand.
Like the time boxing approach, this is intended to make you aware of how long
things actually take, at a finer resolution than you may be used to, again
prompting reviews of your work. Sure, doing that refactor in an hour made sense
when you thought you could do it, but it's now a little over two hours; do you
keep pushing or do you stop and take stock of where you are at? I'd say the
latter is to be preferred, as the &quot;why&quot; to our outcomes should always trump the
cardinality of our outcomes.</p>
<p>Lastly, there is an experiment I am doing with my calendar, which I suspect
people who have physical planners already benefit from, which is to spend a lot
of time on the yearly and monthly views rather than the weekly or daily. This
shoves my attention into a larger scale, meaning I am more conscious to think
about tasks extending out into weeks or months and the consequences of that
extension, rather than to focus on merely getting through what's immediately in
front of me for this week. This isn't solely identifying problematic upcoming
events, but again is about becoming sensitive to how time drags on, and
revisiting the question of whether or not the value of me or others dedicating X
amount of time on an initiative, or some part of those initiative, makes sense,
or if the flashlight of focus needs shifting.</p>
<p>Ultimately, despite what I've said in the past about <a href="https://www.justanotherdot.com/posts/fools-gold-time-estimates.html">software
estimates</a>,
I think the approximate value of estimation is valuable, but it requires
thinking about estimates on a real time scale, and not a BS scale such as
t-shirt sizes or other intangible metrics that are vague to others, and, worse
yet, yourself! Since writing that article, I've come to learn more estimation
techniques,  but estimation functions within the notion that what we are working
on can matter less or become completely pointless as time advances. The pitting
of efficiency against effectiveness is a classic dichotomy to this point; we can
be highly efficient, focusing moreso on cardinality of outcomes, rather than
effective, where the the outcomes have impact. In the efficient sense, we are
managing time by cutting the cost of evaluation out. But we aren't machines
where we spend extra cycles doing extra work and choosing after the fact. The
output we have available to us is limited, and easily put of course by even the
slightest of factors, hence we need to be wary of how we spend time, and we
don't get that awareness unless we have practice honing our sensitivity to time.
Surely we can be effective, and after that efficient, but focusing squarely on
efficiency misses the point of time management.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Default to trust</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/default-to-trust.html</link>
      <guid>https://justanotherdot.com/posts/default-to-trust.html</guid>
      <pubDate>Thu, 05 Jan 2023 19:51:34 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You may get to the point in your career where you see enough things break and
assume everything is fundamentally flawed. Paranoia sets in, robust
architectures are designed and implemented, and you witness a spectacular
explosion of failure on the faintest breeze of unexpected input. Alternatively,
the paranoia bonds with apathy and you sit on our hands, unsure what to do,
trying to settle on an option like the <a href="https://en.wikipedia.org/wiki/Buridan%27s_ass">donkey deciding between hay or
water</a>, eventually starving from
an inability to decide.</p>
<p>Cut the cycle and default to trust. </p>
<p>You can have correct, safe, and performant code that is delivered in time, and
we get there by growing.  When trust is determined to be misplaced, that is
where the growth can happen. All this requires opening up, as guarded attitudes
will keep you from change. Be receptive to being wrong and to listen. When the
tools, interactions, or knowledge are mended, trust again and repeat. </p>
<p>This isn’t blind optimism. There is no hoping things will get better on their
own, without any intervention. Rather, it is a form of strengthening against
<a href="https://english.stackexchange.com/a/56532">variable perturbations</a>. </p>
<p>Trust your tools such that you gain an arsenal of quality at your fingertips,
forged through consistent applied improvements.</p>
<p>Trust your teams such that you assume peers are acting with clarity and
intent, turning towards the difficult and challenging interactions that
strengthen the collective fabric.</p>
<p>Trust your knowledge such that you make exceptional maps of a variety of
different fidelities, updating swathes as you become surprised at new
knowledge.</p>
<p>Engineering out of fear leads to miserable results, always flexing between the
poles of apathy or stress. You can abandon fear by adopting an attitude of
curiosity, defaulting to trust, and being open to the inevitable surprises that
will come your way. </p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Put your oxygen mask on first</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/oxygen-mask.html</link>
      <guid>https://justanotherdot.com/posts/oxygen-mask.html</guid>
      <pubDate>Fri, 11 Nov 2022 19:53:39 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Before the plane takes off you’re told to attach your own oxygen mask first before helping those around you. Putting the oxygen mask on first feels counterintuitive. Giving is about putting others first, is it not? The vision blurs, our senses numb, and we no longer clearly identify how to help others, let alone ourselves. </p>
<p>You are a unique individual, and a team is a stitching of the unique into the fabric of the whole, not some gooey-grey amalgam of paste that thought leaders purport under the guise of “carefully curated” cultures and communication processes.</p>
<p>Psychological safety can be disrupted externally by others as much as it can be internally by our own complications. Internal issues can be untangled, gently with a bit of patience. Untangling means engaging, despite it feeling paradoxical. After each engagement, take note and question any perceived threats. The more we dismantle these threats, the more confidence we build. Over time you’ll have developed enough confidence that you can spend it on others, too, encouraging similar engagement. </p>
<p>Be clear and vocal about what you want and need. No one can reach into you and pull this out. No one should be expected to fulfill needs and wants you can’t articulate yourself. Give yourself the benefit of the doubt for holding <em>your</em> opinions. Yes, they are not facts, and you may be shown to be in the wrong, but opinions, as of you holding them right now, are true to <em>you</em>. It is ok to be focused on the things that matter and make an impact to <em>you</em>. <strong>Why</strong> is a powerful word, unwavering in its might. Become relentless with asking why and you will become intentional in your actions. With why, we can take apart our own requirements and opinions and keep what’s left standing. Do this often enough and you will have built a feedback mechanism for your own narrative and values. Be a team player, and get your mask on first.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Reading Diet</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-reading-diet.html</link>
      <guid>https://justanotherdot.com/posts/a-reading-diet.html</guid>
      <pubDate>Tue, 19 Apr 2022 07:39:19 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Thorsten Ball had this interesting thought bubble about having a personal
reading coach:</p>
<div class="centered"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Why is picking new books to read so hard?<br><br>What I need is a personal trainer, but instead of writing workout programs they tell me what to read next.<br><br>It’s based on what I read in past and what my goals are.<br><br>They also make sure enough fiction is in there, cause it‘s important.</p>&mdash; Thorsten Ball (@thorstenball) <a href="https://twitter.com/thorstenball/status/1513184857089880069?ref_src=twsrc%5Etfw">April 10, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>
<p>Following your own curiosity is a highly personal activity. The crux of books
such as <a href="https://visakanv.gumroad.com/l/introspect">Introspect</a> gather around
honing one's curiosity, letting it blossom, and nurturing more of it in the
world. A coach <em>could</em> slowly learn one's personal sense of curiosity, but it
will never be to the same depth that you will know yourself.</p>
<p>Coaches are sometimes hired to help create a tailored <em>diet</em>, however. I
consistently drown in content. I think I fare better than most, but I still am
enticed by the idea of someone helping me create a specific plan for my reading
to be able to glean the benefits of reading while having time and sanity for the
other things I want to do.</p>
<p>In lieu of paying someone for this imaginary role, here's an exercise doing it
with myself. What does my current reading regiment look like?</p>
<ul>
<li>mailbrew emails with updates of
<ul>
<li>twitter</li>
<li>news</li>
<li>hackernews</li>
<li>weather forecast</li>
</ul>
</li>
<li>articles on the web (easily 20-70+ multiplied across a few devices)</li>
<li>books, largely technical</li>
</ul>
<p>Major changes I enacted on immediate review was:</p>
<ul>
<li>no more news updates first thing in the morning. I will get this anyways
throughout the day.</li>
<li><em>far more</em> fiction rather than being squarely focused on non-fictional
content.</li>
<li>drastically thinning down the web content.</li>
<li>less twitter, by using a fifteen minute limit as well as a stopping the
mailbrew updates</li>
</ul>
<p>A diet isn't only <em>what</em> you consume, but also adjusting the methodology of
<em>how</em> you consume. Often, with non-fiction books, there's a tendency for people
to treat them as fiction by attempting to read them linearly, but this is a bit
aimless. Being first clear about what you are after is good, but not always
possible if you are interested in a variety of topics inside of the book.</p>
<p>One technique of handling this is to continually make progressive passes on the
book in increasing detail. This is similar to how you can reading abstracts,
introductions, and conclusions with research papers. The aim here is to pick up
the things you are interested, and not waste your time sludging along with other
material you don't care as much about.</p>
<p>Alternatively, you can use your passion as a gauge and if you find yourself
bored, simply skip ahead. There is no shame in it.</p>
<p>If the book is massive, with lots of things you <em>are</em> interested in, a useful
trick is to abandon the book for awhile once you've learned exactly thing, or
perhaps max three things. Whatever limit you impose depends on you, but the aim
is to limit you trudging through the book, and to make actual use of it in a
limited scope of time, rather than letting a day, become a week, become a month,
etc. This is a topic I've been quite absorbed in lately, in that gains should
be brought into smaller and smaller time frames.</p>
<p>Another thing I sometimes do is to use a front, back, and middle bookmark,
forming a sort of semi-binary search of information. The point of all of these
techniques is that they are means of filtering out content that is less useful
for us, and ultimately eliminating or reducing work to find the most useful
content for us. It is rare I find a non-fiction book that I want to read front
to back.</p>
<p>The highest value reading in my diet are books. Books given me a tremendous
amount of value. Books inspire me to write, they give me deep, refined
conversations with others who have taken the time to structure their thoughts
onto page, and they let me dream. This is why a completely non-fiction diet of
books is ineffective: solving problems is not just about discovering knowledge
and patterns to cycle through, but instead exploring problems in unexpected
ways. This means having <em>many</em> non-fiction books, using the techniques above to
work through them, as well as a single fiction book, one at a time. This allows
me to let my mind relax, in the same way that I assume artists explain the
process of doing &quot;automatic drawing&quot;.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>On breathing room</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/on-breathing-room.html</link>
      <guid>https://justanotherdot.com/posts/on-breathing-room.html</guid>
      <pubDate>Sun, 06 Mar 2022 11:03:22 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You need to make changes to produce new states of the system you expose. Code
change velocity is a metric people sometimes use to determine health of the
platform. While a high change rate may seem good, it can also be a sign of a lot
of churn. Change is inevitable, but need not be unsafe. To avoid releasing code
you regret, try these approaches to making changes in the future.</p>
<p>At a macro level, any emergent change to the system consists of three fundamental phases:</p>
<ol>
<li>thinking, specification, communication, problem decomposition and solution synthesis</li>
<li>execution, cycle of design and implementation, discussion around issues where the rubber meets the road, preferably working in a way that avoids cleanup in the next step (“baggage”)</li>
<li>cleanup, tidying, ensuring work doesn't need revisions or bug fixes in the future,  <em>preferably only</em>ceremony around things being done such as communication and demoing</li>
</ol>
<p>The goal of these three phases is to produce work that is itself <a href="https://medium.com/@jamesacowling/stepping-stones-not-milestones-e6be0073563f">a single stepping stone</a>, allowing steady progress. Phases (1) and (3) can be thought of as &quot;bookends&quot;, allowing you to gain traction and wind down ensuring work is both thorough and complete by the time you leave it.</p>
<p>Making changes should support <em>breathing room</em> where you feel comfortable making the switch from the old state of the system to the new one.</p>
<h2>Immutable Changes: The Default You Need</h2>
<p>Not all changes are backwards compatible, but most can be. When you work to push backwards compatibility as a principle of development, you gain the ability to easily move forward with new work without consistently carrying knowledge of the old system along with you. how is that possible when you are adhering to an older form of the system? In an immutable view of change, the new changes don’t have to be immediately, or ever, touching the old system. compatibility is contained naturally, without even holding in the thoughts of “how do i make this work with what is there?”. sometimes the thought must be there, especially when you are making changes mutably (more on this later), in general, with immutable changes we can focus instead on making the boundary compatible, rather than the union of all changes.</p>
<p>A rough sketch of what this looks like is:</p>
<ol>
<li>Make a new target in isolation, lots of testing and verification</li>
<li>Provide means to make changes reversible, such as feature flagging, rapid deployments by decoupling releases as artifacts and deployment as the action.</li>
<li>Make the switch and observe</li>
</ol>
<p>Mental model: you are going from previous changes to new changes and allow <em>decisions to be reversible</em>. Some changes are not going to be reversible and that's ok! But with a framework in place to allow reversal, knowing when something can or cannot be reversed gets easier.</p>
<p>This system promotes flexibility. Want to try out three different rendering engines for performance? Treat the immutable approach as what others might call &quot;growth&quot; or experiment based. Remember: the stable state of the system is when all the flags are off, or, put another way, a flag should never be required on to make the system stable. That way if you lose the flag service, you still have the stable state of the system, hence releasing something eventually means pruning the need for the flag.</p>
<p>You also don't need to lean heavily on flags. The new change could be pushed up “into the shadows” without any rigging, and the independent work of connecting everything can be done as it’s own switched behavior. Isolating surface area in this way is a great alternative to what else is possible, which is that you may want to run one or two adjacent implementations of large or whole parts of the system in parallel. <a href="http://www.sevangelatos.com/john-carmack-on-parallel-implementations/">These could just as easily be forks of the codebase, where one split implementation is gong to win out.</a> The problem with these massive changes is that people tend to prefer small, sectional changes to a codebase, thus the forks would continually need to absorb new changes as they come available, and deal with any shortcomings of the newly introduced code. Hence it is a good idea to focus forks on short-lived experiments, and why I feel immutable changes should be a default, but they may not be preferable given the sheer complexity of making the switch, therefore we sometimes do want to keep mutable changes in place where it is less error prone, either due to drastically reduced code or complexity involved with switching; anytime you see deeply embedded flags or <em>lots</em> of usage for the same flag, you know that the changeover is going to involve a wide surface area that, although may be great to get the benefits of reversibility, may also mean it is harder, and longer, to get into place in general. As a guide, anytime things grow to a particular size, either in time or space, there is a tendency for that growth to continue rather than have a definite stop.</p>
<h2>Mutable Changes: Occasionally Avoiding Increased Complexity</h2>
<p>Occasionally changes are so massive that we don't want to become a blocker for other's making changes on the same system. When that happens it may be preferable to work on a mutable change, but this doesn't mean introducing a massive PR. For every new line introduced in a PR is another line of code to have to worry about causing more trouble in production. Instead, with mutable changes, there is nothing wrong with raising a massive PR, so long as we harvest it for smaller, atomic changes we can introduce incrementally. These incremental changes, along with a good release system, allow us to mitigate failures and contain them to specific places. Five individual changes affecting different areas is more likely to have one change reversed as opposed to a change that affects all five changes simultaneously.</p>
<p>This process is noticeably more involved than the above, but you will see it is mostly a multi-pass process. In general:</p>
<ol>
<li>Raise the massive PR that has all the changes holistically displayed</li>
<li>Identify all the add-only changes in the PR: these can be added in the shadows without having anything rigged up, similar to our immutable approach above.</li>
<li>Make precise changes to prior interfaces and running logic in pieces. Accompany these with 'migration' tests that may or may not be permanent but nonetheless support hardening the platform by baking expected logic into place. When you introduce the change, the tests ought to verify that the old behavior / properties are upheld.</li>
<li>Start working on glue logic rigging up your new code to work. Sometimes steps (2), (3), and (4) have to be paired to ensure the system stays stable, and that's fine. We are aiming to keep changes minimal and atomic.</li>
<li>At this point, your new code is in, the prior parts of the system have shifted over to their new shape(s), and old code can be deleted, but you needn't rush to do this. In the same way adding codes into the shadows lets you decide when you want to make it public, unused code is simply harmless and living in the dank recesses of the shadows now, one day waiting to be swept up. If you are worried others may end up accidentally using it, add warnings, errors, or commentary to the code making it effectively useless, or just delete it on the spot. If you decide to delete on the spot, you may have to do a bit more work to bring it back into existence, rather than simply flipping out the glue change back to the way the old world was.</li>
</ol>
<p>I detailed this process a bit more with the article <a href="https://www.justanotherdot.com/posts/harvesting-pull-requests.html">Harvesting Pull Requests</a>. I likely ought to do the same thing with a deeper dive on the immutable process above, as there are nuances and characteristics to it that intertwine with the problem at hand and what one has available to them to make shipping things safer.</p>
<h2>How do you decide which to pick?</h2>
<p>Whether to enact change immutably or mutably is a matter of deciding what will help you avoid sharp corners. We want breathing room when we make changes. Breathing room is where you don’t release a change with bated breath, crossing your fingers for the next hours, days, weeks, hoping that it won’t simply blow up and be a fiasco for everyone. Breathing room means that <em>when</em> it blows up it will be easy to fix without having to rush at the ready, dropping everything, to perform a hotfix on the system. Hotfixes suck. They may sometimes be required to resolve an incident, but they will not help your mean time to resolution. If you can go back to a good known state in less than a minute with a good deployment system in place, and the changes you’ve imposed are backwards compatible, you will allow you and others on call or responsible to feel a bit less pressure, which in turn gives you more freedom to keep progressing towards your the goals that you <em>want</em> to be dealing with rather than the goals you <em>have</em> to be dealing with.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Ship it</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/ship-it.html</link>
      <guid>https://justanotherdot.com/posts/ship-it.html</guid>
      <pubDate>Thu, 03 Mar 2022 20:13:03 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>shipping things can be done in a way that isn’t tasteless, where you won’t have to go scrambling back to fix things. shipping work can be done in an incremental way, forming <em>progress</em> towards the things you want to achieve.</p>
<p>how do you publish articles rapidly? where is the threshold for “this is done?”. is there even a threshold that must be upheld?</p>
<p>a thought experiment. what is the smallest article we can possibly produce? a sentence? a paragraph? possibly a paragraph. ok.</p>
<p>what is the most unrefined piece we can publish? could we publish it with grammatical  errors and typos? perhaps there is a threshold of how much is acceptable. since we have established a basis for a paragraph being suitable for the smallest article, could we get away with a typo in every sentence? in the whole block? a grammatical error every sentence? in the whole block? likely one typo and one grammatical mistake in the whole block is fine.</p>
<p>writing a single paragraph with enough revisions to narrow down to a single typo or grammatical mistake is often one revision.</p>
<p>onto content: what kind of content should be present in our theoretical paragraph? often the content could be something that rings true for us. or is a record of something we’ve done. but more generally an article that is <em>useful</em> for someone else is far more enticing. here is an idea: removing friction is useful, but removing friction alone won’t increase the desire of performing the action. often we can be present to people’s pains and address those. it can also be our own pains, but those may not be shared.</p>
<p>what is the achievement when it comes to content? is it going to be interesting enough for someone to read the article. is it going to be short (or long) enough to engage them that they can leave feeling intrigued, inspired, or improved by the content.</p>
<p>a good lead makes the difference. some say every sentence should be a lead, but let’s  say, again, that a single lead in every block, preferably (definitively?) the first sentence.</p>
<p>what does the threshold look like so far?</p>
<ul>
<li>some kind of content, it’s preferably addressing a pain, but maybe it is just us musing</li>
<li>there’s a paragraph, with at most one typo, one grammatical mistake, and exactly one lead smack dab at the start of the sentence</li>
</ul>
<p>another thought experiment: what if we want to extend this minimum viable article? we could extend this one paragraph at a time. each with the same qualities. how many minimum viable paragraphs will we call it quits? here’s where divergent and convergent thinking can come to our rescue. first we barf out everything we need in the article. this can be as many paragraphs as we want. then we can refine each paragraph per the above requirements. we could likewise build one paragraph at a time, and then refine it, and continue. there ought to be one final revision that is exclusively focused on pruning deadweight and improving the ordering of points made. any paragraph that doesn’t contribute to the main theme or should be shifted earlier to explain should be cut and reordered ruthlessly.</p>
<p>here’s an exercise: can you reduce every paragraph down to it’s lead or summarising sentence and the article still hold together? the grouping of this should make up a list. how each point or set of points can be grouped forms the structure. that structure forms the table of contents.</p>
<p>and now we have an article, or a book, or a thread and so on. the medium doesn’t matter. there may be some quirks to each medium. but the aim is that the threshold for deployment should be sufficient quality that allows you to produce a daily cadence, while still allowing you to accept a sufficient amount of mess that you can come back to revise articles with sufficient feedback without making that process too internal.</p>
<p>while shipping all of this work is great, what about the people who are tremendously, even dangerously, wrong about a subject? cooling one’s jets on a subject for a day, a week, and so on, can avoid that issue, largely with the aid of a bit of research. but there is another escape hatch: honesty about what you don’t know or where you are uncertain. <a href="https://www.swyx.io/blogpost-annealing">swyx call this “irresponsible learning”</a> and i think that perfectly describes the careless activity of writing content and thrusting it out in to the public forum to expressly drive learnings from others, rather than merely being corrected or guided by input from the larger community.</p>
<p>getting over the threshold of shipping is hard, but establishing clear criterion for acceptance that improves the temp for discovery paired with the willingness to be honest about where you are on unsteady footing with your thoughts reduces the same type of paralysis that people suffer from as they keep expecting more information to form a decision. forming these criteria is a deeply personal thing, depending on where you are at now in your journey and the context of each situation, and not something you can base on others’ opinions, for example the idea that you should make a decision after you have 70% of the information. doing this will help build your skill to thrive in chaos, thriving in chaotic situations will help you practice more of a skill you want to get better at, and hooking up your practice with clear feedback mechanisms means you can rapidly learn, and the more you learn the more you grow.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Harvesting Pull Requests</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/harvesting-pull-requests.html</link>
      <guid>https://justanotherdot.com/posts/harvesting-pull-requests.html</guid>
      <pubDate>Thu, 24 Feb 2022 15:21:52 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Commonly the ideal way to work with code changes is to do them immutably. What i mean by this is that you</p>
<ol>
<li>begin the work as an independent thing</li>
<li>provide a way to switch from the old thing to the new thing (feature flag, fork, and so on)</li>
<li>switch to the thing under particular conditions (sampling users, explicit flag curation, other rules)</li>
</ol>
<p>the point of (2) is that it gives you the ability to make a decision reversible, so long as the code is backwards compatible. backwards compatibility gets a bad rap, but the reality is that by working in a backwards compatible way, you allow yourself to take that step back if you need it. if you’ve also separated deploys and releases in your infrastructure, deploying a release becomes yet another mechanism for you to go back in time. notice the difference here where we are saying we can return to a prior, good known state, rather than needing to deploy new, future ‘hot-fixed’ code. any system that can be arbitrarily restarted or can time-travel at will gives you a great deal of certainty around its stability running in production.</p>
<p>however, not all changes are easy to write in this way. while building the new thing and switching to it is ideal, it is not always feasible or practical. sometimes the new thing is going to have a lot of overlapping behavior that also needs changing. perhaps the way things were initially designed, either intentionally or unintentionally, may cause boundaries to be hazy and implementations to be tangled together. other times things are quite well defined, but the change is menial enough that changing something in place is far faster. being backwards compatible here is still feasible, but the emphasis is on reducing time to delivery to production by simplifying the change in question.</p>
<p>however, doing things in-place tends to come with consequences. there is a common tendency to continually grow out a pull request that seeks to patch trunk by changing behavior or properties of the system as-is; adding, modifying, removing all conjoin in such a way that the holistic pull request works precariously with all the changes involved, but experience shows us bigger pull requests (more lines of code changed) means more risk. whenever i find myself with this type of pull request on my hands, i try to attack it methodically by harvesting it down repeatedly into safer, easier to reason about patches.</p>
<p>firstly, identify all the changes that are absolutely safe to add. this means finding all the additions that can sit in the shadows. tests can be initially marked as skipped, modules can live without being used (potentially incurring warnings, but this is only a temporary measure). this change does not impact the stable state of the system in any way, and is safe to put in and think alone.</p>
<p>next, identify all the changes for removals. remove as much as possible that is authentically safe to remove such that the stable state of the current system remains fine. these removals then can declutter the mental space of suspicion on review of the upcoming pull request(s) that involve modifying the existing code.</p>
<p>any type of refactoring or moving things around should probably be done independently now before the final step. factoring code will mean the ultimate modifications we perform can likely be easier to test and measured, but it also, again, means we can reason about factoring and modifications to the existing behavior as separate issues. factoring code means changing the way it is organized and structured without impacting the current behavior of the system.</p>
<p>and now, the part that is left; with all the remaining parts above done, the final pull request(s) are the ones transitioning the system from the old state it was in into the new state. one can think of this as the emergent state of the system where all the states of our system are but immutable nodes in a big state machine. once more, if the change is non-breaking, then we are safe to rollback if we want to, hence it makes some sense for us to make this a solitary change to avoid having to roll back many patches at once, or have others changes intermixed with our deployment. that said, it may make sense to break the changes up into smaller changes that incrementally transition the system to the newer system, but beware; this approach is fragile. it is not one that is going to give you easy understanding of where things went wrong unless you have a system instrumented for observability and can easily correlate a specific deploy and the outcomes, or if you use a canary with minimal traffic to ascertain problems early. problems aren’t always provoked early on, either. it can take time for a bug to manifest, and it makes sense to consider the clump of changes as still a group that should be considered related, even if the intent is to have finer grain understanding of what went wrong and only roll back the problem behavior (while the rest may be fine). in my opinion, if the goal is to avoid rework, it is better to think of distinct pieces that makes sense as a unit, rather than a bunch of broken up pieces that one must stitch back together to understand their interconnection.</p>
<p>but isn’t breaking up what we did with those initial steps, adding, removing, and factoring before we did our final “risky” change? yes, we did break it up, but we were methodical in sieving out all of the authentically unrelated changes to the primary work. in the end we were left with a patch in our hands that we could demonstrate to others is doing the exact change from the old state of the system to the new state, whereas all of the unrelated changes may be related superficially, they also make review of the bigger pull request harder as they may (quite easily) mask bugs.</p>
<p>or, as the classic addage goes. 10 lines, 10 bugs. 1000 lines, lgtm.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Review 2021</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/reading-review-2021.html</link>
      <guid>https://justanotherdot.com/posts/reading-review-2021.html</guid>
      <pubDate>Sun, 12 Dec 2021 20:21:20 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Instead of focusing on the entirety or majority of books I’ve read this year, I
wanted to focus, instead, on the books that made a mark on me in some way or
another, and the key takeaways for each book.</p>
<p>Summaries for books vary drastically. Good writing is an interplay between
compression of information on the part of the author as well as compression of
information on the part of the reader. To put it another way, a good author
engages with an audience, and that engagement requires a magical sense of
economy with words.</p>
<p>All the takeaways here are from memory. I’ve explicitly not gone back to dig up
quotes or reread anything. I think the impressionism of this approach may be
more useful to you, the reader of this article, as it shows my reflections and
digestions of the work, rather than giving you a digestion of the book in its
raw, unaltered form.</p>
<h2>Write Useful Books</h2>
<p>This was likely the best book I read all year. It had the most tangible
information for writing I’ve yet to encounter. It practices what it preaches,
and the bulk of useful information is at the front, while the rest of the book
fills out information without being too lengthy. Rob’s other books, such as The
Mom Test, are equally as effective at delivering value for the topics they
address.</p>
<h2>A Monk’s to a Clean House and Clean Mind</h2>
<p>How you treat objects translates to how you treat others. There is a great deal
of practice, habit, and care in each of the common tasks in this book. Little
notes such as opening windows or clean your face when you first get up drive the
essence of cleanliness which helps, per the author’s sentiment, hone the mind in
a way similar to meditation.</p>
<h2>Goodbye, Things</h2>
<p>Living minimally doesn’t have to be reductionist for the sake of being
reductionist. Given there is an upper threshold to the things we can reasonably
care about, a la Marie Kondo’s “spark joy” enthusiasm, we can offload the
storage of things onto other places, such as accepting that stores will have the
things we need when we need them or search engines the results rather than us
having to hoard bookmarks, as well as stripping back to the things that matter
to us in the most direct forms as possible, such as giving away whole lots of
possessions or throwing things away without worrying about the lost value.</p>
<h2>Nation</h2>
<p>A young indigenous boy is left out of a freak accident that causes his whole
island to be killed off. A shipwrecked girl winds up on the same island. The two
meet and eventually work together to rebuild the island as other stranded
individuals seek refuge. Classic Pratchett, but with the silliness knob turned
down drastically.</p>
<h2>Tao Te Ching, Le Guin translation</h2>
<p>I’ve lost count of how many translations of the tao te ching I’ve read over the
year. Some have been quite dry, unintelligible, others a bit <em>too</em> imaginative.
This version has interesting twists and turns around power dynamics with a
modern light. I’ll break my promise about finding quotes and will share my
favorite section, although favorites always ebb and flow on subsequent readings.</p>
<blockquote>
<p>Raw silk and uncut wood</p>
</blockquote>
<blockquote>
<p>Stop being holy, forget being prudent,
it’ll be a hundred times better for everyone.
Stop being altruistic, forget being righteous,
people will remember what family feeling is.
Stop planning, forget making a profit,
there won’t be any robbers.</p>
</blockquote>
<blockquote>
<p>But even these three rules
needn’t be followed; what works reliably
is to know the raw silk,
hold the uncut wood.
Need little,
want less.
Forget the rules.
Be untroubled.</p>
</blockquote>
<h2>A Wizard of Earthsea (first four books)</h2>
<p>Another from Ursula K. Le Guin, the story of a sorcerer named Ged. It is a fun
read in light of Rust’s focus on ownership around names and the emphasis of
names being the means of power over the world; this isn’t a new thought in
fantasy fiction, as Gandalf mentions how “the I belong to the name” in the Lord
of The Rings and others, as well as the Structure and Interpretation of Computer
Programs talking about the notion that names allow a sorcerer/wizard power of
the spirits.</p>
<p>An interesting note from Le Guin I read this year had to do with her regretting
that Ged was chosen to be a man. There is a surprising lack of female
characters. Nonetheless, Ged’s exploration into the rift of the world where
names may not apply is an interesting existential investigation on the manner of
how we struggle with meaning and uncertainty, but not something that means all
hope is lost to the chaos lurking underneath. As Pema Chödrön would say,
“welcome chaos with a cup of tea.”</p>
<h2>Klara and the Sun</h2>
<p>An artificial friend is chosen by a sickly child to accompany her at home in a
world where children are regularly genetically enhanced and attend school
remotely. Klara is convinced that the sun is a living entity that nourishes her,
and views pollutants as “his” (the sun’s) enemy. Despite being a material good
purchased for “socialisation” in the place of a child’s life, there is no
denying that Klara has had her own experiences, perceptions, and a reflections
after she is finally scrapped in a yard with other artificial friends.</p>
<h2>Thinking Physics</h2>
<p>It doesn’t teach you physics, but it can teach you aspects and characteristics
of physics through a question and answer styled puzzle book. The book poses a
problem, has you think about it and come up with an answer (multiple choice),
and then read the solution. It builds up an understanding over time that helps
drive other solutions, but it does lack a reflective, conversational tone that I
find better helps instill the ideas. That said, I do think it is a style of
important deliberate practice that is often missed in many other books, or
relegated to oft ignored “exercise” sections.</p>
<h2>The Practice</h2>
<p>I can see how people hate this book. At it’s core it is an assemblage of various
commentary around consistent practice of <em>anything</em> you are involved in doing.
The points about receding into the shadows as hiding, which is unfair to others,
and how a fear of sharing comes from lack of identification with an abundance
mindset resonated particularly well. I find books like this good because they
are fractal in that you can open the book wherever you please and find something
useful.</p>
<h2>4000 Weeks and The Practice Of Not Thinking</h2>
<p>I mention both of these books in tandem because some of thoughts overlap.</p>
<p>In 4000 weeks, we are confronted with a discussion around the point that we will
inevitably need to make hard, uncomfortable choices around what we really want
to do; that in the end, we will not be able to do everything we set out to do,
no matter what productivity hacks we employ. This ties into something I’ve been
thinking about as of late, which is that we tend to de-emphasize the importance
of estimations and think less and less about targets or deadlines for ourselves
with the assumption that we will simply live forever, even though we may
consciously be quite well aware of the counter being true. In this sense, we
turn some things into games. Wondering how fast you can summarise an article or
a book? How quickly can you finish one piece of the puzzle of your work or
hobby?</p>
<p>I want to give a head tilt to Seth Godin’s The Dip here, where the notion of
knowing when to quit or not is the main treatise of the book. It is generally
applied to whole sets of skills, but I think this same sense can be equally
applied to anything we do; is it really worth me summarising this article in a
short period of time? Is this project really <em>the</em> thing I want to be doing
right now?</p>
<p>In The Practice of Not Thinking, the author Ryonuske Koike explores the notions
of meditation around the fundamental premise that, practically, being attached
to our thoughts is an issue all of itself. If we can constrain our focus to
singular points rather than pretending we can multitask, then we can feel more
connected with the world and ourselves. If we can reduce work in progress, we
not only are making the hard choices in face of the facts, but we are also
getting better at focusing itself.</p>
<p>The big thing here for others is probably handling information. The crux of this
is that we tend to drown ourselves with information without considering how we
can better cut through it. This also means being ruthless in how you approach
any sort of content, in any sort of medium. Calling a book done doesn’t have to
be because you read every imaginable word. It can be “done” for you because
you’ve cleared through the chapters or subsections that matter most to you. In
the same sense, work can be “done” given what you’ve built up so far in an
iterative or incremental way. In short, I like a lot of this thinking because I
really like the idea of incremental computation, and I think that the notion of
incremental computation doesn’t stop at just how we design systems or programs.</p>
<h2>The Dawn of Everything</h2>
<p>I haven’t quite finished this one yet, but the premise of the book is a great
one; indigenous people have been marginalised unnecessarily despite sparking the
enlightment period itself with their insights, imagination, and ability to
consider complex social structures where individuals can act as autonomous
authorities in the context of a larger collective. This last point is
interesting to me; I and others I’ve seen often act in a constrained way,
fearing the repercussions of the larger group, and while this is valid in
itself, it also means individuals don’t act as boldly or as outwardly as they
<em>could</em> while still being OK according to social norms. I think part of the
restraint may be because social norms <em>aren’t</em> as loose, hence a new wave of
“don’t give a fuck” attitudes floating around. To quote an amazing line from
Arvid Kahl’s The Embedded Entrepreneur, “embrace fear, listen to it, invite it
in, but ultimately, ignore it.” To paraphrase, while it’s good to contemplate
your fears, they should not be the thing that guides who you are as a person
through being the core of your actions.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Pull Requests Also Go Stale</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/pull-requests-also-go-stale.html</link>
      <guid>https://justanotherdot.com/posts/pull-requests-also-go-stale.html</guid>
      <pubDate>Wed, 24 Nov 2021 21:27:04 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Pull requests aren’t precious things. They propose changes, and there is often an unfair assumption in a company setting that they will always make their way to trunk. Pull requests, just like issues, go stale. Not just in respect to the being up to date with the latest changes that may have surfaced since the pull request has been raised, but also in terms of general age.</p>
<p>Why do pull requests fester? We’ve all done it; if I fix this one more bug, add this one more test, address this one more wave of changes, and whatever else might be necessary, I’ll have a change in place that I can finally merge. The sweet satisfaction of merging your changes on top of trunk and bliss herever after.</p>
<p>In reality, there should be a weekly non-zero rate of closing pull requests. Code will be complex or not related to the model used in helping conceptualize the change. It may have tradeoffs and bugs that are tricky to resolve. It may be an experiment or a proof of concept unfit for continued production use. All of these cases, and more, produce learnings. The sludgefest starts as soon as someone wants to get to done because it will end the discomfort of the work continuing, but struggling isn’t the answer; starting fresh is.</p>
<p>Here’s a helpful tip I’ve learned in the last year. It is a variation of Kent Beck’s “test AND commit OR revert”. In Kent’s system, every time you make some changes, you run the test suite and see if it goes green, and if it does, you immediately, automatically, commit the changes. This forms a growing wave of passing changes, moving the baseline every time something proves itself it works. We can de-automate this process and take a slightly more general approach. Every time we have a broken test suite or even broken compilation, say, we don’t thrash any longer than 30-60s. Sometimes we need to simply cut wood and carry water to get to the end of it, which may be longer than a minute, but if we feel like we are not continually making progress, we should reset our expectations back to the clean slate, except the clean slate is the <em>last</em> clean slate we were at. Making incremental progress in this way is a core tenant to working without churn. You don’t need to blow away everything you’ve done and start clean, although that’s not necessarily an impossibility given the circumstance, but you can establish a line where you’ve completed work that can be truly called done and move forward in a monotonic manner.</p>
<p>Teams can work together to establish this practice. When the author of a change is in the middle of a sludgefest, they often don’t see it themselves. It takes practice to realise that work is sitting about, no longer fresh or relevant or applicable. In this situation, it should be ok for others on the team to be able to make a comment in a blame-free manner that the pull request should probably be closed. This isn’t about the author being bad at code or engineering or whatever; it is simply a friendly reminder to get out of the tunnel and back into the sunlight. This can be a comment, or even provided as a proper review.</p>
<p>A specific approach we felt might work isn’t actually feasible or has unexpected tradeoffs. An implementation isn’t going as intended, and the outcome is contentious or inconsistent with the rest of the team’s understanding of the problem and its paired solution. A seemingly innocuous refactor winds up being extremely difficult to apply and is leading to more code with high cognitive load and bugs. **Finishing work shouldn’t be accomplished for the sake of finishing work. If we accept that pull requests aren’t precious, we accept that we make mistakes, and when we accept mistakes we give ourselves feedback on a variety of contexts. **</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Performance Analysis is Cost Analysis</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/performance-analysis-is-cost-analysis.html</link>
      <guid>https://justanotherdot.com/posts/performance-analysis-is-cost-analysis.html</guid>
      <pubDate>Mon, 22 Nov 2021 20:35:03 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Benchmarks and profiler in hand, you’re ready to start taking stabs to improve
your program’s performance. Looking around, there are some large numbers that
seems obvious to attack, but this is the start of a broken perception about
performance analysis. Bottlenecks are worth evaluating, but they aren’t the only
thing you should be evaluating. The title of this article may seem obvious at
first glance, but the truth is that <strong>performance analysis is not about focusing
on one view on the measurements you’ve collected.</strong></p>
<p><strong>tl;dr</strong> Next time you are profiling, benchmarking, or laying out plans for
what you’re about to build, think about <em>all</em> of the costs. By thinking
<em>holistically</em> you’ll better understanding the value of every expenditure,
meaning you can both build an intuition for the benefit of certain classes of
costs as well as tangibly improving overall program, and non-program,
characteristics. The value of some costs will be obvious, but if the value of a
cost isn’t apparent, it may still be valuable in an indirect manner; usually
this means an upfront cost that leads to improvements later on or a “tradeoff”
that improves a quality that may seem unrelated. For example, amortized growing
of dynamically resizable arrays is done on purpose to help improve overall
performance on the whole, despite the effort put in at each stage of allocation
and copying. Or, we may decide it’s not worth optimising anything in the program
at all because we need to work on a new feature that is much more valuable to
customers than a 5ns improvement to a rarely used endpoint. And when in doubt,
if your program is hurting others or damaging the world around it, consider if
the software should even exist rather than helping it accomplish its job faster
(<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjbrLeX5rD0AhUnTmwGHb8uC_4QwqsBegQIChAB&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Ds_xWflagwbo&amp;usg=AOvVaw32CWmP0sDX92TBm5MCbPLz">hat tip to Itamar Turner-Trauring on this
point</a>).</p>
<p>Costs are everywhere; they are in the pooled costs we see pile up as well as the
costs that spread about a program. These latter costs constitute the diffuse
profile. Turning your attention on evaluating all costs means changing your
attitude from being frugal in the local sense to being economic in the global
sense. In other words, you might save a lot on a big item purchase, but you
might equally save as much over the course of a year with smaller, consistent
savings over time. Having both as savings is the real aim of performance
analysis as cost analysis.</p>
<p>In this cost analysis view of the world, payments should come with returns.
Sometimes the return on investment is definite, but other times it is not, in
which case it is deemed either an acceptable or careless risk depending on the
variables at play. We can define the former as direct and the latter as indirect
returns.</p>
<p>If we write code that brings random allocations in the heap into a single
contiguous block of memory, we are paying for an indirect return on investment
such that every attempt to read the memory afterwards is now much faster than it
would have been.</p>
<p>Alternatively, a direct return for a cost paid out might be the actual core
operation we need done; consider the difference of running several instructions
to calculate a population count (number of ones) of an integer or calling a
processor specific <code>POPCNT</code> instruction.</p>
<p>Keep in mind that not all costs are about the characteristics of your program.
There are the costs of a team of engineer’s salaries or the readability of code.
What is wasteful for our system’s context can be considered an acceptable loss
if we consider what it pays for elsewhere.</p>
<p>Projecting what costs are going to be potentially encountered is just as
important as reviewing costs. This can take two forms: either considering the
budget(s) of what we’re about to spend, or projecting what we assume we might
spend. One gives us a threshold for which to judge future expenditures while the
other gives us a a baseline for where we might might know if we are drifting too
far where we thought we would wind up. The distinction is subtle but important.
With a budget you want to stay as close as possible to the line, while with a
projection you are not trying to aim for anything, really, but you know how far
off your estimate was when you wind up with something tangible.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Performance Principles, Visualised</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/performance-principles-visualised.html</link>
      <guid>https://justanotherdot.com/posts/performance-principles-visualised.html</guid>
      <pubDate>Sun, 24 Oct 2021 13:35:01 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Not all of our decisions around performance are going to be driven after the thing we want to optimise has been built. In order to design programs with performance in mind at the start, we need a set of heuristics to guide us to make better choices.</p>
<p>Often performance gurus have a set of principles they mention early on in their seminal texts. These are useful, but usually only given a short segment. I want to expand on these. Over time, through profiling and observing systems, you’ll grow an intuition for what will and won’t make an improvement. Sometimes your intuition will be wrong, but the probability of you being wrong versus right doesn’t mean that you shouldn’t be thinking abstractly about performance at a higher level.</p>
<p>Some of my favorite mathematical texts focus on visual or geometric interpretations of their subjects. I am a big fan of systems thinking, and I have, over time, grown to relate most of the principles I’m about to discuss with a specific, well-known degenerate system known as a “pipeline”.</p>
<p>For the uninitiated, a pipeline is a system representing very clear entrances and exits, with obvious stages that may, themselves, be pipelines if we are to zoom in on them. We compose stages together into a pipeline to simultaneously increase isolation between concerns and specific transformations while also increasing cohesion of the pipeline as a whole. Pipelines are used in a variety of places where both performance and rigor of correctness matter, and while they are not the only solid architectural pattern, they are definitely one of the foundational ones.</p>
<p>Pipelines are a great way to visualise these principles because they allow us to focus on how the principles work both on the local and global levels. The system as a whole is improved by applying a principle, but we can also see how it was the direct result of a local change. Conversely, sometimes we apply a principle generally across all stages, and a local stage or two in particular are the primary benefactors.</p>
<p>There are a handful of meta principles at play across several of these. Some have to do with laziness, others have to do with bounding resources. There is also the view that every stage of a pipeline represents a layer deeper into a system. As data progresses from stage to stage, we can also see that in a different light as data passing from one layer into the next. However, unlike most client-server architectures where data trampolines, the pipeline’s exit is at the final layer.</p>
<h2>The Principles</h2>
<h3>Don’t do it</h3>
<p>When we break down a large transformation into a pipeline, we are actively seeking how to decompose the transformation into steps or stages. Stages let us clearly see what can and can’t be removed. If a step isn’t necessary, or is duplicating work that is done earlier on, we can avoid doing it altogether.</p>
<p>Additionally, we can choose to not support specific functionality in order to simplify and reduce the work that the overall pipeline has to make. If I make an explicit choice to reduce expressiveness, I may be making things less ergonomic for users in one way, but also drastically improving ergonomics via another way, namely latency. Fast incurs usage.</p>
<h3>Reduce, Reuse, Recycle</h3>
<p>This is partly noted in the last principle, but the idea is that every stage should be focused on producing an output, and should not be repeating work that a prior stage has performed. This isn’t quite the same as having multiple passes on some data, because maybe each stage needs to look at the same data over again, but this is more that if we have created a specific output, only to later recreate the same thing, we are creating waste that should be pruned.</p>
<p>Recycling earlier work forms the backbone of larger techniques such as immutability, incremental computation, dynamic programming, buffers, caches, and so on.</p>
<h3>Skip the travel time</h3>
<p>In the same way that keeping tools or information close by is easier than having to drive out to some distant location every time you need them, the same is true for relevant details to the pipeline. This practically means making things cache friendly, or reducing pointer chasing on the way that data is structured in a program.</p>
<h3>Work on reasonable chunks</h3>
<p>Working on small morsels of data is inefficient in the same way that working on the entirety of the data is inefficient; the more that we can work on at a given time is good, but only up to the point that we can manage. After a particular point, working in large multiples may not be friendly to cache line sizes, or it can cause thrashing on the operating systems virtual, paged memory.</p>
<p>Bulk interfaces over unit-level interfaces, loop unrolling, as well as streaming, and by consequence stream fusion, are all great examples of working on ranges or runs of data. We may internally bound to a particular buffer size, but simultaneously elevate our work on several things at the same time. This is distinctly different from scaling techniques such as parallelisation we’ll talk about later as this isn’t saying the units in the multiples will be worked on exactly the same time, but instead is saying that a bigger payload upfront or at each stage can help speed things up tremendously.</p>
<h3>Focus on throughput</h3>
<p>Choosing what to do is a cost. Every decision requires having to expend the actual energy contemplating what is the right thing to do. In programs this means computing conditionals and performing checks, as well as jumping to the correct, new location of code. Most modern processors perform branch prediction, in which the processor tries to make an educated guess at which branch will be taken, preemptively executing the guessed code before the decision is verified. If it turns out the decision is wrong, we have to throw out a lot of work in order to perform the alternative. Hence, whenever we can eliminate the need to decide, we are likely to reduce an unnecessary cost.</p>
<p>Sometimes performing both outcomes may be far cheaper than deciding which to do, and it also allows future invocations of the code to potentially reuse the results. We can also achieve this by writing direct bitwise operations that compute results without needing to introducing any conditional tests or branch logic. It’s important to remember that the moment you start writing bitwise code, you should verify the compiler isn’t already making a similar optimisation or better.</p>
<h3>Bring the cheap work upfront</h3>
<p>Amortization is the act of performing work at infrequent intervals such that the cost of the infrequent activity is outweighed by the alternative cost of doing it frequently. Dynamic arrays such as vector do this by growing two times in size every time they reach capacity. Ideally we don’t ever need to make this decision and we know the exact size upfront, which is even better. Furthermore, we may want to push certain activities in the front the same way that IO bound tasks are favored in modern operating systems to improve user interaction latency by deferring CPU bound tasks. Paying for a scheduler by using async/await in your programs is another way that we can move particular classes of activities upfront given their priority, either through push or pull semantics or via characteristics of the task being invoked.</p>
<p>In the more general sense of a pipeline, every stage that we get through is more costly to bail at. If we can figure out that things are wrong early on, we can avoid doing lots of unnecessary work.</p>
<h2>Parallelisation is a scaling technique</h2>
<p>Pipelines have clear entrances and exits for a reason; they showcase how data transformation is a sequential operation that is composed of several stages which, themselves, can be pipelines and so on ad infinitum.</p>
<p>However, most people get told to write fast programs by parallelising them out of the box. Parallelisation should be seen as a scaling technique, it allows us to oversee how fast we can make the straight line first and where dependencies lay, and then we can trivially parallelise the isolate parts. It should always be safe to work on multiple chunks by scaling a stage horizontally, and in turn it should be easy to work on multiple raw inputs by horizontally scaling the pipelines themselves. As noted, we can also parallelise the throughput oriented cases, which may run in tandem, even though only one will be chosen or used subsequently later on the pipeline. And lastly, we can parallelise the stages themselves, in what is known as “pipeline parallelism”, where each stage does what work it can in isolation until it needs to block on the stage before it for more input to process.</p>
<p>At the instruction level, this is exactly what SIMD et. al. do; Single Instruction Multiple Data allows strips of data to be processed in true parallel at the hardware level as there are no shared dependencies. All we are saying here in this section is that we can take this same thinking to a systems level to identify where we may or may not need to take advantage of immutability for isolation-safe sharing of data, or conversely pay the tax of lock contention for consistency of reads and writes on solitary allocations of shared memory.</p>
<h2>Instrumentation</h2>
<p>The other advantage of designing things in terms of a pipeline, or any system for that matter, is how it allows us to clearly identify where we want to put instrumentation to help guide us after the implementation of the system is complete. This means we can think about how we are going to profile and benchmark our code from the start, and be able to observe, i.e. ask questions of, the system as it continues to run.</p>
<p>Instrumentation is often an afterthought. If we include it as part of implementation stage, then it may be unclear if what we are instrumenting is worthwhile, although we may be unduly smug in the sense that we are collecting some kind of metrics. If we, instead, focus on instrumentation as a thought bubble with respect to the design of the system, we can better see where, and therefore what, instrumentation should be plugged in.</p>
<h2>Conclusion</h2>
<p>Learning about patterns of systems has been wildly more effective a way to view performance tuning heuristics and principles, as well as understanding “algorithms and data structures” through the lens that they are patterns themselves, is wildly more effective than learning specific implementations as it allows you to do what we just did where we can see how specific principles apply to the design in question.</p>
<p>Pipelines are such a common trope, whether it’s the stages of a compiler, a graphics rendering engine, or a query interface for a database, that knowing <em>why</em> they are amenable for solid performance design, and not just optimisation after-the-fact, is worth revisiting over and over again.  In my mind, these foundations form the basis of lots of other performance discussions and changes. Knowing why you want to do something allows you to work with intent, and working with intent in relation to performance means you aren’t taking wild stabs in the dark at what may or may not work. It isn’t to say all these principles can blindly lead to results, but having a mental model for a system of how your code is designed will allow you to think about performance for the model rather than getting bogged down initially with the specifics of code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Write Honest Benchmarks</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/write-honest-benchmarks.html</link>
      <guid>https://justanotherdot.com/posts/write-honest-benchmarks.html</guid>
      <pubDate>Sat, 21 Aug 2021 09:57:06 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Do benchmarks feel really <em>really</em> hard to get right and that you are never sure you are measuring what you want to measure? It’s likely you feel this way because benchmarks are like mini experiments; you run the experiment to see if there are reproducible results given some fixed parameters. Yet despite the relative ease of running experiments and collating data, experiments can be a net negative as they are prone to lie if you don’t heed some straightforward mistakes.</p>
<p>What do we want out of benchmarks?</p>
<ul>
<li>Insight on the impact our changes have on runtime characteristics</li>
<li>To prove to others that the system in question  possesses or lacks runtime characteristics</li>
</ul>
<p>Thankfully, the tradition of experimentation and statistics is a long one, full of lots of lessons learned. Many of the tips I’m about to go into I’ve learned the hard way, but I can say that once you have these in hand, writing reliable, honest benchmarks should feel painless.</p>
<h2>tl;dr</h2>
<p>Much of these things can be tackled by choice of benchmark harness. Other of these are important to keep in mind as you write benchmarks.</p>
<ul>
<li>Choose a benchmark harness that runs your code across many iterations, performs trial runs, and tells you a good range of statistical information to help you confirm if you can trust the result</li>
<li>Randomize the order of benchmark runs, as well as varying the workloads you feed them, to root out pathological cases</li>
<li>Treat your benchmarks as an extension of a profiler, instrumenting many facets of the system as a whole to gain insights on the relative sizes each part plays in the bigger picture</li>
<li>Pin down as many details as possible and mention them in documentation paired with the benchmarks</li>
<li>Run benchmarks on as quiet a machine as possible, unless the intent is to draw conclusions about how the system will run on a fully loaded host, in which case be clear about this distinction.</li>
<li>Verify the code under measurement is what you expect it to be by measuring overheads or inspecting generated assembly. Although you can guard against. compiler optimisations by using <code>black_box</code>, use it sparingly as it is unlikely that unoptimised code is what you want to be measuring!</li>
</ul>
<h2>Variance and you</h2>
<p>The aim of the game in benchmarking is to derive a single figure of merit. This is usually an arithmetic mean. Why aggregate a mean when we can run the program once and get a number we can use immediately? Unfortunately, if we did that, we’d be potentially hiding away all sorts of useful information. Consider a system that half the time observed performs some task in 20 milliseconds and then the other half of the time observed performs the task for 20 seconds. If we were to accept the first number we saw, we’d draw a drastically different conclusion depending on which number we saw!</p>
<p>By taking the average, we understand a better sense of <em>center</em>, but we can’t just accept an average by itself, either! If we did, we’d ignore how widely spaced our observations are. The greater degree of variance across the numbers, the less we can trust this sense of center. Thus, when we run benchmarks we generally</p>
<ul>
<li>Run many iterations of the experiment</li>
<li>Average the results, and</li>
<li>Report the range of the results and their standard deviation</li>
</ul>
<p>This gives you a decent amount of information to tell you if stray statistical outliers are tugging an average in a certain direction or if the results are all over the place. If timings are all over the place, the only reasonable conclusion we can draw from the result is that the system under test does not display dependable characteristics.</p>
<h2>Variety is the spice of life</h2>
<p>Hardware is stateful; data stored in memory, caches, buffers, and so on, from prior runs can have an impact on making a benchmark look better or worse than it might have if it was run in a different order or if it didn’t have warmup runs. If you run warmups, ensure their results are included in the mix or they may show up as unexpected variance in the results.</p>
<p>It’s additionally important to treat your benchmarks as an extension of a profiler. While performance isn’t necessarily additive, it is imperative to observe the different parts of the system to understand the relative sizes of the role of each part in the sum. It is tempting to look at a single part of a program and label the whole thing with “bad” or “good”, but understanding if a single bottleneck is skewing results helps you and others better understand the strengths and weaknesses of the system as a whole.</p>
<p>Lastly, it’s important to consider varying benchmark inputs or “workloads”. The purpose of this is to understand corner cases. Regular strides or powers of two may seem fine, but if you don’t explore the crevices between these inputs you’ll likely leave pathological cases lurking behind what seem like perfectly fine benchmark results.</p>
<h2>Be precise and be public</h2>
<p>A common mistake with benchmarks is assuming two environments are comparable. Hence a developer may run benchmarks on their laptop and try to infer the runtime characteristics of a production machine from the local results, or vice versa!</p>
<p>The answer to this is to be public with your results. Documenting everything will force you to get feedback early about their legitimacy. A great way to drive this publicity is by documenting everything you do. In part of documenting everything, I find it helps me actually pin down specific details of the benchmarking: what configurations am I opting into? What exact hardware am I running things on? If it is variable, it’s likely to change when others are to run my benchmarks.</p>
<p>Speaking of comparing machines, it’s also to consider a machine that is quiet versus a machine that is in full effect running other programs is not the same environment: always prefer to run your benchmarks on as quiet a machine as possible.</p>
<h2>Compilers are aggressive optimisers</h2>
<p>With benchmarks, you usually will be running code that has had some or all optimisations turned on. However, compilers will do all sorts of things to make code fast, which can involve replacing or removing whole chunks of code, for example.</p>
<p>If you want to verify that the code you are benchmarking is exactly the code you expect it to be, you can always check the assembly in either godbolt or generated locally. The one benefit of checking the assembly locally is that you may have, for example, the <code>target-cpu</code> flag turned on with an architecture that isn’t supported by godbolt, thus you’re assembly output will best match what was chosen for your target environment.</p>
<p>If you want the compiler to skip optimisations in rust within a benchmark, you can always use the <code>black_box</code> hint, which will issue a volatile memory access crippling optimisation attempts in the resulting code block.</p>
<p>“But I <em>want</em> the compiler to optimise this chunk of code!” you say? And you’re right! The compiler <em>should</em> be allowed to optimise away at will! Using <code>black_box</code> is a blunt tool used in specific places rather than huge chunks.</p>
<p>A traditional trick is to ensure the code you are testing is in a dedicated function, and that the benchmark is calling that function, and that the compiler isn’t aggressively replacing <em>that</em> function call. Then, you can write a benchmark to record the overhead of a function call separately, and use that as a baseline against the specific code you are expecting to measure.</p>
<p>You can do this for other overheads, too. If there is some overhead I want to compare that isn’t necessarily a function call, I will sometimes <code>black_box</code> the thing I want to record the measurement of, say a loop with a particular number of iterations, and put something like an <code>asm!(&quot;nop&quot;)</code> in there to ensure that no actual activity is taking place, but that the compiler won’t look at this code, rightfully determine it does nothing, and delete it.</p>
<h2>Conclusion</h2>
<p>Hopefully each of these has shown you a way to improve your benchmarks or benchmarking harnesses. The aim of the game is to have dependable, reproducible ways that provide insight into our systems, forming a foundation for driving improvements over time rather than making lofty guesses about the overall performance of a program.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Profiling with perf and DHAT on Rust code in Linux</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/profiling-with-perf-and-dhat-on-rust-code-in-linux.html</link>
      <guid>https://justanotherdot.com/posts/profiling-with-perf-and-dhat-on-rust-code-in-linux.html</guid>
      <pubDate>Mon, 12 Jul 2021 10:18:23 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You've got a slow Rust program you're sure you can improve, but you have no idea
where to start. If you begin changing things and rerunning the program to test
with something like <code>time</code>, it will take you ages. Isn't there anything better?</p>
<p>Profilers and hardware architectures are complex beasts. It can take awhile to
understand how everything connects together and what metrics a profiling tool is
reporting are worth investigating. It doesn't help that programmers tend to act
like profiling is a dark art, left only for the elite.</p>
<p>At its core, profilers inform you about how your program utilizes hardware,
usually paired with some information about <em>where</em> in the program this specific
utilization is occurring. Is the service slow when it talks to the database or
when talking over the network to the client? Is disk access a slog or are
needless cycles being wasted doing duplicated work? We <em>want</em> our programs to
utilize hardware, but we want to do it in a way that is taking full advantage of
of the potential of the hardware in question.</p>
<p><code>perf</code> works by using Performance Monitoring Counters or PMCs (also known as
HMCs for Hardware Performance Counters). These counters track various metrics in
hardware rather than in software, which can carry its own performance penalty.
<code>perf</code> is generally a CPU oriented profiler, but it can track some non-CPU
related metrics. Try giving <code>perf list</code> a try in your terminal and have a look
at what's available your target machine. All the events listed can be explicitly
tracked by passing <code>-e NAME</code> to the bulk of the <code>perf</code> subcommands. In this case
<code>NAME</code> means any of the names listed in <code>perf list</code>, but it can also be exact
counter names if you are working directly with a specific CPU architecture, but
don't worry about this for now.</p>
<p><strong>Here's what we'll cover in this post:</strong></p>
<ul>
<li>Why we want instrumentation to gain clarity on where to spend our efforts</li>
<li>Problem description ( you are here )</li>
<li>Establishing benchmarks to drive our understanding and comparison of improvements</li>
<li>A quick intro to <code>perf</code>, and where it shines as a profiling tool</li>
<li>Checking memory usage via <code>dhat</code> without having to use <code>valgrind</code></li>
</ul>
<p>If you are the eager sort, <a href="https://github.com/justanotherdot/perf-and-dhat-profiling-example">have a look at the code up
front</a>. The
commits are broken up to allow easily following the below tutorial.</p>
<h3>Problem Description</h3>
<p>Let's begin with a problem; we have a program that consumes a CSV, and processes
every row to aggregate some result. It's noticeably a bit sluggish, and gets
slower the bigger the input.</p>
<p>In this initial implementation, we deserialize every row into a <code>HashMap</code> of the
headers (<code>String</code>s) to a designated value as a series of bytes (<code>ByteBuf</code>). We
want to later parse the bytes into something we care about. We want raw bytes to
avoid parsing them immediately into unicode, which they may not be!</p>
<p>The headers are <code>String</code>s rather than pre-encoding the struct names as the
headers because we want this code to run on any sort of CSV we hand it,
regardless of the number and name of the headers.</p>
<p>The point of this example is to show that there may be multiple places where
bottlenecks live, and of different types, each of which we'll use various forms
of profiling or assertions to check.</p>
<p>Astute readers might catch the main inefficiency while checking the code, but
the point here is to size up problems with actual data backing the necessary
changes we are looking to take, rather than blindly stabbing in the dark and
seeing if things work or not. Often this blind approach can undo perfectly good
performance gains from other work!</p>
<h3>Setup and prerequisites</h3>
<p>Unfortunately, <code>perf</code> is a linux-only tool. We will cover other profilers in
other articles, and if you are using something like <code>dtrace</code> on a BSD, you can
similarly flume output into something like <code>flamegraph</code>, but this article will
focus on <code>perf</code> exclusively. <code>perf</code> should come equipped with most linux
distros. If it's not, you may have to do some googling on how to get it for your
current machine.</p>
<p>When running <code>perf</code> you'll see some output about changing settings for the
purposes of security. If this is a local machine, you're likely safe to simply
run <code>echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid</code>. This setting is
to ensure that other unprivileged processes can't see what they're not supposed
to see.</p>
<p>As with any optimizing adventure, we need to always ensure we are actually
running comparisons on release builds.</p>
<pre><code>cargo build --release
</code></pre>
<p>Artifacts are generated under <code>target/release/*</code>.</p>
<p>Next, we'll tweak a few common things in our cargo manifest (<code>Cargo.toml</code>):</p>
<pre><code>[profile.release]
incremental = true
debug = true
lto = &quot;fat&quot;
</code></pre>
<p>Sometimes you may want to try <code>codegen-units = 1</code>, but it can aversely affect
compile times and may not yield similar gains in performance. Ditty for <code>lto = &quot;fat&quot;</code>, which you can also try <code>lto = &quot;thin&quot;</code> alternatively (or just leave it
off). As we'll see with our future steps, it pays to verify! You'll notice that
<code>debug = true</code> is also noted; this is going to be necessary when we get to using
<code>perf record</code>.</p>
<p>Lastly, it helps to be running with <code>export RUSTFLAGS=&quot;-C target-cpu=native&quot;</code> to
ensure that the compiler is choosing instructions that are more appropriate for
the actual CPU you are using. <code>native</code> isn't a guarantee you will get the exact
machine you are on, and if you do know the specific architecture you are on it's
better to put that in there, instead. You can list supported names with <code>rustc --print target-cpus</code>.</p>
<h3>Checking wall times</h3>
<p><code>time</code> is a trusty tool, but <code>hyperfine</code> takes the idea of the tool and augments
it with the statistical finesses and reporting of the <code>criterion</code> benchmarking.
Let's install it if you don't have it:</p>
<pre><code>cargo install --force hyperfine
</code></pre>
<p>It's worth showing the difference in performance between <code>--release</code> and debug
if you've never done this. Here's a useful experiment, and a template for how to
use the tool <code>hyperfine</code> to compare multiple targets of measurement at the same
time:</p>
<p><code>hyperfine &quot;target/debug/perf-and-dhat-profiling-example test.csv&quot; &quot;target/release/perf-and-dhat-profiling-example test.csv&quot;</code></p>
<pre><code>Benchmark #1: target/debug/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):     443.2 ms ±  10.8 ms    [User: 437.2 ms, System: 1.6 ms]
  Range (min … max):   431.4 ms … 467.4 ms    10 runs

Benchmark #2: target/release/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):      21.2 ms ±   1.2 ms    [User: 19.9 ms, System: 1.0 ms]
  Range (min … max):    20.1 ms …  31.0 ms    139 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet PC without any interferences from other programs. It might help to use the '--warmup' or '--prepare' op
tions.

Summary
  'target/release/perf-and-dhat-profiling-example test.csv' ran
   20.87 ± 1.30 times faster than 'target/debug/perf-and-dhat-profiling-example test.csv'
</code></pre>
<p>To describe the above output we see:</p>
<ul>
<li>the first benchmark's report</li>
<li>the second benchmark's report</li>
<li>the summary of which was fastest</li>
</ul>
<p>In this case running in release mode led to a 20.87 (give or take 1.30) times
improvement over non-release (debug).</p>
<h3>Describing key metrics with <code>perf stat</code></h3>
<p><code>perf stat</code> gives you a summary of common hardware statistics. You almost always
want to run it with <code>-d</code> and you can add more <code>d</code>s to increase the number of
useful metrics it provides, but I find one is sufficient.</p>
<p>Like any good profiling or benchmarking tool, we are unlikely to observe the
same sample every time, thus variations could be due to any number of factors,
and we can generally describe these as noise. Noise in data is normal, and
unless you are using a profiler like <code>cachegrind</code> which runs your code in a
sandbox and records its own metrics, you re likely to see variations of all
sorts. What we want to know is</p>
<ul>
<li>are the variations extreme or minimal?</li>
<li>what is center across all the variations?</li>
</ul>
<p>Thus it's a good idea to run our program with <code>-r 100</code> which will rerun the
program under <code>perf stat</code> 100 times.</p>
<pre><code>; perf stat -ad -r 100 target/release/perf-and-dhat-profiling-example test.csv
&lt;snip, lots of output from the program itself&gt;

 Performance counter stats for 'system wide' (100 runs):

             84.14 msec cpu-clock                 #    3.838 CPUs utilized            ( +-  0.68%)
               130      context-switches          #    0.002 M/sec                    ( +-  2.67%)
                13      cpu-migrations            #    0.153 K/sec                    ( +-  4.42%)
               301      page-faults               #    0.004 M/sec                    ( +-  5.69%)
        72,827,084      cycles                    #    0.866 GHz                      ( +-  4.81%)  (10.15%)
       139,646,188      instructions              #    1.92  insn per cycle           ( +-  5.22%)  (25.71%)
        36,171,391      branches                  #  429.892 M/sec                    ( +-  4.06%)  (44.76%)
           108,807      branch-misses             #    0.30% of all branches          ( +-  2.81%)  (56.11%)
        48,664,574      L1-dcache-loads           #  578.372 M/sec                    ( +-  1.78%)  (56.60%)
           170,803      L1-dcache-load-misses     #    0.35% of all L1-dcache accesses  ( +-  4.5 1% )  (52.99%)
            67,067      LLC-loads                 #    0.797 M/sec                    ( +-  3.97%)  (36.22%)
            47,521      LLC-load-misses           #   70.86% of all LL-cache accesses  ( +-  4.80 % )  (17.61%)

         0.0219215 +- 0.0000420 seconds time elapsed  ( +-  0.19% )
</code></pre>
<p><strong>A handy tip:</strong> <em><code>perf</code> subcommands often take a <code>pre</code> and <code>post</code> argument, which
you can use to plug in setup and teardown actions, such as <code>--pre=cargo build --release</code>, which is particularly handy to ensure you are getting results on the
latest results.</em></p>
<p><code>perf stat</code> will report output with bolded items of note. In my code above you
won't notice this given how I format things but on your terminal you might. The
output can be deciphered as such, per column, in order, we have:</p>
<ol>
<li>The arithmetic mean of the counted values</li>
<li>The unit, if known</li>
<li>The event name</li>
<li>Summary of event in human terms (sometimes referred to as a shadow metric)</li>
<li>Deviation across samples</li>
</ol>
<p><em>The sixth column is a bit of a mystery to me. I think it has to do with scaling
the metrics somehow, but if you happen to know definitely, please get in touch!</em></p>
<p>It definitely helps to know a little bit about hardware architecture to get a
sense of where problems can arise. You can <a href="https://www.justanotherdot.com/posts/what-makes-up-a-cpu.html">read my longer
summary</a> of a
basic architecture, but what we care about here, in essence, is about
utilization. For example we want to make sure,</p>
<ul>
<li>Is the CPU being utilized as effectively as it can be?
<ul>
<li><code>insn per cycle</code> in the above output next to <code>instructions</code> designates
roughly how many &quot;instructions per cycle&quot; are being executed on average.
If a CPU can run, say, four instructions at the same time in its pipeline,
then we'd hopefully see a value of around 4, but anything from one and
above is good, too.</li>
</ul>
</li>
<li>Are we switching between threads or are threads switching cores often?
<ul>
<li><code>context-switches</code> and <code>cpu-migrations</code> respectively show how often
how commonly threads are being scheduled or run on cores other than the
ones they were originally running on.</li>
</ul>
</li>
<li>Are the CPUs caches being maximized or we having to go to main memory a lot?
<ul>
<li><a href="https://www.justanotherdot.com/posts/making-friends-with-caches.html">Caches</a>
are about reuse and speculation. If you want to get water from a well, it
would take you less time on average if you could bring a lot of water
closer to home. If we are frequently accessing data, it's better if we can
architect a program that doesn't have to communicate excessively with main
memory, although this may not always be possible, depending on context, of
course. All the <code>L1-*</code> and <code>LLC-*</code> metrics describe this, but there is a
difference between the <code>dcache</code> and <code>icache</code> which stand for <code>data cache</code>
and <code>instruction cache</code> respectively. In the same way we can utilize a
cache for data, we can do the same for instructions so we don't have to
decode them repeatedly.</li>
</ul>
</li>
<li>Are we trashing useful work in the CPU?
<ul>
<li>Speculative branch prediction tries to take a best-guess at which path on
a branch can be taken. This allows the CPU to do as much work as possible,
as early as possible, but if the CPU makes the wrong guess, it will have
to give up everything it's doing at that moment and start over.
<code>branch-misses</code> tells us how many of <code>branches</code> have been wrong out of
these best-guesses.</li>
</ul>
</li>
</ul>
<p>We see that <code>L1</code> (first level) cache hits for data (<code>dcache</code>) aren't that great,
and that the last level cache (<code>LLC</code>) is missing a <em>lot</em>. This implies that we
are having to go out to main memory often, which is a good lead on something to
improve! However, this output is failing to tell us <em>where</em> we should be looking
in our program to see this manifest itself. For this we'll use <code>perf record</code> to
associate metrics to symbols in our program.</p>
<h3>Digging deeper with <code>perf record</code> and <code>perf report</code></h3>
<p><code>perf record</code> takes samples of events over a given period of time, and lines
them up with the specific instructions (and, therefore, symbols those
instructions belong to) giving us insight into which particular part of our code
is underutilizing or misusing our hardware,</p>
<pre><code>perf record -e L1-dcache-loads,LLC-load-misses --call-graph dwarf -- target/release/perf-and-dhat-profiling-example test.csv
</code></pre>
<p>The above is going to track the two events <code>L1-dcache-loads</code> and
<code>LLC-load-misses</code> on the command, mentioned after the double hyphen <code>--</code>. It is
going to use a <code>dwarf</code> debug symbol format, which we enabled output of for
release builds above in our setup under our cargo manifest.</p>
<p>When we run <code>perf report</code>, we'll be able to interactively explore hot spots in
our code. We should start with the biggest ones first. By digging into the
report we see that event <code>L1-dcache-loads</code> has 98.42% of the event happening
around lots of code that looks to be <code>deserializing</code> and allocating memory:</p>
<pre><code>Samples: 91  of event 'L1-dcache-loads', Event count (approx.): 44200363
  Children      Self  Command          Shared Object                    Symbol
-   98.42%    41.08%  perf-and-dhat-p  perf-and-dhat-profiling-example  [.] perf_and_dhat_profiling_example::main                                                                                  ◆
   - 57.33% perf_and_dhat_profiling_example::main                                                                                                                                                  ▒
      - perf_and_dhat_profiling_example::go (inlined)                                                                                                                                              ▒
         - 52.78% perf_and_dhat_profiling_example::read_csv (inlined)                                                                                                                              ▒
            - 18.41% &lt;csv::reader::ByteRecordsIter&lt;R&gt; as core::iter::traits::iterator::Iterator&gt;::next (inlined)                                                                                   ▒
               - 11.44% csv::reader::Reader&lt;R&gt;::read_byte_record (inlined)                                                                                                                         ▒
                    csv::reader::Reader&lt;R&gt;::read_byte_record_impl (inlined)                                                                                                                        ▒
                  - csv_core::reader::Reader::read_record                                                                                                                                          ▒
                     - csv_core::reader::Reader::read_record_dfa (inlined)                                                                                                                         ▒
                          csv_core::reader::DfaClasses::scan_and_copy (inlined)                                                                                                                    ▒
               - 6.97% csv::byte_record::ByteRecord::clone_truncated (inlined)                                                                                                                     ▒
                    csv::byte_record::ByteRecord::new (inlined)                                                                                                                                    ▒
                    csv::byte_record::ByteRecord::with_capacity (inlined)                                                                                                                          ▒
                    alloc::boxed::Box&lt;T&gt;::new (inlined)                                                                                                                                            ▒
                    alloc::alloc::exchange_malloc (inlined)                                                                                                                                        ▒
                    &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (inlined)                                                                                                           ▒
                    alloc::alloc::Global::alloc_impl (inlined)                                                                                                                                     ▒
                    alloc::alloc::alloc (inlined)                                                                                                                                                  ▒
                    __rdl_alloc (inlined)                                                                                                                                                          ▒
                    std::sys::unix::alloc::&lt;impl core::alloc::global::GlobalAlloc for std::alloc::System&gt;::alloc (inlined)                                                                         ▒
                  - __GI___libc_malloc (inlined)                                                                                                                                                   ▒
                       2.77% tcache_get (inlined)                                                                                                                                                  ▒
                       1.43% checked_request2size (inlined)                                                                                                                                        ▒
            - 12.87% csv::byte_record::ByteRecord::deserialize (inlined)                                                                                                                           ▒
                 csv::deserializer::deserialize_byte_record (inlined)                                                                                                                              ▒
                 serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize (inlined)                                                            ▒
                 &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::Deserializer&gt;::deserialize_map (inlined)                                                                                   ▒
               - &lt;serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize::MapVisitor&lt;K,V,S&gt; as serde::de::Visitor&gt;::visit_map (inlined)      ▒
                  + 7.33% serde::de::MapAccess::next_entry (inlined)                                                                                                                               ▒
                  + 5.55% std::collections::hash::map::HashMap&lt;K,V,S&gt;::insert (inlined)                                                                                                            ▒
            + 5.31% core::ptr::drop_in_place&lt;std::collections::hash::map::IntoIter&lt;alloc::string::String,serde_bytes::bytebuf::ByteBuf&gt;&gt; (inlined)                                                 ▒
            + 4.99% core::ptr::drop_in_place&lt;csv::byte_record::ByteRecord&gt; (inlined)                                                                                                               ▒
            + 4.26% perf_and_dhat_profiling_example::parse (inlined)                                                                                                                               ▒
            + 4.17% core::ptr::drop_in_place&lt;alloc::string::String&gt; (inlined)                                                                                                                      ▒
            + 1.70% core::ptr::drop_in_place&lt;serde_bytes::bytebuf::ByteBuf&gt; (inlined)                                                                                                              ▒
              1.08% &lt;alloc::vec::Vec&lt;T,A&gt; as core::ops::deref::Deref&gt;::deref                                                                                                                       ▒
         + 4.03% perf_and_dhat_profiling_example::histogram (inlined)                                                                                                                              ▒
         + 0.53% perf_and_dhat_profiling_example::read_file (inlined)                                                                                                                              ▒
Tip: Show individual samples with: perf script                                                                                                                                                     ▒
</code></pre>
<p>Specifically, check out these lines:</p>
<pre><code>&lt;snip&gt;
-   98.42%    41.08%  perf-and-dhat-p  perf-and-dhat-profiling-example  [.] perf_and_dhat_profiling_example::main                                                                                  ◆
&lt;snip&gt;
         - 52.78% perf_and_dhat_profiling_example::read_csv (inlined)                                                                                                                              ▒
&lt;snip&gt;
            - 18.41% &lt;csv::reader::ByteRecordsIter&lt;R&gt; as core::iter::traits::iterator::Iterator&gt;::next (inlined)                                                                                   ▒
&lt;snip&gt;
               - 11.44% csv::reader::Reader&lt;R&gt;::read_byte_record (inlined)                                                                                                                         ▒
&lt;snip&gt;
            - 12.87% csv::byte_record::ByteRecord::deserialize (inlined)                                                                                                                           ▒
&lt;snip&gt;
               - &lt;serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize::MapVisitor&lt;K,V,S&gt; as serde::de::Visitor&gt;::visit_map (inlined)      ▒
&lt;snip&gt;
            + 5.31% core::ptr::drop_in_place&lt;std::collections::hash::map::IntoIter&lt;alloc::string::String,serde_bytes::bytebuf::ByteBuf&gt;&gt; (inlined)                                                 ▒
            + 4.99% core::ptr::drop_in_place&lt;csv::byte_record::ByteRecord&gt; (inlined)                                                                                                               ▒
&lt;snip&gt;
</code></pre>
<p>Here's how I read this:</p>
<ul>
<li><code>read_csv</code> is taking up half the number of these last level cache load misses</li>
<li>Part (18.41%) of that is reading a byte record; this is expected.</li>
<li>another part (12.87%) is deserializing each byte record into a HashMap</li>
<li>a small, but still sizeable, part is dropping the HashMap (5.31%), as well as the
ByteRecord (4.99%)</li>
</ul>
<p>Which leads us to ask; can we get rid of all this deserialization and HashMap
construction work? A principle of performance is to be lazy, and the best way to
be lazy is to avoid doing work you don't need to do! Before we get to tweaking
the code, let's analyze this outside of the terminal with flamegraphs.</p>
<h3>Making it more visual with flamegraphs</h3>
<p><code>inferno</code> is a collection of flamegraph related tooling that Jon Gjenset built
to produce different types of flamegraphs. The original flamegraph generation
was a perl script but I find it easier to install and reuse a compiled tool, so
we'll use <code>inferno</code>.</p>
<pre><code>cargo install --force inferno
</code></pre>
<p>In order to generate a flamegraph, we have to take the <code>perf.data</code> file that is
generated as part of <code>perf record</code> and run it through <code>perf script</code>. Then we'll
take <code>inferno-collapse-perf</code> to turn the stack traces into a &quot;folded&quot; format.</p>
<pre><code>perf script | inferno-collapse-perf &gt; stacks.folded
</code></pre>
<p>Then, we'll spit the folded format into <code>inferno-flamegraph</code> and dump the output
into an SVG.</p>
<pre><code>cat stacks.folded | inferno-flamegraph &gt; profile.svg
</code></pre>
<p>You can load this into a browser to interactively zoom into sections.
flamgegraphs are purely shown in terms of scale of magnitude. The x-axis has no
bearing on chronological ordering, but the relative sizes of elements is
comparable.</p>
<figure>
  <img
    src="/assets/images/perf-flamegraph-screenshot-before.jpg"
    alt="A flamegraph of our unoptimised program"
    title="A flamegraph of our unoptimised program">
  </img>
</figure>
<p>This visually shows us that <code>read_csv</code> is the largest cost center. At a glance,
there is a fair amount of allocation from the byte record iteration that is
getting dropped, possibly needlessly, and that the deserialize code is also
spending a fair bit of time allocating memory and constructing HashMaps, as we
saw with the terminal. If we look at the code this makes sense; we are doing
this for every single record!</p>
<p>What about specific allocations? <code>perf</code> isn't going to be able to tell us
specifics about allocations, and for that we'll turn to Nicholas Nethercoate's
wonderful <code>dhat</code> Rust library, built out of the same tooling for <code>dhat</code> on
valgrind.</p>
<h3>Allocation analysis</h3>
<p>We'll plug in the <code>dhat</code> stuff under a feature flag so we can easily turn it on
later when we want, and leave it out of the way when we don't. I'll be
unimaginative and call this feature flag <code>dhat-on</code>, thus in our cargo manifest,</p>
<pre><code>[features]
default = []
dhat-on = []
</code></pre>
<p>Plus, you'll need to make sure you turn off <code>lto</code> that we had on before with
<code>lto = false</code>. We want to be able to see the stack traces for the allocations,
and optimisations such as <code>lto</code> will do as much inlining across dependencies as
possible.</p>
<p>Here's the patch plugging it in to the codebase,</p>
<pre><code>diff --git a/src/main.rs b/src/main.rs
index bd43846..b140355 100644
 a/src/main.rs
+++ b/src/main.rs
@@ -1,4 +1,6 @@
 use csv::Reader;
+#[cfg(feature = &quot;dhat-on&quot;)]
+use dhat;
 use serde_bytes::ByteBuf;
 use std::collections::HashMap;
 use std::convert::TryInto;
@@ -7,6 +9,12 @@ use std::fs::File;
 use std::io::{BufReader, Read};
 use std::path::PathBuf;

+#[cfg(feature = &quot;dhat-on&quot;)]
+use dhat::{Dhat, DhatAlloc};
+#[cfg(feature = &quot;dhat-on&quot;)]
+#[global_allocator]
+static ALLOCATOR: DhatAlloc = DhatAlloc;
+
 type Record = HashMap&lt;String, ByteBuf&gt;;

 const NULL: &amp;'static str = &quot;NULL&quot;; // or whatever.
@@ -77,6 +85,9 @@ fn go(input: &amp;str) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {
 }

 fn main() {
+    #[cfg(feature = &quot;dhat-on&quot;)]
+    let _dhat = Dhat::start_heap_profiling();
+
     go(&quot;test.csv&quot;).unwrap_or_else(|e| {
         eprintln!(&quot;[csv-count] {}&quot;, e);
         std::process::exit(1);
</code></pre>
<p>Then, just run the program as you would. The resulting analysis will dump out a
<code>dhat-heap.json</code> that we can take and load into a
<a href="https://nnethercote.github.io/dh_view/dh_view.html">viewer</a>. You can also clone
the valgrind project to get access to the same HTML to view the output.</p>
<pre><code>▼ PP 1/1 (7 children) {
    Total:     18,443,933 bytes (100%, 8,095,768.23/s) in 374,530 blocks (100%, 164,395.96/s), avg size 49.25 bytes, avg lifetime 121.47 µs (0.01% of program duration)
    At t-gmax: 2,312,738 bytes (100%) in 26 blocks (100%), avg size 88,951.46 bytes
    At t-end:  1,024 bytes (100%) in 1 blocks (100%), avg size 1,024 bytes
    Allocated at {
      #0: [root]
    }
  }
  ├── PP 1.1/7 {
  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 112,347 blocks (30%, 54,230.42/s), avg size 8.67 bytes, avg lifetime 13.41 µs (0% of program duration)
  │     Max:       26 bytes in 3 blocks, avg size 8.67 bytes
  │     At t-gmax: 26 bytes (0%) in 3 blocks (11.54%), avg size 8.67 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23a8ab5: &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23a8ab5: alloc::raw_vec::RawVec&lt;T,A&gt;::allocate_in (raw_vec.rs:195:45)
  │       #3: 0x556ef23a8ab5: alloc::raw_vec::RawVec&lt;T,A&gt;::with_capacity_in (raw_vec.rs:136:9)
  │       #4: 0x556ef23a8ab5: alloc::vec::Vec&lt;T,A&gt;::with_capacity_in (mod.rs:580:20)
  │       #5: 0x556ef23a8ab5: &lt;T as alloc::slice::hack::ConvertVec&gt;::to_vec (slice.rs:211:25)
  │       #6: 0x556ef23a8ab5: alloc::slice::hack::to_vec (slice.rs:163:9)
  │       #7: 0x556ef23a8ab5: alloc::slice::&lt;impl [T]&gt;::to_vec_in (slice.rs:476:9)
  │       #8: 0x556ef23a8ab5: alloc::slice::&lt;impl [T]&gt;::to_vec (slice.rs:453:9)
  │       #9: 0x556ef23a8ab5: &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::Deserializer&gt;::deserialize_byte_buf::{{closure}} (deserializer.rs:449:50)
  │       #10: 0x556ef23a8ab5: core::result::Result&lt;T,E&gt;::and_then (result.rs:704:22)
  │       #11: 0x556ef23a8ab5: &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::Deserializer&gt;::deserialize_byte_buf (deserializer.rs:448:9)
  │       #12: 0x556ef23a87a7: &lt;serde_bytes::bytebuf::ByteBuf as serde::de::Deserialize&gt;::deserialize (bytebuf.rs:251:9)
  │       #13: 0x556ef23a87a7: &lt;core::marker::PhantomData&lt;T&gt; as serde::de::DeserializeSeed&gt;::deserialize (mod.rs:785:9)
  │       #14: 0x556ef23a87a7: &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::MapAccess&gt;::next_value_seed (deserializer.rs:654:9)
  │       #15: 0x556ef23a87a7: serde::de::MapAccess::next_entry_seed (mod.rs:1812:34)
  │       #16: 0x556ef23a87a7: serde::de::MapAccess::next_entry (mod.rs:1860:9)
  │       #17: 0x556ef23a87a7: &lt;serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize::MapVisitor&lt;K,V,S&gt; as serde::de::Visitor&gt;::visit_map (impls.rs:1284:61)
  │       #18: 0x556ef23a87a7: &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::Deserializer&gt;::deserialize_map (deserializer.rs:520:13)
  │       #19: 0x556ef23a84f6: serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize (impls.rs:1293:17)
  │       #20: 0x556ef23a84f6: csv::deserializer::deserialize_byte_record (deserializer.rs:47:5)
  │       #21: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │     }
  │   }
&lt;snip&gt;
  ├─▼ PP 1.3/7 (2 children) {
  │     Total:     7,939,272 bytes (43.05%, 3,832,323.84/s) in 37,450 blocks (10%, 18,077.29/s), avg size 212 bytes, avg lifetime 22.18 µs (0% of program duration)
  │     At t-gmax: 212 bytes (0.01%) in 1 blocks (3.85%), avg size 212 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23a6e3b: &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23a6e3b: hashbrown::raw::alloc::inner::do_alloc (alloc.rs:11:9)
  │       #3: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&lt;A&gt;::new_uninitialized (mod.rs:1157:38)
  │       #4: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&lt;A&gt;::fallible_with_capacity (mod.rs:1186:30)
  │     }
  │   }
  │   ├── PP 1.3.1/2 {
  │   │     Total:     7,939,188 bytes (43.04%, 3,832,283.29/s) in 37,449 blocks (10%, 18,076.81/s), avg size 212 bytes, avg lifetime 22.1 µs (0% of program duration)
  │   │     Max:       212 bytes in 1 blocks, avg size 212 bytes
  │   │     At t-gmax: 212 bytes (0.01%) in 1 blocks (3.85%), avg size 212 bytes
  │   │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │   │     Allocated at {
  │   │       ^1: 0x556ef23a6e3b: &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (alloc.rs:226:9)
  │   │       ^2: 0x556ef23a6e3b: hashbrown::raw::alloc::inner::do_alloc (alloc.rs:11:9)
  │   │       ^3: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&lt;A&gt;::new_uninitialized (mod.rs:1157:38)
  │   │       ^4: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&lt;A&gt;::fallible_with_capacity (mod.rs:1186:30)
  │   │       #5: 0x556ef23a0102: hashbrown::raw::RawTableInner&lt;A&gt;::prepare_resize (mod.rs:1396:29)
  │   │       #6: 0x556ef23a0102: hashbrown::raw::RawTable&lt;T,A&gt;::resize (mod.rs:788:17)
  │   │       #7: 0x556ef23a0102: hashbrown::raw::RawTable&lt;T,A&gt;::reserve_rehash (mod.rs:693:13)
  │   │       #8: 0x556ef23a6ca7: hashbrown::raw::RawTable&lt;T,A&gt;::reserve (mod.rs:646:16)
  │   │       #9: 0x556ef23a6ca7: hashbrown::raw::RawTable&lt;T,A&gt;::insert (mod.rs:827:17)
  │   │       #10: 0x556ef23a7d1e: hashbrown::map::HashMap&lt;K,V,S,A&gt;::insert (map.rs:1266:13)
  │   │       #11: 0x556ef23a87ff: std::collections::hash::map::HashMap&lt;K,V,S&gt;::insert (map.rs:845:9)
  │   │       #12: 0x556ef23a87ff: &lt;serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize::MapVisitor&lt;K,V,S&gt; as serde::de::Visitor&gt;::visit_map (impls.rs:1285:29)
  │   │       #13: 0x556ef23a87ff: &lt;&amp;mut csv::deserializer::DeRecordWrap&lt;T&gt; as serde::de::Deserializer&gt;::deserialize_map (deserializer.rs:520:13)
  │   │       #14: 0x556ef23a84f6: serde::de::impls::&lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&lt;K,V,S&gt;&gt;::deserialize (impls.rs:1293:17)
  │   │       #15: 0x556ef23a84f6: csv::deserializer::deserialize_byte_record (deserializer.rs:47:5)
  │   │       #16: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │   │     }
  │   │   }
&lt;snip&gt;
  ├── PP 1.6/7 {
  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 37,449 blocks (10%, 18,076.81/s), avg size 26 bytes, avg lifetime 39.84 µs (0% of program duration)
  │     Max:       26 bytes in 1 blocks, avg size 26 bytes
  │     At t-gmax: 26 bytes (0%) in 1 blocks (3.85%), avg size 26 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23ad5e4: &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23ad5e4: alloc::raw_vec::RawVec&lt;T,A&gt;::allocate_in (raw_vec.rs:195:45)
  │       #3: 0x556ef23ad5e4: alloc::raw_vec::RawVec&lt;T,A&gt;::with_capacity_in (raw_vec.rs:136:9)
  │       #4: 0x556ef23ad5e4: alloc::vec::Vec&lt;T,A&gt;::with_capacity_in (mod.rs:580:20)
  │       #5: 0x556ef23ad5e4: &lt;T as alloc::slice::hack::ConvertVec&gt;::to_vec (slice.rs:211:25)
  │       #6: 0x556ef23ad5e4: alloc::slice::hack::to_vec (slice.rs:163:9)
  │       #7: 0x556ef23ad5e4: alloc::slice::&lt;impl [T]&gt;::to_vec_in (slice.rs:476:9)
  │       #8: 0x556ef23ad5e4: alloc::slice::&lt;impl [T]&gt;::to_vec (slice.rs:453:9)
  │       #9: 0x556ef23ad5e4: csv::byte_record::ByteRecord::clone_truncated (byte_record.rs:508:23)
  │       #10: 0x556ef23ad5e4: &lt;csv::reader::ByteRecordsIter&lt;R&gt; as core::iter::traits::iterator::Iterator&gt;::next (reader.rs:2166:33)
  │       #11: 0x556ef23aa82f: perf_and_dhat_profiling_example::read_csv (main.rs:43:19)
  │     }
  │   }
&lt;snip&gt;
</code></pre>
<p>Pay attention to a few things here:</p>
<ul>
<li>blocks is just a specific call to an allocation, so &quot;100 blocks&quot; means 100
individual allocation calls, I think.</li>
<li>the hashbrown (HashMap) calls end up producing more total bytes than the
actual deserialize calls themselves, but the byterecord iteration seem to be
broken up, likely because the byterecord needs to allocate for the individual
values or handing back the ByteRecord value itself.</li>
</ul>
<p>Particularly, let's look at this output:</p>
<pre><code>  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 112,347 blocks (30%, 54,230.42/s), avg size 8.67 bytes, avg lifetime 13.41 µs (0% of program duration)
  │     Max:       26 bytes in 3 blocks, avg size 8.67 bytes
&lt;snip&gt;
  │     Allocated at {
  │       #1: 0x556ef23a8ab5: &lt;alloc::alloc::Global as core::alloc::Allocator&gt;::allocate (alloc.rs:226:9)
&lt;snip&gt;
  │       #21: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │     }
  │   }
</code></pre>
<p>Although it's useful to know the total, max, and etc. what this is telling us is
that each deserialize step is leading to plenty of allocations. We had
previously estimated, based on <code>perf</code>s output, that it would be good idea to
stop deserializing if we can, but this confirms that it is definitely happening
a lot and is non-trivial in the scheme of things. Before we begin changing code,
we should put in benchmarks to let others running our code confirm results for
themselves (possibly documenting our process like how we've done in this
article, too!).</p>
<h3>Benchmarks for reproduction</h3>
<p>We now know we want to remove our <code>deserialize</code> code and that means finding
something we can recycle on every iteration of our loop. Reading through the
<code>csv</code> docs it looks like we can reuse a single <code>ByteRecord</code>. Having benchmarks
in place will help us verify for ourselves, and others, that the change we've
put in has actually made a net positive gain.</p>
<pre><code>#[cfg(test)]
mod tests {
    extern crate test;

    use super::*;
    use test::{black_box, Bencher};

    #[bench]
    fn bench_read_csv(b: &amp;mut Bencher) {
        let bytes = read_file(&amp;&quot;test.csv&quot;.into()).expect(&quot;failed to read file&quot;);
        b.iter(|| {
            for _ in 1..2 {
                black_box(read_csv(&amp;bytes)).expect(&quot;benchmark failure&quot;);
            }
        });
    }
}
</code></pre>
<p>You'll have to chuck <code>#[feature(test)]</code> at the top of <code>main.rs</code> and run the
benches with nightly. You can pass <code>+nightly</code> to do that on the fly without
having to change the current toolchain.</p>
<pre><code>; cargo +nightly bench
   Compiling perf-and-dhat-profiling-example v0.1.0 (/home/rjs/repos/perf-and-dhat-profiling-exam
ple)
    Finished bench [optimized] target(s) in 1.54s
     Running unittests (target/release/deps/perf_and_dhat_profiling_example-3cb69d975d903441)

running 1 test
test tests::bench_read_csv ... bench:  18,044,486 ns/iter (+/- 551,993)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 0 filtered out; finished in 5.47s
</code></pre>
<p>This shows us an average of 18,044,486 nanoseconds per iteration. 1,000
nanoseconds is a microseconds, and 1000 microseconds is a millisecond, thus we have 18
milliseconds per iteration to run against our test case.</p>
<p>Now let's try the single allocation for ByteRecord,</p>
<pre><code>diff --git a/src/main.rs b/src/main.rs
index 8d425c1..a68b0ec 100644
 a/src/main.rs
+++ b/src/main.rs
@@ -3,7 +3,6 @@
 use csv::Reader;
 #[cfg(feature = &quot;dhat-on&quot;)]
 use dhat;
-use serde_bytes::ByteBuf;
 use std::collections::HashMap;
 use std::convert::TryInto;
 use std::error::Error;
@@ -17,8 +16,6 @@ use dhat::{Dhat, DhatAlloc};
 #[global_allocator]
 static ALLOCATOR: DhatAlloc = DhatAlloc;

-type Record = HashMap&lt;String, ByteBuf&gt;;
-
 const NULL: &amp;'static str = &quot;NULL&quot;; // or whatever.

 #[derive(Clone, Debug, PartialEq, Eq, Hash)]
@@ -40,12 +37,12 @@ pub fn read_file(data: &amp;PathBuf) -&gt; Result&lt;Vec&lt;u8&gt;, Box&lt;dyn Error&gt;&gt; {

 pub fn read_csv(data: &amp;[u8]) -&gt; Result&lt;Vec&lt;Option&lt;Field&gt;&gt;, Box&lt;dyn Error&gt;&gt; {
     let mut reader = Reader::from_reader(data);
-    let headers = reader.headers().unwrap().clone().into_byte_record();
+    let _headers = reader.headers().unwrap().clone().into_byte_record();
     let mut fields = vec![];
-    for record in reader.byte_records() {
-        let record = record?;
-        let record: Record = record.deserialize(Some(&amp;headers))?;
-        for (_, value) in record.into_iter() {
+    let mut record = csv::ByteRecord::new();
+    while !reader.is_done() {
+        reader.read_byte_record(&amp;mut record).unwrap();
+        for value in record.iter() {
             fields.push(parse(&amp;value));
         }
     }
</code></pre>
<p>and rerun the benchmark,</p>
<pre><code>   Compiling perf-and-dhat-profiling-example v0.1.0 (/home/rjs/repos/perf-and-dhat-profiling-exam
ple)
    Finished bench [optimized] target(s) in 1.22s
     Running unittests (target/release/deps/perf_and_dhat_profiling_example-3cb69d975d903441)

running 1 test
test tests::bench_read_csv ... bench:   4,528,433 ns/iter (+/- 1,328,048)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 0 filtered out; finished in 4.22s
</code></pre>
<p>We're down to four milliseconds! Which is a ~4x improvement in terms of timing. Let's confirm it in a few ways:</p>
<p><code>perf stat</code></p>
<pre><code> Performance counter stats for 'system wide' (100 runs):

             37.61 msec cpu-clock                 #    3.537 CPUs utilized            ( +-  1.93% )
                94      context-switches          #    0.002 M/sec                    ( +-  3.05% )
                11      cpu-migrations            #    0.300 K/sec                    ( +-  3.59% )
               300      page-faults               #    0.008 M/sec                    ( +-  6.57% )
        34,636,088      cycles                    #    0.921 GHz                      ( +-  6.86% )  (13.69%)
        57,927,487      instructions              #    1.67  insn per cycle           ( +-  6.59% )  (51.81%)
        15,272,168      branches                  #  406.078 M/sec                    ( +-  5.84% )  (88.26%)
            38,666      branch-misses             #    0.25% of all branches          ( +-  4.23% )  (86.61%)
        16,682,061      L1-dcache-loads           #  443.566 M/sec                    ( +-  5.51% )  (48.15%)
           452,163      L1-dcache-load-misses     #    2.71% of all L1-dcache accesses  ( +-  8.41% )  (11.68%)
             5,912      LLC-loads                 #    0.157 M/sec                    ( +- 70.99% )  (0.03%)
     &lt;not counted&gt;      LLC-load-misses                                               (0.00%)

         0.0106321 +- 0.0000668 seconds time elapsed  ( +-  0.63% )

</code></pre>
<p>where we can see <code>LLC-load-misses</code> aren't even counted anymore.</p>
<p><code>perf record</code> and friends, which show that read_byte_record takes just as long
as our <code>parse</code> code.</p>
<figure>
  <img
    src="/assets/images/perf-flamegraph-screenshot-after.jpg"
    alt="A flamegraph of our optimised program"
    title="A flamegraph of our optimised program">
  </img>
</figure>
<p><code>dhat</code> instrumentation, which shows our overall memory footprint is drastically reduced,</p>
<pre><code>&lt;snip&gt;
PP 1/1 (18 children) {
    Total:     3,539,231 bytes (100%, 115,733,004.15/s) in 40 blocks (100%, 1,308/s), avg size 88,480.78 bytes, avg lifetime 8,511.5 µs (27.83% of program duration)
    At t-gmax: 2,312,340 bytes (100%) in 16 blocks (100%), avg size 144,521.25 bytes
    At t-end:  1,024 bytes (100%) in 1 blocks (100%), avg size 1,024 bytes
    Allocated at {
      #0: [root]
    }
  }
  ├─▼ PP 1.1/18 (3 children) {
  │     Total:     262,280 bytes (7.41%, 8,576,567.15/s) in 22 blocks (55%, 719.4/s), avg size 11,921.82 bytes, avg lifetime 4,408.95 µs (14.42% of program duration)
  │     At t-gmax: 131,184 bytes (5.67%) in 5 blocks (31.25%), avg size 26,236.8 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x565479a3edc2: &lt;dhat::DhatAlloc as core::alloc::global::GlobalAlloc&gt;::alloc (lib.rs:244:9)
  │       #2: 0x565479a37aa5: alloc::raw_vec::finish_grow (result.rs:0:23)
  │     }
  │   }
  │   ├── PP 1.1.1/3 {
  │   │     Total:     262,136 bytes (7.41%, 8,571,858.34/s) in 15 blocks (37.5%, 490.5/s), avg size 17,475.73 bytes, avg lifetime 1,068.07 µs (3.49% of program duration)
  │   │     Max:       131,072 bytes in 1 blocks, avg size 131,072 bytes
  │   │     At t-gmax: 131,072 bytes (5.67%) in 1 blocks (6.25%), avg size 131,072 bytes
  │   │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │   │     Allocated at {
  │   │       ^1: 0x565479a3edc2: &lt;dhat::DhatAlloc as core::alloc::global::GlobalAlloc&gt;::alloc (lib.rs:244:9)
  │   │       ^2: 0x565479a37aa5: alloc::raw_vec::finish_grow (result.rs:0:23)
  │   │       #3: 0x565479a307f4: alloc::raw_vec::RawVec&lt;T,A&gt;::grow_amortized (raw_vec.rs:442:19)
  │   │       #4: 0x565479a307f4: alloc::raw_vec::RawVec&lt;T,A&gt;::reserve::do_reserve_and_handle (raw_vec.rs:333:28)
  │   │       #5: 0x565479a3819f: alloc::raw_vec::RawVec&lt;T,A&gt;::reserve (raw_vec.rs:337:13)
  │   │     }
  │   │   }
&lt;snip&gt;
</code></pre>
<p>Which shows us that we've gone from 18,443,933 bytes to 3,539,231 bytes and we
are mostly spending time largely building <code>RawVec</code>s now.</p>
<p>Finally <code>hyperfine</code>.</p>
<pre><code>; hyperfine &quot;target/release/perf-and-dhat-profiling-example test.csv&quot; &quot;target/release/perf-and-dhat-profiling-example-unoptimized test.csv&quot;
Benchmark #1: target/release/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):       8.9 ms ±   0.5 ms    [User: 8.0 ms, System: 0.9 ms]
  Range (min … max):     8.4 ms …  11.8 ms    291 runs

Benchmark #2: target/release/perf-and-dhat-profiling-example-unoptimized test.csv
  Time (mean ± σ):      23.7 ms ±   1.2 ms    [User: 22.5 ms, System: 1.1 ms]
  Range (min … max):    22.6 ms …  31.8 ms    121 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet PC without any interferences from other programs. It might help to use the '--warmup' or '--prepare' options.

Summary
  'target/release/perf-and-dhat-profiling-example test.csv' ran
    2.66 ± 0.19 times faster than 'target/release/perf-and-dhat-profiling-example-unoptimized test.csv'
</code></pre>
<p>Lesson: &quot;trivial allocations&quot; are not so trivial!</p>
<h2>Conclusion</h2>
<p>Using <code>perf</code> and a memory analysis tool like <code>dhat</code> can give you a pretty solid
picture of what is going on in terms of hardware utilization. Obviously it's not
the only profilers at your disposal, and we'll likely tackle some other articles
doing the same above with different tooling, but discovering issues backed by
numbers is fundamentally crucial in demonstrating to yourself and others that a
change is worth accepting.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Many Uses Of The Empty Tuple</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-many-uses-of-the-empty-tuple.html</link>
      <guid>https://justanotherdot.com/posts/the-many-uses-of-the-empty-tuple.html</guid>
      <pubDate>Thu, 11 Feb 2021 08:12:41 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>For newcomers to Rust unfamiliar with the empty tuple (<code>()</code>), it can be a
confusing type; what's the point of this thing and how can I use it
idiomatically rather than having to endure its presence? Here are some common
patterns where unit actually plays a helpful role in both guiding program
construction, helping with readability, and even reducing unnecessary memory
allocations.</p>
<h3>A Primer</h3>
<p><em>If you feel comfortable with unit or don't really care about the nuances, feel
free to skip this section, but it is short and recommended as it informs the
tips below.</em></p>
<p>The empty tuple (<code>()</code>), sometimes affectionately, and historically, referred to
as <code>unit</code>, as I will call it for the rest of this article, describes the type of
something that <em>does</em> have a value, but a value you don't care about. The empty
tuple is perfect for these situations because it can never carry any information
with it, but it is different to the <code>never</code> (<code>!</code>) type in Rust because the
<code>never</code> type designates the type of things that can never be constructed (hence
the name). Thus, functions that may never return are technically <code>!</code>, but
functions that perform some sort of effect are <code>()</code>. The <code>void</code> keyword fills a
similar role to unit in C-derived languages, but unit has the advantage of being
usable in type annotations, which we'll see come in handy later.</p>
<p>For starters, <code>()</code> is both the type and the value of the type. Sometimes unit is
implicit, such as when you write a function with no return type, or when you
slap a semicolon on an expression. In Rust, a great many things are expressions,
which means they have values and therefore have types. For example, assigning to
an assignment, such as in the case <code>let x = let y = 12;</code> gives us <code>y = 12</code> and
<code>x = ()</code> as an assignment expression itself has type unit. For each example we
don't care about the value and only care about the action that took place (the
action of the function invoked or the expression that was run, the act of
assigning a value to a variable name, and so on.)</p>
<h3>Why Use An If-Let?</h3>
<p><code>if-let</code> let's us combine both the niceties of pattern matching with a <code>match</code>
statement, without having to be explicit about fall-through cases. If you are
pattern matching only to perform some action at the end, you can be more concise
and simply turn this:</p>
<pre><code>fn side_effect() {
  println!(&quot;a side effect&quot;);
}

let connection = socket.accept();

match connection {
  Ok(_) =&gt; side_effect(),
  _ =&gt; (),
}
</code></pre>
<p>Into this:</p>
<pre><code>fn side_effect() {
  println!(&quot;a side effect&quot;);
}

let connection = socket.accept();

if let Ok(_) = connection {
  side_effect();
}
</code></pre>
<p>Given that we used a semicolon on <code>side_effect</code> it would have compiled just as
fine if we had a return value from <code>side_effect</code>, unless you use <code>Result</code> which
is marked as <code>must_use</code> in the compiler, forcing you to deal with the errors
that may crop up. In that case, if you truly wanted to ignore a return value of
a function, you could do <code>let _ = side_effect();</code> instead.</p>
<h3>Clarifying The Presence And Reason Of Why Things Failed</h3>
<p>One common mistake that newbies will make is to avoid error handling with
<code>Result</code> in favor of simply having functions that return <code>()</code> and panicking via
<code>expect</code> or <code>unwrap</code> et. al. <code>()</code> tends to designate that things are <em>not</em> going
to blow up at runtime, and so this type is actually the wrong thing to signal to
other peers. Rust has a type called <code>!</code> or &quot;never&quot; that implies that something
may never return or fail (&quot;infallible&quot;).</p>
<p>A simple flow chart for choosing a type for error handling could go something
like this:</p>
<ol>
<li>
<p>Nothing obvious in the code path in question is going to fail, including via
panics, e.g. with <code>unwrap</code>, <code>expect</code>, etc.: use <code>()</code></p>
</li>
<li>
<p>I know this might fail and I</p>
<ol>
<li>Want to know why: use <code>Result</code></li>
<li>Only care about the presence or absence of something: use <code>Option</code></li>
</ol>
</li>
<li>
<p>I know this is going to live forever and never return (like a socket
connection): use <code>!</code></p>
</li>
</ol>
<p>Although I don't commonly see the third case used as often as the prevalence of
the first and second cases, I do think it is useful to signal to others that
something is going to loop indefinitely, replace the current process with
<code>exec</code>, and so on. It currently requires a crate attribute, which may be enough
to keep you away from using it until it lands on stable without the need for the
annotation.</p>
<h3>The Traverse Trick</h3>
<p>There is a common pattern in Haskell called <code>traverse</code>. A way to think about it
is like turning a collection of things inside out. For example, if we have a
<code>Vec&lt;Result&lt;T, E&gt;&gt;</code> we can 'traverse' on this collection, treating it's values
as inputs to a function, and turn it into a <code>Result&lt;Vec&lt;T&gt;, E&gt;</code> instead. This is
wildly useful, and you can extend this pattern for your own types and
collections, too, but one common use case is a function that returns <code>Result&lt;T, E&gt;</code> that we want to run over several elements:</p>
<pre><code>struct Error;

fn may_fail(x: i32) -&gt; Result&lt;i32, Error&gt; {
  Ok(x)
}

fn main() {
  let inputs = vec![1, 2, 3];
  let outputs: Result&lt;Vec&lt;i32&gt;, Error&gt; = inputs.into_iter().map(may_fail).collect();
}
</code></pre>
<p>What if we didn't want the outputs? What if all we wanted to do was to run
<code>may_fail</code> for the effects it produces? We could change this around:</p>
<pre><code>struct Error;

fn may_fail(x: i32) -&gt; Result&lt;i32, Error&gt; {
  Ok(x)
}

fn main() {
  let inputs = vec![1, 2, 3];
  Result&lt;Vec&lt;i32&gt;, Error&gt; =
  inputs.into_iter().map(|i| { may_fail(i).map(|_| ()) }).collect::&lt;Result&lt;Vec&lt;()&gt;, Error&gt;&gt;();
}
</code></pre>
<p>But now we are allocating a vector just to fill in all the units. Let's fix
that:</p>
<pre><code>struct Error;

fn may_fail(x: i32) -&gt; Result&lt;i32, Error&gt; {
  Ok(x)
}

fn main() {
  let inputs = vec![1, 2, 3];
  Result&lt;Vec&lt;i32&gt;, Error&gt; =
  inputs.into_iter().map(|i| { may_fail(i); }).collect::&lt;Result&lt;(), Error&gt;&gt;();
}
</code></pre>
<p>This version is specialized as it will never allocate; no collection is being
created and each <code>()</code> type can be optimized away by the compiler as having no
bearing on program semantics.</p>
<p>You can similarly do this for Option, and, as mentioned, can implement the same
trick for your own custom types.</p>
<h3>Figuring Out Types With Invalid Annotations</h3>
<p>This one is particularly helpful if you are not using something like
<code>rust-analyzer</code> or the like. If you prefer to simply run the compiler in a loop,
such as with <code>cargo watch</code>, you can get immediate feedback on the type of
something by assigning the value to an invalid type, such as:</p>
<pre><code>main() {
  let x: () = mystery();
}
</code></pre>
<p>You just want to use some type of annotation you are absolutely sure this thing
is <em>not</em>, and <em>most</em> things are <em>not</em> unit. If unit doesn't work, you can switch
it up to other unlikely things: <code>u128</code>, <code>!</code> (requires crate attribute), and on
and on. If you know the thing is a collection, try a scalar value. Usually it
doesn't take much guesswork to get the compiler to spit something out, but you
will wind up with something like the following:</p>
<pre><code>   Compiling playground v0.0.1 (/playground)
error[E0308]: mismatched types
 --&gt; src/main.rs:6:17
  |
6 |     let x: () = mystery();
  |            --   ^^^^^^^^^ expected `()`, found struct `BTreeSet`
  |            |
  |            expected due to this
  |
  = note: expected unit type `()`
                found struct `BTreeSet&lt;i32&gt;`
</code></pre>
<p>Above we see what we claimed was the real type and what the compiler inferred or
realized is the real type. You can flip this trick on the head, too. With
generics it can be easy for types to become things you didn't quite intend
simply by how they got used in other contexts, hence it can be helpful to
sprinkle around annotations in code to be really clear on precisely what final
shape(s) you are expecting to deal with.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Increase The Rate Of Oxidation: Getting More Rust Into Your Life</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/increase-the-rate-of-oxidation-getting-more-rust-into-your-life.html</link>
      <guid>https://justanotherdot.com/posts/increase-the-rate-of-oxidation-getting-more-rust-into-your-life.html</guid>
      <pubDate>Thu, 04 Feb 2021 08:47:37 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>With the growing number of resources to learn Rust nowadays, where does one even
begin? How does one get into writing idiomatic Rust code as quickly as possible?
If passively consuming content isn't for you, here's some tips and ideas to
kickstart your <a href="https://en.wikipedia.org/wiki/Carcinisation">carcinisation</a>.</p>
<h3>Tips For Breaking Down Your First Bit Of Metal</h3>
<p>You're excited to learn Rust and you want to build something; awesome!</p>
<p>Here's the rub, though: projects are great for learning, but have you ever
stopped to consider the scale and dimensions of what you're about to undertake?
Pick a project that's too gargantuan and you wind up stopping dead in your
tracks early on. Pick a project that's too trivial and you feel unsatisfied from
what you've learned. Both of these finishes can put a knot in your quest to learn
the language.</p>
<p>This isn't to say that you can't have failed projects that have taught you
lessons. The bigger problem stems from failed projects that affect your
motivation to continue learning.</p>
<p>Here's a number of tips and approaches I've picked up over the years that have
worked for me coming to new languages or technologies:</p>
<ul>
<li>
<p><strong>Don't fuss over quirks</strong> - Quirks of a language and its tooling are
important, and you'll get to them, but it's wise to think about knowledge like
<a href="https://www.developgoodhabits.com/rock-pebbles-sand/">a jar that's to be filled with
rocks</a>; focus on locking
in the big concepts first, then fill out your knowledge with the next smaller
granularity of detail. The jar-filled-with-rocks analogy is a bit like Zeno's
paradox, however, as <a href="https://www.justanotherdot.com/posts/an-infinite-barrage-of-mountains-to-climb.html">there is always another finer grained detail you can put
into the jar</a>.</p>
</li>
<li>
<p><strong>Be specific before you generalize</strong> - try to write concrete implementations
before you tackle generics. This is true even if you're <em>not</em> starting anew
with Rust! It is drastically simpler to generalize a tableau of concrete
things than it is to first start with a generalization. Keep in mind this
isn't a rule, and sometimes starting with the generalization will save you a
lot of headaches.</p>
</li>
<li>
<p><strong>You can borrow/reference things later</strong> - writing correct programs that clone all
over the place should be your first order of business. In the same spirit of
&quot;make it work&quot;, the beauty of having <code>clone</code> be explicit is you will be able
to tell where copies are happening, and hence be able to switch over to
borrowing <strong>in time</strong>. Until then, focus first on getting the foundations
laid down, namely with ownership.</p>
</li>
<li>
<p><strong>Foundations are built from smaller insights</strong> - if there is anything you
take away from this article, it should be this: large-scale projects are
educational, but the learnings from them can be sparse and unclear. It's far
better to start with smaller understandings and build up from there to bigger
projects. If you do want to do a bigger project, focus on each module as it's
own encapsulated thing that can be tested and run in isolation and later
rigged up to the main system.</p>
</li>
<li>
<p><strong>Hold an experimenters mindset</strong> - as you cover the surface area of the
language, you'll start to ask questions. Those questions are best answered
<em>and captured</em> in minimal code examples or in notes. I find code examples
better here because I can continually run the example against different
versions of a language as time goes on. Using gists or gist-like services is a
great way to store ad hoc solutions to questions you have. In fact, the <a href="https://play.rust-lang.org/">Rust
playground</a> will save its permalinks to gist
files, but it can help to have everything all in a single place for yourself.
GitHub gists also support multi-file gists meaning you can tuck in a
<code>Cargo.toml</code> manifest or other parts that may be relevant to the code. This is
why I also suggest sometimes having a <code>lab</code> or <code>playground</code> repository for
various snippets, too, but <code>cargo new</code> already bootstraps a git repo that it
is easy to push up various experiments. If you are using GitHub, you can
automate the <code>gh</code> CLI to also create a new repository, too, allowing you to
automate the whole process without having to go to a browser.</p>
</li>
<li>
<p><strong>Read rust code from major projects</strong> - If you want to start writing
idiomatic code, try observing the common patterns you see in major projects.
Idioms are formed as a communal reaction. Understanding global and local
idioms alike can help you discover which work for you or your team. Resources
like <a href="https://lib.rs/">lib.rs</a>, the Rust standard library, and other major
projects on various source code hosting platforms can let you dip into
interesting portions of code without being overwhelmed by the whole of the
project.</p>
</li>
<li>
<p><strong>Translate code from another language to explore the shape of things</strong> - For
those coming to Rust from another language, it can help to translate from your
language of choice <em>with the caveat that Rust is its own language and the
mapping is not going to be one-to-one</em>. Translations are helpful as lessons
around what is similarly possible, different, and non-existent between your
source and target languages. As an example, I learned a lot about closures and
function types in Rust when I <a href="https://github.com/justanotherdot/rust-hedgehog">tried to port a property-based testing library
from Haskell to Rust</a>, but I
now know I would not build it the same way that the Haskell code was written.
There is a similar desire to do the same for C, C++, or other C-derived
languages since they are so similar to Rust, but again, Rust is enough of it's
own beast that you are likely better off deeply understanding the concepts and
having a mental model of what you are about to build and going off that rather
than directly porting code. Also, side note: if you have a desire to cleanup
the code you are about to translate, keep in mind that the sooner you start
writing <em>Rust</em> code. This is also true if the intent of the conversion isn't
about education and more about bringing over code into Rust, as you will be
able to refine the code once you have the rough semblance of logic laid down.</p>
</li>
</ul>
<p>If there is a single most important point above, it is the point about the
&quot;experimentalist&quot; or &quot;explorer's&quot; mindset. Treat what you are about to go into
like an alien landscape; if you make small, insular understandings over time,
they will chalk up to forming a really intricate patchwork of knowledge. <strong>Rust
is a hard language, but the notion is that by moving slower in localised spots
we move faster overall</strong>. Rust seeks to help you discover more problems at the
time you are writing code than when you have released it into the wild. With
time, any programming language gets faster to write code in overall: you know
the idioms, the options for solving various problems, common libraries, and what
is caked into the language from the start.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Cut Down Time Debugging With Rich Error Types</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/cut-down-time-debugging-with-rich-error-types.html</link>
      <guid>https://justanotherdot.com/posts/cut-down-time-debugging-with-rich-error-types.html</guid>
      <pubDate>Fri, 29 Jan 2021 06:47:15 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Have you ever spent a considerable amount of time tracking down the meaning of
an error flag or code after a program has crashed? In languages that don't let
you break down values with pattern matching, booleans and error codes run
rampant and require extra investigative effort on the part of the programmer.
Diagnosing problems in programs doesn't have to be hard in Rust given we have
<code>Result</code> to carry along lots of useful information for us.</p>
<p>There was a recent video on why <a href="https://www.youtube.com/watch?v=zQC8T71Y8e4">std::process::exit is
'evil'</a> demonstrating that by
requesting normal or abnormal termination by the operating system through
<code>std::process::exit</code> you could fail to do cleanup that the operating system may
fail to do. I would say std::process::exit is quirky rather than evil here
because it is doing exactly what you ask of it. The example is roughly like
this:</p>
<pre><code>#[derive(Debug)]
struct Resource(i32);

impl Drop for Resource {
    fn drop(&amp;mut self) {
        println!(&quot;cleaning up resource: {}&quot;, self.0);
    }
}

enum Error {
    Foo
}

impl Error {
    pub fn exit_code(self) -&gt; i32 {
        match self {
            Error::Foo =&gt; 114,
        }
    }
}

fn main() {
    let _x = Resource(0);
    println!(&quot;about to terminate the process&quot;);
    std::process::exit(1); // &quot;cleaning up resource: 0&quot; never prints.
}
</code></pre>
<p>In the above code, the destructor for <code>Resource</code> never runs because the program
is effectively terminated at the point that <code>std::process::exit</code> is called. It's
a blunt tool, and can be used for both zero and non-zero exit codes, which in an
operating system execution context can roughly relate to success or failure
respectively. Exit codes allow both minimal diagnostic information and sometimes
even a way to handle control flow, <a href="https://www.justanotherdot.com/posts/discovering-problematic-commits-with-git-bisect.html">as is the case with driving <code>git bisect</code>
automatically</a>.</p>
<p>Some resources definitely need cleanup on program failure and the solution is to
wrap the main logic in another function, preferably one that returns <code>Result</code>,
to ensure the resources go out of scope before calling <code>std::process::exit</code>. The
solution given in the video has this function returning an exit code (an i32)
for it's error:</p>
<pre><code>#[derive(Debug)]
struct Resource(i32);

impl Drop for Resource {
    fn drop(&amp;mut self) {
        println!(&quot;cleaning up resource: {}&quot;, self.0);
    }
}

fn run() -&gt; Result&lt;(), i32&gt; {
    let _x = Resource(0);
    Err(114)
}

fn main() {
    let _x = Resource(0);
    run().unwrap_or_else(|exit_code| {
        println!(&quot;about to terminate the process&quot;);
        std::process::exit(exit_code);
    });
}
</code></pre>
<p>Now we have destructors running on exit, but we have totally lost relevant
<em>human readable</em> diagnostic information in the process. <strong>By using richer types
for our errors we gain that information back</strong>:</p>
<pre><code>use std::fmt::{Formatter, Display};

#[derive(Debug)]
struct Resource(i32);

impl Drop for Resource {
    fn drop(&amp;mut self) {
        println!(&quot;cleaning up resource: {}&quot;, self.0);
    }
}

enum Error {
    MissingData
}

impl Display for Error {
    fn fmt(&amp;self, f: &amp;mut Formatter) -&gt; Result&lt;(), std::fmt::Error&gt; {
        match self {
            Error::MissingData =&gt; write!(f, &quot;could not find any data&quot;),
        }
    }
}

impl Error {
    pub fn exit_code(self) -&gt; i32 {
        match self {
            Error::MissingData =&gt; 114,
        }
    }
}


fn start() -&gt; Result&lt;(), Error&gt; {
    let _x = Resource(0);
    return Err(Error::MissingData);
}

fn main() {
    let _x = Resource(0);
    start().unwrap_or_else(|e| {
        println!(&quot;about to terminate the process&quot;);
        eprintln!(&quot;[program] {}&quot;, e);
        std::process::exit(e.exit_code());
    });
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=275ccf13e2ca30f6bdb0f96c8b58463f">playground</a></p>
<p>In the above code we can propagate failures from various parts of our program
and we don't have to lose that information that is useful for diagnostics. By
using a method to map the error to an exit code, we decide to shed that
information at the point when we call <code>std::process::exit</code>, allowing us
to print out our error onto stderr or pattern match on it to do emit metrics,
and so on. <strong>If you want to avoid the headache of tracking down bugs in production
systems, keep information as semantically rich as possible for as long as
possible</strong>. If you think of a program like a parser that builds up values from
external input or stimuli, then you want to take advantage of that work for as
long as possible and only discard it at the fringes.</p>
<p><em>Astute readers will note that what I've we've written above is the
<a href="https://doc.rust-lang.org/std/process/trait.Termination.html"><code>Termination</code>
trait</a> that is
<a href="https://github.com/rust-lang/rust/issues/43301">pending stabilization</a> but I
personally feel teaching others how to get similar results without having to
rely on unstable features is a reasonable tradeoff. When
std::process::Termination stabalizes I'll be sure to give this article a
refresher.</em></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Quick and Dirty Benchmarking</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/quick-and-dirty-benchmarking.html</link>
      <guid>https://justanotherdot.com/posts/quick-and-dirty-benchmarking.html</guid>
      <pubDate>Mon, 11 Jan 2021 08:19:45 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>In the past I've advocated for the default use of criterion in Rust projects as
the but it isn't always the fastest to run in a development loop for quick
feedback on optimisations. First, let's make our release builds incremental by
specifying the following in your project's <code>Cargo.toml</code> file.</p>
<pre><code>[profile.release]
incremental = true
</code></pre>
<p>Next, let's setup a baseline benchmark. I've made a template
<a href="https://gist.github.com/justanotherdot/0b0051f96bdeb44c25ad58998910f6a1">here</a>
you can dump into a pre-existing module or give it it's own module. We use
<code>cargo watch</code> here but it could just as well be any other tool that does the
same job, such as <code>entr</code>. This benchmarking suite is bundled only with nightly,
as it comes from the <code>libtest</code> crate.</p>
<p><code>cargo +nightly watch -x bench</code></p>
<p>Remember, the aim here is to get a really fast local feedback loop and not to be
producing rigorous, publishable results. We want to know if changes have general
speedups or slowdowns without having to wait for excessive periods of time.
Granted, the runtime of the the code the benchmark is executing plays a lot into
this, regardless of choice of harness. To minimize, focus on reducing:</p>
<ul>
<li>
<p>Number of iterations - as a rule of thumb, try to pick somewhere between 5 to
100 iterations depending on the code under inspection. You want to have some
confidence of an average between runs, but also not spend too long honing that
average.</p>
</li>
<li>
<p>Size of input - try to pick input sizes that are neither trivial nor massive,
as you want to ensure the code is getting properly exercised but also reduce
the time to completing a benchmark in general.</p>
</li>
</ul>
<p>It's worth stressing again that this isn't about building rigorous benchmarks
for comparisons to other projects but to build benchmarks that help you
understand the general trend of whether or not your changes are making
improvements or regressing.</p>
<h3>Alternative Approaches</h3>
<p>Sometimes a benchmark like the above may be a bit awkward given the way the code
is laid out, and if you have a binary or can cake the logic into a binary, it
may be alright to record the respective wall times across invocations with a
process. From scratch, let's build out a tester binary for us to run. First,
we'll put in <code>structopt</code> for easily switching between changes we want to
experiment against. As <code>structopt</code> is a nice veneer over <code>clap</code>, there's really
no advantage to either except it might help you get results faster.</p>
<pre><code>#[derive(Debug, StructOpt)]
#[structopt(name = &quot;cli&quot;, about = &quot;Benchmark harness for X.&quot;)]
struct Opt {
    #[structopt(parse(from_os_str))]
    input: PathBuf,

    #[structopt(long = &quot;x1&quot;)]
    x1: bool,

    #[structopt(long = &quot;x2&quot;)]
    x2: bool,
}

&lt;snip&gt;

fn main() {
    let opt = Opt::from_args();
    if opt.x1 {
        example1(&amp;opt.input).expect(&quot;[cli] example1 failure&quot;);
    } else if opt.x2 {
        example2(&amp;opt.input).expect(&quot;[cli] example2 failure&quot;);
    } else {
        baseline(&amp;opt.input).expect(&quot;[cli] example failure&quot;);
    }
}
</code></pre>
<p>We use an input file above, but we could just as easily take input from
anywhere, either embedded in the program or even from stdin, for example. We are
going to run the program using <code>hyperfine</code> which wraps up <code>criterion</code> into a
neat bundle and is infinitely useful for comparing wall time averages versus
manually invoking <code>time</code> multiple times and performing the aggregations
yourself:</p>
<pre><code>; cargo build --release
; hyperfine &quot;cli test.in&quot; &quot;cli --x1 test.in&quot; &quot;cli --x2 test.in&quot;
</code></pre>
<p>Which gives us some nice output <strong>and a summary</strong> of the fastest variant in
relation to the others:</p>
<pre><code>; hyperfine &quot;target/release/cli test.in&quot; &quot;target/release/cli --x1 test.c
sv&quot; &quot;target/release/cli --x2 test.in&quot;
Benchmark #1: target/release/cli test.in
  Time (mean ± σ):       7.7 ms ±   0.5 ms    [User: 6.5 ms, System: 1.2 ms]
  Range (min … max):     7.1 ms …  11.2 ms    385 runs

Benchmark #2: target/release/cli --x1 test.in
  Time (mean ± σ):      16.3 ms ±   0.7 ms    [User: 13.9 ms, System: 3.7 ms]
  Range (min … max):    15.2 ms …  20.7 ms    171 runs

Benchmark #3: target/release/cli --x2 test.in
  Time (mean ± σ):      18.0 ms ±   2.3 ms    [User: 58.9 ms, System: 2.9 ms]
  Range (min … max):    14.7 ms …  29.4 ms    155 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet PC without any interferences from other programs. It might help to use the '--warmup' or '--prepare' options.

Summary
  'target/release/cli test.in' ran
    2.13 ± 0.17 times faster than 'target/release/cli --x1 test.in'
    2.35 ± 0.34 times faster than 'target/release/cli --x2 test.in'
</code></pre>
<p>In my messy use of <code>hyperfine</code> above, it recommends it notes that it detected
outliers and I might consider running this on a quieter system, which is a good
suggestion and one that shouldn't simply be ignored, especially if the changes
you are performing are producing rather minimal gains or reductions to
performance.</p>
<p>You don't need to rig up an explicit benchmark harness program for the purposes
of this, either. I've had luck using on-hand binaries from previous builds and
newer binaries simply renamed or at different locations on a filesystem to
compare relative performance. If a build from three months ago felt a lot
faster and I can easily do a build of the latest version off my main branch, I
can chuck those into <code>hyperfine</code>, too.</p>
<p>One last thing I'll recommend is that it can be handy to rig up profiling tools
in scripts to get numbers across changes. For a variety of tooling, you are
likely to get somewhat unstable numbers across runs on a target. If you want
something rock-solid across runs, you might consider chucking <code>valgrind</code> into a
script and comparing output across commands, such as the following:</p>
<pre><code>#!/bin/sh -eux

cargo build --release
COMMAND=&quot;target/release/cli test.in&quot;

valgrind --tool=cachegrind &quot;$COMMAND&quot; 2&gt;&amp;1 | rg '^=='
valgrind --tool=cachegrind &quot;$COMMAND&quot; --x1 2&gt;&amp;1 | rg '^=='
valgrind --tool=cachegrind &quot;$COMMAND&quot; --x2 2&gt;&amp;1 | rg '^=='
</code></pre>
<p>The <code>rg '^=='</code> and stream redirection will make sure we only see output from
valgrind and not our tools (unless our tools are emitting lines with two or more
equal signs). Cachegrind has a <code>I ref</code> field which stands for instruction
references recorded. valgrind runs your program in a sandbox where it can do
checking of various actions, hence numbers should not change depending on noisy
neighbors. If you want something more direct from, say, PMC (performance
monitoring counter), you could plug in <code>perf stat -ad</code>, or rig up a flamegraph
to be generated and reloaded into a browser or preview tool each time you make a
change.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Shape Systems That Are Easy To Take Apart</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/shape-systems-that-are-easy-to-take-apart.html</link>
      <guid>https://justanotherdot.com/posts/shape-systems-that-are-easy-to-take-apart.html</guid>
      <pubDate>Thu, 17 Dec 2020 07:19:20 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>In imaging, there's a notion called &quot;separation of tone&quot; or sometimes curtly
referred to as &quot;separation&quot;. Images that clearly distinguish objects from one
another possess this quality. Achieving clear tonal separation enables the
author of an image to put more effort into composition of the contained objects.
Separation and composition are also ways to think about system design.</p>
<p>Systems are, usually, different to images in on chief regard; a collection of
shapes remains a collection of shapes until the shapes start to interrelate, at
which stage the collection becomes a system. Unfortunately, when software
engineers go off to implement the various shapes of a system, there is an absurd
focus on the <em>volume</em> of the shapes and little effort put into the <em>surface
area</em>. The volume may be the code, whereas the surface area is the respective
interface(s) to the code in question.</p>
<p>Focusing on surface area gives you the ability to,</p>
<ul>
<li>couple shapes together without deforming them</li>
<li>reason about a system and its properties</li>
<li>version shapes and the system that contains them</li>
</ul>
<h2>Coupling Is A Fundamental Act Of Programming</h2>
<p>Despite the 'best practice' advice to keep elements of a system loosely coupled,
the reality is that most of programming is identifying/building components and
connecting them together. You've likely used the term &quot;glue&quot; or &quot;plumbing&quot; for
this reason. When surface areas are large and poorly defined, shapes subject to
coupling turn into giant globs that may be intractable to pull apart. Squishing
together pieces of clay may or may not retain the original lines of separation,
for example. In stark contrast, clearly defined shapes only touch at designated
places, and possess the important ability to <em>decouple</em> from one another.</p>
<p>By being able to couple and decouple shapes, we can perform experiments or
optimize segments of a system, change around whole topologies for the purposes
of resilience or to expose the system as a shape with its own surface area, and
so forth. An often overlooked capacity of having clear interfaces is the ability
to constantly ship functionality to production, but in the shadows, only to be
plumbed together when the time calls for it. If the interface is clear, we can
also choose to route traffic or subject the shape to feature flagging logic to
ensure particular backing logic is slowly integrated as an engineer gains
confidence in the implementation.</p>
<h2>Reasoning Tames Complexity</h2>
<p>Systems ought to embody properties rather than being piecemeal creations. This
is because we can always judge the state of the system against the properties
and whether or not something is upholding the current properties (a target) or
if it <em>was</em> upholding a particular property. Healthy teams assume that prior
actions were done <em>on purpose</em>. This mindset avoids the narrative that &quot;everyone
else is constantly making mistakes&quot; as well as continually driving an attitude
that properties guide actions.</p>
<p>Properties can go by many names. The classic nomenclature for interfaces are
usually described by the terminology of preconditions, postconditions, and
invariants. Pre- and postconditions tend to be taken from the notion of
contractual obligations, and invariants are about properties upheld at all
points of execution. All of these are basically <strong>guarantees and expectations</strong>,
which are fundamentally part of the design of a shapes surface area. <a href="https://www.hyrumslaw.com/">Even
unintended but semi- or fully-observable properties of an interface can become
part of the expectations</a>.</p>
<h2>Versioning Supports Stability</h2>
<p>Any mutable entity in constant flux is difficult to couple against as the
constant change places work on the dependent to adapt. To combat this, software
engineering has a long history of versioning. Looking at a system as an
immutable stream of points with keys (versions) means dependents can choose
which versions to couple against. Semantic versioning, changelogs, etc. intend
to express the degree of potential effort needed to couple against a particular
version. Clearly defined shapes in a system are subject to this form of snapshot
control.</p>
<p>We tend to think solely of going forward to avoid backwards incompatible
changes, but versioning that tries hard to be both backwards and forwards
compatible supports a form of time traveling for dependents, albeit potentially
only in ranges if there are breaking changes outside of the range. By pinning
compatible pairs of surface areas we reduce the amount of effort spent
constantly upgrading and may reallocate that same effort onto other more
important tasks.</p>
<h2>But That's Not My Remit</h2>
<p>There may be someone with a title that specifically entails designing systems,
but in reality they are not the ones with the final say. Each commit of code
into production reflects a final decision about the form of the system and it's
constituent shapes as a whole. <strong>Everything is a system</strong>, from frontends,
backends, tooling, distributed systems, libraries, et. al. Systems are amoebic
in how the authors of a system go about making changes to it, but they need not
be out of control in terms of their growth. <strong>The interface to backing logic is
as much part of the implementation as the logic itself</strong>.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How do you cast generic values you're sure are numbers?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/how-do-you-cast-generic-values-youre-sure-are-numbers.html</link>
      <guid>https://justanotherdot.com/posts/how-do-you-cast-generic-values-youre-sure-are-numbers.html</guid>
      <pubDate>Tue, 20 Oct 2020 16:29:46 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Imagine you have a generic collection that holds values, but the struct
mentioned in the generic is not the actual values of the holder <em>(astute
observers will realize this is a bit like <code>ArrowPrimitiveType</code> in <code>arrow</code>).</em>:</p>
<pre><code>trait NumberLike {
  type Native;
}

impl NumberLike for Int64 {
  type Native = i64;
}

impl NumberLike for Float64 {
  type Native = f64;
}

#[derive(Debug, Clone, PartialEq)]
struct NumberLikeArray&lt;A: NumberLike&gt;(Vec&lt;A::Native&gt;);
</code></pre>
<p>Notice how the generic on our struct is different from the value stored, as specified by the trait.
You might think you can use this associated type to do primitive casts, like this:</p>
<pre><code>fn cast_vec&lt;A, B&gt;(xs: NumberLikeArray&lt;A&gt;) -&gt; NumberLikeArray&lt;B&gt;
where
  A: NumberLike,
  B: NumberLike,
{
  NumberLikeArray(xs.0.into_iter().map(|x| x as B::Native).collect())
}
</code></pre>
<p>But this won't work, the compiler tells us:</p>
<pre><code>error[E0605]: non-primitive cast: `&lt;A as NumberLike&gt;::Native` as `&lt;B as NumberLike&gt;::Native`
  --&gt; src/lib.rs:24:44
   |
24 |   NumberLikeArray(xs.0.into_iter().map(|x| x as B::Native).collect())
   |                                            ^^^^^^^^^^^^^^ an `as` expression can only be used to convert between primitive types or to coerce to a specific trait object
</code></pre>
<p>This error helpfully informs us that <code>as</code> only works on primitive types or
specific trait objects on the right hand side of the keyword. With generics we
reduce the amount of duplicated code we need to write, but we also lose less
information about the values themselves. To gain back some information about
the types, we can put bounds on the generic values which will tell us about
whether or not there are associated types available to us
or functions we can run on said types.</p>
<p>The associated type for <code>NumberLike</code> could be anything, hence Rust won't let us
compile this code because it can't be 100% sure that <code>x</code> is a primitive value
or a specific trait object, nor can it confirm that <code>B::Native</code> is a primitive
type. Rust isn't going to try to figure out that information from all the trait
bounds it knows about during compilation, either. What we need is something
that can allow us to convert values but expressed as a trait; what about
<code>From</code>?</p>
<pre><code>fn cast&lt;A, B&gt;(xs: NumberLikeArray&lt;A&gt;) -&gt; NumberLikeArray&lt;B&gt;
where
  A: NumberLike,
  B: NumberLike,
  B::Native: From&lt;A::Native&gt;,
{
  NumberLikeArray(xs.0.into_iter().map(|x| x.into()).collect())
}
</code></pre>
<p>but we're going to hit another wall with this the moment we try to use it;</p>
<pre><code>fn main() {
    let xs: NumberLikeArray&lt;Int64&gt; = NumberLikeArray(vec![12, 13, 14]);
    let ys: NumberLikeArray&lt;Float64&gt; = cast(xs);
    assert_eq!(ys.0, vec![12.0, 13.0, 14.0]);
}
</code></pre>
<p>there is no implementation for <code>From&lt;i64&gt; for f64</code> or the other way around!</p>
<pre><code>error[E0277]: the trait bound `f64: std::convert::From&lt;i64&gt;` is not satisfied
  --&gt; src/main.rs:30:40
   |
19 | fn cast&lt;A, B&gt;(xs: NumberLikeArray&lt;A&gt;) -&gt; NumberLikeArray&lt;B&gt;
   |    - required by a bound in this
...
23 |   B::Native: From&lt;A::Native&gt;,
   |               required by this bound in `cast`
...
30 |     let ys: NumberLikeArray&lt;Float64&gt; = cast(xs);
   |                                        ^^^^ the trait `std::convert::From&lt;i64&gt;` is not implemented for `f64`
   |
   = help: the following implementations were found:
             &lt;f64 as std::convert::From&lt;f32&gt;&gt;
             &lt;f64 as std::convert::From&lt;i16&gt;&gt;
             &lt;f64 as std::convert::From&lt;i32&gt;&gt;
             &lt;f64 as std::convert::From&lt;i8&gt;&gt;
           and 3 others
</code></pre>
<p>This makes sense as <code>From</code> is really intended for cheap, infallible conversions
between types, and <code>as</code> is quite a blunt tool. <code>as</code> can perform truncations and
other changes to the data depending on the arguments. Converting from an <code>i64</code>
to an <code>f64</code> may seem odd, but we may want the behavior that <code>as</code> supports in
our program. You might reach to <code>impl From for f64</code> and <code>i64</code> respectively to
make this happen:</p>
<pre><code>impl From&lt;i64&gt; for f64 {
    fn from(x: i64) -&gt; f64 {
        x as f64
    }
}

impl From&lt;i64&gt; for f64 {
    fn from(x: i64) -&gt; f64 {
        x as i64
    }
}
</code></pre>
<p>but alas another wall:</p>
<pre><code>error[E0117]: only traits defined in the current crate can be implemented for arbitrary types
  --&gt; src/main.rs:16:1
   |
16 | impl From&lt;i64&gt; for f64 {
   | ^^^^^^^^^^
   | |    |             |
   | |    |             `f64` is not defined in the current crate
   | |    `i64` is not defined in the current crate
   | impl doesn't use only types from inside the current crate
   |
   = note: define and implement a trait or new type instead

error[E0117]: only traits defined in the current crate can be implemented for arbitrary types
  --&gt; src/main.rs:22:1
   |
22 | impl From&lt;f64&gt; for i64 {
   | ^^^^^^^^^^
   | |    |             |
   | |    |             `i64` is not defined in the current crate
   | |    `f64` is not defined in the current crate
   | impl doesn't use only types from inside the current crate
   |
   = note: define and implement a trait or new type instead
</code></pre>
<p>Let's follow the advice the compiler has given us:</p>
<pre><code>trait Cast&lt;A&gt; {
    fn cast(self) -&gt; A;
}

impl Cast&lt;i64&gt; for f64 {
    fn cast(self) -&gt; i64 {
        self as i64
    }
}

impl Cast&lt;f64&gt; for i64 {
    fn cast(self) -&gt; f64 {
        self as f64
    }
}
</code></pre>
<p>and we'll update our <code>cast</code> function:</p>
<pre><code>fn cast&lt;A, B&gt;(xs: NumberArray&lt;A&gt;) -&gt; NumberArray&lt;B&gt;
where
    A: NumberLike,
    B: NumberLike,
    A::Native: Cast&lt;B::Native&gt;,
{
  NumberArray(xs.0.into_iter().map(|x| x.cast()).collect())
}
</code></pre>
<p>We could have easily swapped the argument order of <code>Cast::cast</code> just like
<code>From</code> and <code>Into</code>'s symmetry. The choice felt arbitrary here and I've picked to
have things feel like they say &quot;A::Native supports casts into B::Native's&quot; and
simply replace the <code>into</code> call we had earlier with <code>cast</code>. If we wanted to push
this further, we could also describe <code>NumberLikeArray</code> with a <code>Cast::cast</code>
implementation:</p>
<pre><code>fn cast&lt;A, B&gt;(xs: NumberLikeArray&lt;A&gt;) -&gt; NumberLikeArray&lt;B&gt;
where
  A: NumberLike,
  B: NumberLike,
  A::Native: Cast&lt;B::Native&gt;,
{
  NumberLikeArray(xs.0.into_iter().map(|x| x.cast()).collect())
}

impl&lt;A, B&gt; Cast&lt;NumberLikeArray&lt;B&gt;&gt; for NumberLikeArray&lt;A&gt;
where
  A: NumberLike,
  B: NumberLike,
  A::Native: Cast&lt;B::Native&gt;,
{
    fn cast(self) -&gt; NumberLikeArray&lt;B&gt; {
        cast(self)
    }
}
</code></pre>
<p>This shows a way of writing method implementations I sometimes like to do. When
you define <code>Cast::cast</code> you get two functions for free: <code>Cast::cast(array)</code> and
<code>array.cast()</code>. I also like to have the option of doing a qualified import via
a module, s.t. someone could do <code>crate::convert::cast(array)</code> if they so
desired. With the above approach, you get all three. Here's the <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=514a19b873cf4226f3ac7315550657ba">full
listing</a>
to explore on your own.</p>
<p>The good news is, you don't have to do this 'from scratch' every time you start
a project, and you don't need to spin up your own crate because the <code>num</code> crate
already has a <code>NumCast</code> and <code>ToPrimitive</code> pair of buddy traits that do this.
The one major difference is that <code>NumCast::from</code> produces an <code>Option</code> that
we'll need to handle. We could flatmap this, causing items to go missing during
the cast, or we could make it error on first cast, like this:</p>
<pre><code>fn cast&lt;A, B&gt;(xs: NumberLikeArray&lt;A&gt;) -&gt; Option&lt;NumberLikeArray&lt;B&gt;&gt;
where
  A: NumberLike,
  B: NumberLike,
  A::Native: NumCast,
  B::Native: ToPrimitive + NumCast,
{
  Some(NumberLikeArray(xs.0.into_iter().map(|x| NumCast::from(x)).collect::&lt;Option&lt;Vec&lt;B::Native&gt;&gt;&gt;()?))
}

fn main() {
    let xs: NumberLikeArray&lt;Int64&gt; = NumberLikeArray(vec![12, 13, 14]);
    let ys: NumberLikeArray&lt;Float64&gt; = cast(xs).expect(&quot;could not cast array&quot;);
    assert_eq!(ys.0, vec![12.0, 13.0, 14.0]);
}
</code></pre>
<p>Using bounds to specify requirements on generics is a way of gaining back
information about the types we abstract over. Instead of working over all A's
and B's, we're specifically working over A's and B's that implement certain
charecteristics, and we can leverage those characteristics to transform values,
perform effects, or simply declare that some type has been marked or tagged, as
is the case with <code>Eq</code> and others.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Think Spatially to Grok Lifetimes</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/think-spatially-to-grok-lifetimes.html</link>
      <guid>https://justanotherdot.com/posts/think-spatially-to-grok-lifetimes.html</guid>
      <pubDate>Sat, 19 Sep 2020 14:32:14 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Have you ever felt like you could use some sort of lifetime visualizer or
debugger for tricky borrow checker issues in Rust? Has building up a mental model
for complex lifetime interactions and their rules felt like one of the hardest
things about writing Rust? I am by no means impervious to particularly tricky
borrow checking woes, but I have built up a way of thinking about lifetimes and
borrowing that I want to share with you that has helped me reason about a wide
variety of borrow checking cases and I think is simple enough to keep in your
head.</p>
<p>programs represent a <strong>space</strong>.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-01.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>this space may contain other spaces, values, and <strong>bindings</strong>.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-02.jpg"
    alt="A diagram depicting bindings."
    title="A diagram depicting bindings.">
  </img>
</figure>
<p>a binding <em>binds</em> or <em>associates</em> a name to a value. we use values in functions, operations, in data structures, and so on, by their name or by their value directly. in order to use a value we must <strong>move</strong> it into place. when values move they are never in both places at the same time, hence <em>original</em> values in Rust can be moved exactly zero or one times. this requirement allows us to track where values are used in a program.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-03.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>we could duplicate a value, but such a <strong>clone</strong> would not increase the number of times we can use the value, it simply has created a new, distinct value that can move around independent of the other. Some families of values automatically clone when they move, but this isn’t the norm. if we truly want to <strong>reuse</strong> an original value, we can create a <strong>reference</strong> to it. values do not live outside of the space where they are defined, unless they are moved to another space explicitly for use.</p>
<p>When a value is left behind in a space and we can no longer reach it, we say that it has been <strong>dropped</strong>.   in Rust, we can only take references to values that have not been dropped. in other words, if the value resides in a space that <em>contains</em> or <em>is equal</em> to the space where a reference will live, then we can create the reference. a similar analogy might be talking to someone in a house; you can only talk to the person if they still exist and are living in the same space or some attached space where they can hear or see you. This is in contrast to languages where a references is entirely valid if it points to something that no longer exists.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-04.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>if the intent is to <em>change</em> a value through a reference, you can only have one reference at a time, but if the intent is purely to <em>view</em> a referent, the value being referenced, then you can have as many references as you like. we call these <strong>mutable</strong> and <strong>immutable</strong> references respectively. Containment isn’t always required for references, though; sometimes the use of a reference can be elastic, meaning that the compiler understands the use is only for a given period of time and no longer. This elasticity comes from a property of references you may hear called <strong>non-lexical lifetimes</strong>, AKA <strong>NLL</strong>, meaning the space of the reference isn't tied to the space where the binding happened.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-11.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>the most obvious definition of space is with blocks using curly brackets, but functions, data types, and loops all define space. functions hide away space like a fold on a piece of paper. loops define space by compressing several spaces into what looks like the space of one. closures and data structures are portable spaces that <em>capture</em> or <em>close over</em> values and references to values. As we saw before, each space tends to also provide ways for values to be bound to names.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-05.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>the space of the entire program is not the same space of the <code>main</code> function. we call the space of the entire program a specific label, <code>'static</code>. other threads may run to completion well <em>after</em> the main thread has finished, for example, meaning we can not spin up threads that take references to original values in another thread. again, the static label is special in that it is <em>not</em> a placeholder but rather the name of a specific space.</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-06.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>otherwise, these labels are placeholders which are <em>generic</em> names of spaces that will get filled in based on the context of where they are mentioned. because spaces are like spans describing where a value lives, along with some sense of time, as the program executes, we call these labels <strong>lifetimes</strong>. we say that original values are <strong>owned</strong> as the name owns the value it was bound to. since values have owners, we say that a reuse of a value through a reference is a <strong>borrow</strong> since the value will be given back.</p>
<p>Rust will assume that all references in function arguments point to the same space. this is known as <strong>elision</strong> as the labels of the spaces do not need mentioning and are removed (elided). however, sometimes you want to make it clear that each reference points to different spaces. we can declare these differences using explicit lifetime labels on the type or function in question that we tend to call <strong>annotations</strong>.</p>
<p><strong>when you run into a borrowing or a lifetime issue it can help to think <em>spatially</em>.</strong> consider how the spaces look unpacked across your code. this can be tricky, but you can practice by occasionally inlining code directly from a function or unrolling a loop to help visualize the spaces in question. as with this article, drawing out the spaces can help abstract away a lot of the other noise that code may supply.  Here are some examples of spaces inclined into larger spaces:</p>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-07.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-08.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-09.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<figure>
  <img
    src="/assets/images/think-spatially-to-grok-lifetimes-10.jpg"
    alt="A diagram depicting a program."
    title="A diagram depicting a program.">
  </img>
</figure>
<p>lastly, you should generally make lots of small programs to learn the ins and outs of fundamentals, and making minimally reproducible cases of borrows and moves is no different. Running <code>cargo new</code> is cheap, and so is going to the Rust playground when you want to scratch an itch about a question you have.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Let The Caller Decide The Return Value!</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/let-the-caller-decide-the-return-value.html</link>
      <guid>https://justanotherdot.com/posts/let-the-caller-decide-the-return-value.html</guid>
      <pubDate>Mon, 17 Aug 2020 08:46:39 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Changing function interfaces to get different return values is a chore and can weigh down refactoring sessions. There is a simpler way to &quot;slot in&quot; different functionality based on types and traits!</p>
<p>If you've used Rust long enough you are likely aware of the pattern with iterators where you can <code>collect</code> into different collections based on types. Iterators themselves don't have to keep around all of these definitions: they are implemented <a href="https://doc.rust-lang.org/std/iter/trait.FromIterator.html">via a trait named <code>FromIterator</code></a>. This trait uses a trick often returned to as &quot;return type polymorphism&quot; to accomplish its task; the trait is generic and only requires one method, <code>from_iter</code>, to be implemented. In fact, <code>collect</code> is just a <a href="https://doc.rust-lang.org/src/core/iter/traits/iterator.rs.html#1664-1672">thin wrapper around this function</a>!</p>
<pre><code class="language-rust">fn collect&lt;B: FromIterator&lt;Self::Item&gt;&gt;(self) -&gt; B
    where Self: Sized,
{
    FromIterator::from_iter(self)
}
</code></pre>
<p>What's important about this pattern of using <code>collect</code> is that it is always the same semantics; you are collecting values of an iterator into a collection. You could theoretically abuse this approach to come up with a way to do &quot;dynamic&quot; dispatch to wildly different behavior by specifying different types, and that <em>might</em> work depending on your use case, but the approach would feel a tad unidiomatic in the light of the larger Rust code ecosystem.</p>
<p>Here's a rough example using a trait to allow different types of files to be opened depending on the needs of the caller:</p>
<pre><code class="language-rust">use std::{
    fs::File,
    io::{BufReader, Cursor, Result},
    path::Path,
};

trait Open&lt;T&gt; {
    fn open(&amp;self) -&gt; Result&lt;T&gt;;
}

impl Open&lt;File&gt; for Path {
    fn open(&amp;self) -&gt; Result&lt;File&gt; {
        File::open(self)
    }
}

impl Open&lt;BufReader&lt;File&gt;&gt; for Path {
    fn open(&amp;self) -&gt; Result&lt;BufReader&lt;File&gt;&gt; {
        Ok(BufReader::new(File::open(self)?))
    }
}

impl Open&lt;Cursor&lt;File&gt;&gt; for Path {
    fn open(&amp;self) -&gt; Result&lt;Cursor&lt;File&gt;&gt; {
        Ok(Cursor::new(File::open(self)?))
    }
}

fn main() {
    let p = Path::new(&quot;foo&quot;);
    File::create(p).unwrap();
    dbg!(
        Open::&lt;File&gt;::open(p).unwrap(),
        Open::&lt;BufReader&lt;File&gt;&gt;::open(p).unwrap(),
        Open::&lt;Cursor&lt;File&gt;&gt;::open(p).unwrap(),
    );
    let _: File = dbg!(p.open().unwrap());
    let _: BufReader&lt;File&gt; = dbg!(p.open().unwrap());
    let _: Cursor&lt;File&gt; = dbg!(p.open().unwrap());
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=71b87a2671cc5723b5b164842ff61e85">playground</a>.</p>
<p>The beauty of this approach is that context can dictate which function will run. Note that I've included two approaches to showing the same thing; one using the trait's associated function syntax and the other as part of the type annotation on a <code>let</code>. I could have just as easily wrapped this function and specified concrete types on the function signature to get the same result.</p>
<p>In this example users of the trait can decide if the file ought to be returned &quot;raw&quot;, wrapped for buffered access, or put in a cursor for seeking around the file's contents. This works because we have defined the trait generically, and therefore are really defining implementations for several different traits that all have the same minimal requirements. In this specific case we cannot write a generic implementation for <code>Open</code> for all <code>T</code> because there's no way for us to write a function that could return all possible <code>T</code>. That said, this trick still works even if you are not specifying the return type as part of the function calls, so long as you specify which trait implementation you want to select.</p>
<p>In order to use return type polymorphism you need:</p>
<ul>
<li>A generic trait, usually with a function or functions that use the generic type in the return value</li>
<li>Implementations of the concrete versions of the return type</li>
<li>Usually some final type annotation or type inference that will trigger the right implementation to be picked depending on context</li>
</ul>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>An Architecture Agnostic Intro To Assembly</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/an-architecture-agnostic-intro-to-assembly.html</link>
      <guid>https://justanotherdot.com/posts/an-architecture-agnostic-intro-to-assembly.html</guid>
      <pubDate>Fri, 14 Aug 2020 16:20:36 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Assembly is the last &quot;human readable&quot; frontier before reaching the representation of a program that a machine consumes. Times have certainly changed; the PDP-11 had only <em>16 instructions</em> in total, whereas the <a href="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf">Intel x86_64 and x86 manual is well over 2000 pages</a>. You may not ever need to write assembly yourself, but if you ever want to understand what comes out of a compiler, you'll need to know how assembly works. Due to the staggering complexity of modern processors, learning assembly for different architectures could take forever to grok in their entirety. Alas, there's no need to know every nuance of each platform's assembly language; there's enough generalities between Instruction Set Architectures, or ISAs, to lead you to more research on a case-by-case basis.</p>
<p>No matter the ISA, the instructions they offer tend to fall into three primary groups.</p>
<p>Some terminology first:</p>
<ul>
<li>A <strong>register</strong> is a named place in memory that is fast to access as it is normally right next to the CPU. That is to say data doesn't have to travel across a bus to read or write from the location in memory.</li>
<li>A <strong>word</strong> is the <em>most common</em> width of a CPU. If a CPU works usually 32-bits, then a word is 32-bits. This isn't to say some CPUs that are, say, 64-bits wide can't work in 128 bits or 16-bits.
<ul>
<li>Note: contrast this by the odd Intel convention that a word is sometimes 16-bits. This means there are double-words (32-bits) and quad-words (64-bits) which you'll see in some instruction manuals. This comes from the quirk that Intel stuck to the 8086 convention since they introduced 32-bit mode with the i386. As such, you can actually run an x86 or x86_64 machine in 16-bit mode, also known as &quot;real&quot; mode.</li>
</ul>
</li>
<li>A <strong>byte</strong> is <em>almost always</em> 8 bits. There are some earlier machines that did not have this but you don't have to think about them anymore for the most part unless you are being excruciatingly specific to porting to all sorts of machines.This is now an IEC standard: <a href="https://en.wikipedia.org/wiki/ISO/IEC_80000">IEC 80000-13:2008</a></li>
<li>A <strong>nibble</strong> is half a byte.</li>
</ul>
<p>Some terminology such as &quot;sentence&quot; and &quot;paragraph&quot; are no longer in common use and so I won't bother with them here. Also, if you have no prior understanding of how a CPU works, I strongly recommend you go read my article on <a href="https://www.justanotherdot.com/posts/what-makes-up-a-cpu.html">what's in a CPU</a>, first. Although I will be explaining this with x86 and x86-64 examples, I want to make it clear that the classes of instructions are the more important part here rather than the specific names of instructions or how how they precisely work under the hood.</p>
<p>No matter what, instructions are always short mnemonic words. They take zero or more arguments or &quot;operands&quot;. Some assembly languages support putting direct values or constants into memory by specifying the &quot;section&quot; and writing out specific values. The &quot;text&quot; value is historically the section in a binary where code lives, whereas other sections designate types of memory to be included in the final binary. Different syntaxes, such as &quot;intel&quot; and &quot;att&quot; (short for AT&amp;T), designate effectively the same content but represented with different symbols and words. When in doubt or encountering an instruction for the first time, check the manual or search around for further details. There are normally sites that include good per-instruction explanations as well as other assembly courses for specific architectures. The point of this article being architecture agnostic is that many assembly languages have plenty of things in common and it's best to grok those top-level ideas first before diving into internals. Hopefully these segments help guide you in your deeper explorations.</p>
<p>Pretty much the only supported &quot;data structure&quot; in assembly is that of a stack. This model forms the basis of functions at the assembly level. I mention them here because they're a bit of a combination of several of the classes we're about to talk about. When you want to call a function, you push a number of things onto the stack, such as the return address where you were at before you called the function, local data for the function, and so on. The format for how things should be put on the stack for a function call is called the <em>calling convention</em>. Functions are a unit of computation that drastically improve code legibility and reuse and, well, I really shouldn't need to explain why functions are awesome.</p>
<h3>Reading and Writing Data To Memory</h3>
<p>There are usually instructions to read and write to memory. Data has to move around in a system for it to be changed into anything meaningful. Reading is sometimes known as a load and writing is sometimes known as a store. Depending on the architecture this may include ways to indirectly access memory: you specify an address given some syntax and the value that is at that address is used. This is the assembly version of a pointer.</p>
<p>Memory tends to fall into registers and memory at particular addresses, which could be main memory or could be hardware. Hence writing and reading some address in memory might be communicating to a device, or it may simply be storing or querying a value in memory. When we communicate to memory in this way we call it memory mapped IO.</p>
<p>Some architectures require all memory to be put into registers before before manipulating it or writing it back out. This style is known as &quot;load-store&quot; and is what RISC-V and ARM requires, for example. Intel and AMD, however, allow you to manipulate memory directly, which is known as a &quot;register-memory&quot; architecture. We'll cover what I mean by &quot;manipulate&quot; here in the next section.</p>
<p>Common instruction names for this are called <code>mov</code> for &quot;move&quot;, <code>lea</code> for &quot;load effective address&quot;, instructions that manipulate a &quot;stack&quot; such as <code>push</code> and <code>pop</code>, and others. As you read through assembly or instruction manuals, these will probably be the first or most common instructions you find.</p>
<h3>Transforming Data In Memory</h3>
<p>There is usually some form of an ALU or Arithmetic Logic Unit. These were the &quot;manipulations&quot; I mentioned in the last section. As the unit name implies, there are both instructions that perform arithmetic as well as logic. Boolean operations, addition and subtraction, and bit manipulations such as shifting left or right, all fall into this camp. A lot of these instructions are the same across platforms as they form fundamentals, but some architectures provide fancier manipulations such as <code>popcnt</code> which will count all the ones in a binary operand. Particular platforms such as the now ancient VAX had instructions to do polynomial evaluation with the name <code>POLY</code>. If data is being updated, it falls into this category.</p>
<p>On most platforms when you perform a transformation on a value or set of values, it will generally update one of the registers provided. As noted in the last section, if you are on a &quot;register-memory&quot; architecture, some transformation operations can change data directly in memory, without having to be loaded to registers for manipulation.</p>
<h3>Branching and Conditions</h3>
<p>We can move data around, transform it, but we can't be Turing complete without some kind of way to express conditions and branching. Branching is sometimes known as &quot;jumping&quot; as the code literally loads the value given in the jump or branch argument into the program counter and then executes the instruction found at that address.</p>
<p>Usually with jumps a comparison instruction is run, setting some state on the processor known as a &quot;flag&quot; or set of &quot;flags&quot;, and then the following instruction will depend on which flags got set during that instruction. Some platforms, such as ARM, allow for conditional transformations, such as &quot;add these two values together if the comparison was equal&quot;.</p>
<h3>Instructions That Work On Multiple Data</h3>
<p>Now that we've seen most instructions, there are sets of instructions that can work on multiple sets of data simultaneously. This is hardware-level parallelism. There are different names for this. On x86 it's called &quot;SIMD&quot;. Doing things in batches will always be faster than doing it one-word at a time, but it can be clunky to manipulate code in this manner as you generally must load up specific registers to perform these operations. These special registers are called vectors as they contain multiples of the same unit, hence these instructions are often called <em>vectorized</em> instructions.</p>
<p>Compilers are usually smart cookies. They can figure out how to generate vectorized code themselves when there are chunks of data that can be transferred. This is often called &quot;autovectorized&quot; and if you can learn the look of vectorized instructions for the architecture you are working on, you can easily tell if the compiler is generating the optimized code or not. In fact, this is the primary goal that most people exploring assembly use this knowledge for. If you know what the assembly is doing, then you can tell if the compiler is making good or bad choices and whether or not you have to step in to help nudge it in the right direction or be explicit.</p>
<h3>Quirky Instructions</h3>
<p>I don't group these with transformational instructions as they may do special hardware specific things and possibly not change data at all. Some are about coordinating synchronization when updating memory. Examples of this on CISC, or Complicated Instruction Set Computer, architectures that can do things such as atomic instructions (e.g. <code>cas</code> for Compare And Swap), memory fences such as <code>mfence</code>, and so on. The most common &quot;quirky&quot; instruction that is on all platforms is the noop instruction, which stands for &quot;no operation&quot;. This is useful for progressing the program counter without changing anything. A noop can be useful for waiting for a lock to be returned, for example, in what is commonly known as a &quot;spinlock&quot; on multi-core architectures.</p>
<h2>Where to next?</h2>
<p>You know the basic classes of instructions, now it's time to start dumping out assembly and sifting through it! It will seem like nonsense at first and you will need to explore specific terminology and conventions, but following small examples first can give you a sense of direction. If you need a guide on how to dump assembly for your Rust programs or Rust examples, <a href="https://www.justanotherdot.com/posts/magnifying-glasses-for-rust-assembly.html">you can read my article on the subject</a>. Like any kind of programming it takes time and practice, but with experience you'll slowly get comfortable looking at and deciphering assembly. Picking apart assembly is yet another part of your performance toolkit. For me, when I encounter something I don't understand, I try to put it into a classification above to better know what to expect the instruction to do. As I mentioned, I would recommend generating small examples and watching the output. When you feel comfortable that you understand what is going on, try another small example. When you've got a good understanding of small examples ,try something bigger. Usually when I look at assembly for a program, I am looking one function at a time. It can still be daunting if lots of other functions have been inlined into the code, but you can always break pieces of logic up, granted the generated assembly code doesn't change, if it helps you better understand.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Avoid Build Cache Bloat By Sweeping Away Artifacts</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/avoid-build-cache-bloat-by-sweeping-away-artifacts.html</link>
      <guid>https://justanotherdot.com/posts/avoid-build-cache-bloat-by-sweeping-away-artifacts.html</guid>
      <pubDate>Fri, 14 Aug 2020 11:08:21 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Incremental builds can get huge in Rust and there is no builtin way for them to get pruned over time. This can be particularly important for developers or build bots working with a limited sized disk drive. I have seen some build caches on projects I work on in production into the range of several gigabytes and this is the size <em>after</em> compression!</p>
<p>The easiest way to keep these projects clean is to <a href="https://github.com/holmgr/cargo-sweep">instrument <code>cargo-sweep</code> to run regularly</a>. You can dump the command into a crontab to run at regular periods. For build bots it can pay to pull down a shared cache or caches and clean them at regular intervals, either as part of preexisting steps or as dedicated steps.</p>
<p>We can tell the crontab to run a script and stuff our logic in the script; this way we don't have to change the crontab if we want to change the behavior. Our script will run <code>cargo sweep</code> recursively across all our projects, pruning objects older than thirty days.</p>
<pre><code class="language-bash">#/bin/sh -eux
if ! cargo sweep &gt;/dev/null 2&gt;&amp;1 ; then
  cargo install cargo-sweep
fi
export PATH=&quot;$PATH:$HOME/.cargo/bin&quot;
cd &quot;$HOME&quot;
cargo sweep -r -t 30
</code></pre>
<p>You can write to a crontab with a one-liner. This  will make our script run every hour every day. You can adjust the period based on the expression given. If you are unfamiliar with crontab expressions, you can play with crontab expressions to produce human readable output, <a href="https://crontab.guru/">such as this site</a>.</p>
<pre><code class="language-bash"># Assuming you've named the above script `clean-build-artifacts`
# and have put it under /usr/local/bin/
echo '0 */1 * * * /usr/local/bin/clean-build-artifacts' | sudo crontab -u $(whomami) -
</code></pre>
<p>we could replicate the same behavior that <code>cargo sweep</code> does with a shell script, but we would need to replicate a lot of the behavior that it offers. By default <code>cargo clean</code> is a sledgehammer that will remove <em>everything.</em> This may, in fact, be what you want; perhaps every thirty days you want to build everything fresh again. This can be particularly handy if you are using scheduled CI builds to check for broken windows across your pipelines and you want the sanity that comes with a fresh build.</p>
<p>Which leads me into another use for <code>cargo sweep</code> that isn't time based: sweeping toolchains. You could additionally add a cronjob to sweep toolchains every month or so, like this:</p>
<pre><code class="language-bash">#/bin/sh -eux
if ! cargo sweep &gt;/dev/null 2&gt;&amp;1 ; then
  cargo install cargo-sweep
fi
export PATH=&quot;$PATH:$HOME/.cargo/bin&quot;
cd &quot;$HOME&quot;
for project in $(find . -name &quot;Cargo.toml&quot; -type f | grep -vE &quot;(registry|.rustup|.cargo|target)&quot; | xargs dirname); do
  cd &quot;$project&quot;
  current_toolchain=$(rustup toolchain list | grep &quot;override&quot; | awk '{print $1}')
  if [ &quot;$current_toolchain&quot; ]; then
    cargo sweep --toolchains=&quot;$current_toolchain&quot;
  else
    rustup toolchain list | grep &quot;default&quot; | awk '{print $1}' | xargs -I{} cargo sweep --toolchains=&quot;{}&quot;
  fi
done
</code></pre>
<p>What does this script do? We have the same preamble as before that installs <code>cargo-sweep</code> if it's not present on the system and sets the correct <code>PATH</code> environment variable, just in case. We change into our <code>HOME</code> directory and then look for all the projects that are only present for our projects. Then, we go into each project and try to see if there is an override for the toolchain, usually caused by the presence of a <code>rust-toolchain</code> file, and if there is none, we will clean the default toolchain, instead. When this script is done, all the projects will be cleaned of build artifacts created by unsupported or non-default compilers. Running this on a project of mine of mine that includes a <code>rust-toolchain</code> gives me:</p>
<pre><code class="language-bash">&lt;snip&gt;
[INFO] Cleaned 761.8599634170532 MiB
</code></pre>
<p>which is a pretty hefty savings all things considered! I can name the above script <code>clean-build-artifacts-toolchains</code> and install the script to run monthly by running:</p>
<pre><code class="language-bash">echo '0 0 1 * * /usr/local/bin/clean-build-artifacts-toolchains' | sudo crontab -u $(whomami) -
</code></pre>
<p>This script will run on the first of every month. The best part of dumping these sorts of things into scripts is that they can easily be reused by name, rather than lots of copy-pasting. In particular, we can now take these scripts and have a job run it over our build caches like we mentioned before as a step in a pipeline or even as a dedicated monthly schedule build that will clean out old cruft that we are unlikely to need again. Reducing the size of your build caches means the total time taken to pull down the cache and unpack it is less, speeding up overall build times.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Profiling Doesn't Always Have To Be Fancy</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/profiling-doesnt-always-have-to-be-fancy.html</link>
      <guid>https://justanotherdot.com/posts/profiling-doesnt-always-have-to-be-fancy.html</guid>
      <pubDate>Fri, 14 Aug 2020 09:30:07 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Not all profiling experiences are alike. Some are filled with friction around the tooling. Others are around doubt about whether or not intermediate layers are inflating or shifting numbers in unfair ways. Perhaps you work in a security or data-compliance critical environment and all you want is numbers on what is running in production without having to breach agreements by downloading live data to your work environment. Benchmarks are fantastic for tracking numbers of common or pathological cases over time, but they may still be unrealistic in comparison to the undiscovered cases in prod. When I find that I can't easily wedge in a profile, I get a bit sad and then turn to crude solutions.</p>
<p>Admittedly, performance counters with tools like <code>perf</code> can far better track the performance of resource usage rather than wall times. A wall time may include all the time taken during a context switch out to other running processes, for example, but wall times are important because they are the latency humans feel when they use a user interface to a system or tool.</p>
<p>The simplest way to crudely examine timings is to do what benchmark tooling does; record the wall time before and after the code runs. Benchmarks can track different measures of &quot;center&quot; and calculate statistics around outliers and deviations from these markers of center, but ultimately they take two times and output the difference. I want to reduce friction as much as possible, and as such I'll write this as a macro to do the code generation around the value and still return the value. <a href="https://doc.rust-lang.org/src/std/macros.rs.html#285-305">This is exactly what <code>dbg</code> does</a>.</p>
<pre><code class="language-rust">macro_rules! time {
    ($val:expr) =&gt; {
        let beg = std::time::Instant::now();
        match $val {
            tmp =&gt; {
                let end = std::time::Instant::now();
                let time = (end - beg);
                println!(
                    &quot;[{}:{}] `{}' took {:?}&quot;,
                    std::file!(),
                    std::line!(),
                    std::stringify!($val),
                    time
                );
                tmp
            }
        }
    };
}
</code></pre>
<p>We print out the timings given the file number, line number, expression under timing, and time taken, just like <code>dbg</code> does. You'll note that I am using the <code>Debug</code> format specifier for the time itself and not converting the time into a canonical format such as nanoseconds. I do this because the formatting is usually clearer this way; if we always returned milliseconds but the code under inspection took only a handful of nanoseconds, I'd have to go back and change the code again, or if the code is taking several seconds, then telling me the nanoseconds will require me to do the math in my head to convert.</p>
<p>Times like this are handy for things such as router endpoints. This same stopwatch style is what is also used for distributed tracing libraries. A library will start a clock on function entry, and on exit calculate the result, create a &quot;span&quot;, and add it to a list of spans. Normally spans are identified by some unique trace id and have parent/child identifiers allowing you to shove all the spans up to an aggregator and let them figure out how to stitch the values together, saving you the cost of doing it on the application.</p>
<p>With this macro, we can  go from this code:</p>
<pre><code class="language-rust">use std::time::Instant;

#[get(&quot;/resource&quot;)]
fn index() -&gt; Result&lt;Json&lt;Resource&gt;, HttpError&gt; {
    let beg = Instant::now();
    let rsrc = resource().map(|x| Json(x));
    let end = Instant::now();
    // or, dump as json to logger, stdout, etc. for aggregation...
    eprintln!(&quot;/resource took {} ms&quot;, (beg - end).as_millis());
    rsrc
}
</code></pre>
<p>to this code:</p>
<pre><code class="language-rust">#[get(&quot;/resource&quot;)]
fn index() -&gt; Result&lt;Json&lt;Resource&gt;, HttpError&gt; {
    time!(resource().map(|x| Json(x)))
}
</code></pre>
<p>If you wanted to you could change the macro to dump structured logs rather than free text, or you could push metrics out to a provider under a name. Regardless of where you aggregate the values for inspection, making this like <code>dbg</code> means we are being unobtrusive with our code, allowing us to put in timings and take them out when ready, which is especially handy when you are trying to check the time of a particular chunk of code while developing.</p>
<p>To be complete to the <code>dbg</code> implementation, maybe you want to pass several things separated by commas to the <code>time</code> macro in the same way you can pass multiple things to <code>dbg</code>. Taking a note from the source code of the dbg macro we can see what to add <a href="https://doc.rust-lang.org/src/std/macros.rs.html#302-304">near the end</a>:</p>
<pre><code class="language-rust">macro_rules! time {
    ($val:expr) =&gt; {
        {
            let beg = std::time::Instant::now();
            match $val {
                tmp =&gt; {
                    let end = std::time::Instant::now();
                    let time = (end - beg);
                    println!(&quot;[{}:{}] `{}' took {:?}&quot;, std::file!(), std::line!(), std::stringify!($val), time);
                    tmp
                }
            }
        }
    };
    ($($val:expr),+ $(,)?) =&gt; {
        ($(time!($val)),+,)
    };
}
</code></pre>
<p>This change uses the repeat pattern matches of macros to consistently repeat a pattern for as many times as it is mentioned. what this pattern says is &quot;there may be several (thus the + sign) comma separated expressions, followed finally by an optional (thus the ? sign) comma&quot;. then, when you use the plus sign in the body, it will repeat the same number of times as the pattern was found. The body says &quot;do the time macro on each captured value for as many times the patterns were captured and put them all in a tuple that fits them, followed by a comma just in case there are is only one value passed and this pattern fires&quot;, at least that's how I read that last comma.</p>
<p>You can push this idea further if you want by <a href="https://gist.github.com/justanotherdot/fe4bf2024d2c13e3eace4f8d6730c3d1">writing a benchmark macro</a> that did the timing across runs, perhaps including mean average and standard devation and maybe even warmups. The point is not to get lost in recreating a benchmark or profiling suite inside of macros but to find ways to unobtrusively provide results in such a way that you can quickly get relative sizes between elements in a system or program. Also, be aware that timings can blow out if you are timing the code that also has timing code in it. Ideally you time independent segments of a function.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Making Friends With Caches</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/making-friends-with-caches.html</link>
      <guid>https://justanotherdot.com/posts/making-friends-with-caches.html</guid>
      <pubDate>Fri, 31 Jul 2020 08:34:10 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you lived in a village with only one mode of transport, say, a single car per family, then there would be resources that take longer to get at then others. On a computer, the same is true, where the village is the CPU, and the resources are things where data lives or is communicated over, such as a disk drive or a network socket. Things in your own home are quick to fetch; these would be the registers of a CPU. Some things are still quick for you to fetch but you don't keep directly inside your home. Perhaps you have a few of these sheds so you can fit various things, like your garage and a work shed. When the shed and garage don't suffice and you need new supplies, you travel to the store to purchase supplies, bringing them home with you to put in the shed or garage or whatever outdoor storage you own.</p>
<p>All of this indirectly describes a computer <strong>memory hierarchy</strong> and the idea behind the hierarchy is that things at the top are <em>generally</em> faster to access while things on the bottom are slow. For some reason it's always drawn like a food pyramid.</p>
<figure>
  <img
    src="/assets/images/making-friends-with-caches-memory-hierarchy.jpg"
    alt="A memory hierarchy pyramid"
    title="A memory hierarchy pyramid">
  </img>
</figure>
<p>As noted, memory hierarchies aren't just about storage of data. The network, for example, is a part of the memory hierarchy of a computer, and is usually below storage, but that doesn't mean that talking over a really fast network interface is going to be slower than accessing a spinning disk drive because the pyramid told us so. It simply means that we can reason about the relative performance of things with reasonable educated guesses. The place for profiling and collecting numbers is not ousted by the existence of the memory hierarchy model. My favorite form of a memory hierarchy is the <a href="https://gist.github.com/justanotherdot/e3c6e201fd5495c671e8faf18e9d741b">&quot;Latency Numbers Every Programmer Should Know&quot;</a> collection and <a href="https://colin-scott.github.io/personal_website/research/interactive_latency.html">there's this neat visual aid that has a time slider so you can compare times across relevant years</a>.</p>
<p>If things are far away, it makes sense to bring them closer, but does this always make sense? Time spent shuttling things from some far off place to our homes makes sense, but what if we are just going to bring something home and never look at it again?</p>
<p>Caches are generally designed the way they are based on two core ideas called <strong>temporal locality and spatial locality</strong>. Sometimes these ideas are grouped into the notion of <strong>locality of reference</strong> or just <strong>locality</strong>. Temporal locality refers to the high likelihood that if you bring something closer for use, you are likely to use it again. Spatial locality refers to the high likelihood that if you bring something closer to you for use, you are likely to want adjacent things to that resource.</p>
<p>Specifically with CPUs, caching layers are designed to benefit data that is repeatedly accessed as well as bringing data in by whole <strong>cache lines</strong> such that neighboring values are accessible, thereby favoring contiguous data in memory. Structuring data that favors these qualities of locality is often called being <em>cache friendly</em>. If you have a tight loop over an array and wonder why it's so fast, this is why; depending on the size of the array, you are probably going to bring in large chunks of the array for access and if your program access the array multiple times without touching too much unrelated data, you're likely to get a very high cache hit rate. You can actually inspect the rate by which you are hitting or missing the lookup for a particular value in a cache level by running <code>perf</code> over your program. For example, on linux you can run:</p>
<pre><code class="language-bash">$ # allow perf to do it's sampling.
$ echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
$ # -d for detailed.
$ perf stat -d program
Performance counter stats for 'program':

        117,416.25 msec task-clock                #    1.858 CPUs utilized
            59,648      context-switches          #    0.508 K/sec
             3,324      cpu-migrations            #    0.028 K/sec
         1,875,173      page-faults               #    0.016 M/sec
   406,889,422,900      cycles                    #    3.465 GHz                      (37.34%)
   418,921,344,585      instructions              #    1.03  insn per cycle           (37.40%)
    73,495,121,565      branches                  #  625.937 M/sec                    (37.51%)
     1,542,783,222      branch-misses             #    2.10% of all branches          (37.56%)
   122,094,600,307      L1-dcache-loads           # 1039.844 M/sec                    (37.69%)
     4,173,542,186      L1-dcache-load-misses     #    3.42% of all L1-dcache hits    (37.62%)
     1,041,448,237      LLC-loads                 #    8.870 M/sec                    (37.55%)
       308,710,304      LLC-load-misses           #   29.64% of all LL-cache hits     (37.33%)

      63.190530678 seconds time elapsed

      75.387109000 seconds user
       4.203457000 seconds sys
$ # or more precisely with exact events chosen
$ perf stat -e L1-dcache-loads,L1-dcache-load-misses,LLC-loads,LLC-load-misses,LLC-stores,LLC-store-misses,LLC-prefetch-misses,cache-references,cache-misses program
Performance counter stats for 'program':

    62,785,291,592      L1-dcache-loads                                               (37.44%)
        58,672,227      L1-dcache-load-misses     #    0.09% of all L1-dcache hits    (37.46%)
         9,445,705      LLC-loads                                                     (37.47%)
         1,859,151      LLC-load-misses           #   19.68% of all LL-cache hits     (37.42%)
        10,586,766      LLC-stores                                                    (25.05%)
         2,284,173      LLC-store-misses                                              (25.14%)
   &lt;not supported&gt;      LLC-prefetch-misses
       164,972,297      cache-references                                              (37.64%)
        29,366,970      cache-misses              #   17.801 % of all cache refs      (37.48%)

      35.113199020 seconds time elapsed

      34.627643000 seconds user
       0.071941000 seconds sys
</code></pre>
<p>Where <code>program</code> is the program you want to examine. The events in the middle column we care about start with a capital L; <code>L1</code> is the first, fastest level cache to the CPU, and <code>LLC</code> stands for <code>Last Level Cache</code>. For <code>L1</code> it's a <code>dcache</code> for <code>data cache</code> because instructions can also be cached. If we wanted information on the instruction cache we could also request that with <code>icache</code> instead of <code>dcache</code>.</p>
<p>Check out how <code>LLC-prefetch-misses</code> is unsupported on the CPU I am running this example on; sometimes perf events aren't available on all machines and kernel configurations. Lastly, notice how I chucked in <code>cache-references</code> and <code>cache-misses</code> which we can learn the meaning of by going to the man page for <code>man perf_event_open</code>. Here's a snippet from what mine mentions about the two (<a href="http://web.eece.maine.edu/%7Evweaver/projects/perf_events/perf_event_open.html">link to online reference for those who want to follow along and don't have a computer handy</a>):</p>
<pre><code class="language-bash">&lt;snip&gt;
PERF_COUNT_HW_CACHE_REFERENCES
       Cache accesses.  Usually this indicates Last Level Cache accesses but this may vary depending on your CPU.  This may in‐
       clude prefetches and coherency messages; again this depends on the design of your CPU.

PERF_COUNT_HW_CACHE_MISSES
       Cache misses.  Usually this indicates Last Level Cache misses; this is intended to  be  used  in  conjunction  with  the
       PERF_COUNT_HW_CACHE_REFERENCES event to calculate cache miss rates.
&lt;snip&gt;
</code></pre>
<p>If you want to know more events available to you, you can call <code>perf list</code>. If you are looking for something really specific, sometimes developer guides for CPUs will contain information about event numbers for specific hardware events that you can pass to perf to record.</p>
<p><strong>Bringing things closer is part of a larger principle of being lazy and laziness has performance benefits</strong>. Often performance tuning is an odd mix between both doing as little work as possible and being as slim as possible but also utilizing resources to their maximum. ****If you can get away with collecting supplies once a week it is going to be more efficient than going to the store every day. If you can work on data repeatedly that's all next to one-another, you are going to avoid paying for the cost of accessing main memory repeatedly. If you can store data off a disk that doesn't change into an in-memory data structure acting as a cache, you will avoid paying the cost of trapping into the kernel to run a system call for the various reads only the one time you read the file.</p>
<p>To recap:</p>
<ul>
<li>We can make educated guesses about the relative performance of how fast it will take to reach data based on a mental model of a memory hierarchy and some averages we can keep in our back pocket. You'll likely build up a sensibility for these numbers over time.</li>
<li>Caching data is the act of bringing data closer to where the work is happening. Effective caching takes advantage of two types of <strong>locality of reference</strong>, spatial and temporal, given the high probability that you will want to access something you've used before and the fact that you probably also want to work on adjacent datal. Designing your use of data around this concept is called being <strong>cache friendly.</strong></li>
<li>You can use <code>perf</code> to collect actual hardware samples on linux. Similar solutions exist for other operating systems. This data gives you a rough gauge of whether or caching is being fully utilized in your program under examination.</li>
<li>Caching is a part of the performance principle of being lazy. <strong>If you have to do work, do as little of it as possible.</strong></li>
</ul>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>System's Thinking: A Primer</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/systems-thinking-a-primer.html</link>
      <guid>https://justanotherdot.com/posts/systems-thinking-a-primer.html</guid>
      <pubDate>Thu, 16 Jul 2020 16:18:52 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Are you confused about what to learn when it comes to the fundamentals of programming? Do you constantly get told that algorithms and data structures are the gold standard for what will help propel your understanding? How do you get better at designing solutions rather than operating preexisting ones?</p>
<p>Most of what people try to shill as fundamentals does not end up empowering people with a framework around problem solving. People assume that instead of having the ability to solve problems themselves, they instead need to learn a grab-bag of tool-oriented prior knowledge. This taints one's arc of learning with the idea that it is all about gobbling up technical arcana, all the while reliant on others to find just which mystery to focus on next.</p>
<p>Mathematics and systems thinking has helps me build mental models of almost everything around me, aiding me in how I navigate the world and make decisions. You are likely carrying around <em>lots</em> of mental models with you already. Strong mental models that refine over time give you the ability to understand different programming languages without having to read entire books, immerse yourself in new code you've never seen before without having to ask a number of it's inventors who may have left little to no documentation, or, perhaps, how to reason about a market you are creating as part of a business. <strong>Models describe the essence of these things; them being &quot;mental&quot; is merely a matter of keeping them in your head.</strong></p>
<p>Modeling is about describing systems. While a collection holds all the items contained, a system augments this with descriptions of how the items interrelate. <strong>At it's essence, a system is a collection of shapes, usually boxes or circles, representing distinct entities, and arrows describing the relationships between these entities.</strong> If you're mathematically inclined a system is a graph with nodes (the shapes) and edges (the arrows).</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-pdp11.jpg"
    alt="A functional block diagram of the PDP-11 computer"
    title="A functional block diagram of the PDP-11 computer">
  </img>
</figure>
<p>The shapes in the graph expressed should <strong>not</strong> be thought of as black boxes. Instead, <strong>think of a systems diagram as merely a view of the system at a particular scale.</strong> It is not that we should never see the details of the entities in the system, as the popular notions around encapsulation may lead us to think, but that we are simply ignoring them when discussing the system at this level of detail.</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-555-timer.jpg"
    alt="A schematic for a 555 timer integrated circuit"
    title="A schematic for a 555 timer integrated circuit">
  </img>
</figure>
<p>Systems that describe the flux of some unit, continuous or discrete, often refer to the unit flowing across the connections as a <strong>stock</strong>. <strong>Taps</strong> adjust the rate of flow of the stocks along the arrows. It's is a good practice to not overload the usage of what an arrow means by having it describe verbs <em>or</em> the inlet and outlet of stocks, but not both.</p>
<p>Systems inevitably end up describing repetitive actions, otherwise known as <strong>loops</strong>. A loop may consist of a way to communicate necessary changes to some other entity in the loop which we call <strong>feedback</strong>. A loop with finely-tuned feedback mechanism will seek some state of equilibrium while a loop whose feedback is disregarded or misaligned will self-reinforce in some general trend. If a stock is involved, you can see how producers and consumers are affecting the quantity of the stock in this system with a behavior-over-time graph.</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-feedback-loops.jpg"
    alt="A systems diagram showing the basics of feedback loops. Author: Kjell Magne Fauske licensed under CC BY 2.5"
    title="A systems diagram showing the basics of feedback loops. Author: Kjell Magne Fauske licensed under CC BY 2.5">
  </img>
</figure>
<p>When we talk about misalignment of feedback in loops, we tend to mean the amount of delay of some message or action. <strong>All loops have some delay in their communications</strong>, but the tuning of that delay is important. Given the context of the system or loop, a short delay may be just as bad as a long delay. A system describing the supply and demand of a widget may want a certain sized delay so that demands for manufacture do not lead to oversupply.</p>
<p>An example of a system seeking a state of equilibrium may be a thermostat connected to a heater. If the heater reaches a target point on the thermostat, the heater stops until the thermostat drops below the target point. If the thermostat didn't exist, there would be no feedback and the heater would continue to increase over time. If there was a substantial delay in the feedback of the thermostat to the unit that controls whether or not the heater is on, the heat would also continue to grow up until a point. If you include a cooling system, you may get finer control of both directions of temperature. It's worth mentioning that most systems of growth have a saturation point, but that point may be functionally quite high in the context of use!</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-aircon.jpg"
    alt="A systems diagram showing the parts of an air conditioner. Author: Controlsystemintro licensed under CC BY-SA 4.0"
    title="A systems diagram showing the parts of an air conditioner. Author: Controlsystemintro licensed under CC BY-SA 4.0">
  </img>
</figure>
<p>State machines also make up the basis of a system; the boxes are states, the arrows transitions between states, and the use of a state machine includes tracking the current state of where an agent in the state machine might be. This type of thinking is highly underappreciated when designing transactional systems such as network requests, payment flows, and so on. State machines help clarify if the flow of logic has any odd behavior, such as winding up in invalid states or transitions that lead to inescapable states.</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-state-machine.jpg"
    alt="A state machine describing the TCP/IP control flow. Author: Ivan Griffin licensed under CC BY 2.5"
    title="A state machine describing the TCP/IP control flow. Author: Ivan Griffin licensed under CC BY 2.5">
  </img>
</figure>
<p>The best way to get better at systems thinking is to start drawing out systems diagrams. Drawing by hand in a journal is my preferred route, but there are also many services that make doing these easily. For example, here's an incredibly rough sketch of a diagram I copied from Richard Hamming's classic &quot;The Art of Doing Science and Research&quot; showing the essence of a CPU.</p>
<figure>
  <img
    src="/assets/images/systems-thinking-a-primer-cpu.jpg"
    alt="A hand-drawn diagram of the essential parts of a CPU, based on
    Richard Hammings work in The Art of Doing Science and Engineering"
    title="A hand-drawn diagram of the essential parts of a CPU, based on
    Richard Hammings work in The Art of Doing Science and Engineering">
  </img>
</figure>
<p>It also helps to see other people building up systems, as I've included some examples here. Functional block diagrams are my favorite but if you see any type of graph you could, theoretically, coerce that into the idea of a system and think about its causal relationships, how time plays a part of the system and the states that it may be in, how the author of the diagram has chosen to render distinct parts of the system, or maybe combine both the flow of units <em>as well as</em> the actions between nodes and see how confusing that gets. There are lots of places like the <a href="https://archive.org/">Internet Archive</a> and <a href="http://bitsavers.org/">Bitsavers</a> that have scanned copies of old computer reference manuals with plenty of circuit and functional block diagrams that are great places to start, and an article discussing architectural changes is worth it's weight in gold, <a href="https://www.honeycomb.io/blog/secondary-storage-to-just-storage/">like this one by Charity Majors</a>. <em>Don't worry about system diagrams being perfect; in the same way that writing can help you refine your thoughts, systems diagrams can help refine your thinking around a system.</em></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>What Makes Up a CPU?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/what-makes-up-a-cpu.html</link>
      <guid>https://justanotherdot.com/posts/what-makes-up-a-cpu.html</guid>
      <pubDate>Wed, 08 Jul 2020 09:23:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I consider Ben Eater's series on building an 8-bit CPU from scratch to be a
remarkable learning journey that anyone keen enough to better understand the
hardware they build software for should watch. I liked Ben's course so much that
I thought I'd write a little mini summary of it. This isn't trying to be
satisfactory replacement to Ben's video course and I <em>strongly</em> urge you to <a href="https://www.youtube.com/watch?v=HyznrdDSSGM&amp;list=PLowKtXNTBypGqImE405J2565dvjafglHU">go
watch the whole thing from start to
finish</a>.
For example, I won't cover the details of circuit designs such as s-r latches, d
latches, d flip-flops, and j-k flip-flops the way Ben does, but I will try to
cover an overall view of the CPU as best as I remember and will also mention
some meta things I liked that Ben did regarding the design and assembly of the
CPU as a whole.</p>
<h2>The Core of a CPU</h2>
<p>Modern digital electronics are like irrigation networks for electricity. CPUs
are no different. Data is stored either in memory that requires power (volatile,
as it will disappear when the power is lost) or not (non-volatile, such as flash
memory). Electricity flows to different parts of the system which I'll call
<em>units</em> via the <strong>bus</strong>, which simply holds some fixed size of data at a time.
We might generally call this a <em>word</em> and in this case a word is eight bits, as
this is an eight-bit CPU. Units have to be gated to talk or read from the bus to
avoid cross-talk. Data can be temporarily stored in <strong>registers</strong>, of which
Ben's CPU has six, not including the register he uses to display numbers from
binary, which he calls &quot;out&quot;, and the memory register on the 64 bytes of Random
Access Memory, or <strong>RAM</strong>, he builds up to store data and code for programs.</p>
<p>All the registers on the CPU are eight-bits wide. One of these registers is the
<strong>instruction register</strong> where instructions from RAM will be fetched and stored
so they can be decoded into the actual bits that will control the various units
by the <strong>control logic</strong> unit. These bits enable or disable certain units, as
well as configure them to do different things. The act of translating the
instructions into these individual bits is called <strong>decoding</strong>. Decoding
instructions is merely the act of supplying the instruction as a key to a lookup
table, which we will talk about more later, to get the resulting set of bits.
This CPU is based on <strong>microcode</strong> which means that the instruction plus a
counter built into the control logic unit make up the key that is used for
translation. Not all CPUs need microcode. For example, a CPU might translate
directly to one set of bits rather than several broken up into steps; this is
what RISC, or Reduce Instruction Set Computer, architectures are supposed to be
like. The reality is that most modern RISC processors most likely include a bit
of microcode.</p>
<p>The other two registers are for the Arithmetic Logic Unit or <strong>ALU</strong>, which only
supports addition and subtraction in Ben's case. The registers are simply named
&quot;A&quot; and &quot;B&quot;. Values stored in &quot;A&quot; and &quot;B&quot; are added or subtracted together and
stored back into the &quot;A&quot; register. Ben later augments the ALU with flags to
detect overflow and if an operation resulted in a zero so he can install
conditional instructions, which is stored in a &quot;flags&quot; register. As part of the
change, he also includes the flags as part of the key in the instruction
decoding phase.</p>
<p>From a pedantic point of view, the RAM and the &quot;out&quot; register plus its
associated display aren't actually part of the core of a working CPU, but they
help making writing programs easier by giving us a place to put data and code as
well as visualizing the results of our calculations. The output display is
actually rigged up using four seven-segment displays. With the instruction
decoding phase and the display, Ben uses Electrically Erasable Programmable
Read-only Memory, or <strong>EEPROMs</strong>, as the look up tables we mentioned before; on
one end he can feed some bit pattern and out the other end receive a result. A
good mental model is that with an EEPROM we 'select' some value in the memory
given some provided key, even though this typically called an &quot;address&quot;. For the
&quot;out&quot; display he can provide the binary value and receive a bit pattern that is
just right for showing on one of the four seven segment displays. The display
actually display one-by-one but display so rapidly (&quot;refresh&quot;) that the shift
isn't visible to the eyes.</p>
<p>Behind all of this is a pulse that goes &quot;high&quot;, i.e., it emits a five volt
signal, at regular intervals, called the <strong>clock</strong>. Clocks drive a CPU by
breaking up actions into discrete steps. When the clock pulse is high, we might
call that <strong>tik</strong> and when the clock pulse is low we might call that <strong>tok</strong>.
Going tik, then tok, is called a <strong>cycle</strong>. Imagine some crank that shows single
images for a movie on a screen for every cycle. If you move the crank fast
enough and the images look to be close enough together in time, you wind up with
what looks like a fluid image, but you can also single-step the crank or move it
more slowly and notice all individual images making up the movie.</p>
<p>A <strong>program counter</strong> stores the next instruction that will execute on the
CPU.You can either increment the counter by one or change it to some absolute
value, which is how jump instructions, and therefore conditionals, work. All
instructions have two initial steps which involve pulling a value from the
program counter into the memory register on the RAM and then pulling the
addressed value into the instruction register. This way the program can startup
on any instruction but wind up back to where the program counter is pointing.
The memory register is needed to ensure that whatever address is chosen to
output or read into the RAM won't change simply because the rest of the system
has moved on, e.g., the bus value has changed.</p>
<p>Of notable mention is the reset switch he builds into the system, as well as an
ability to stop the clock to emulate a HALT instruction. These make starting
from the beginning of a program and stopping the program at a particular point
easier than simply having the program &quot;spin&quot; at the end.</p>
<h2>The Design and Assembly</h2>
<p>There's a lot of meta things Ben does I think are applicable to a wide range of
projects.</p>
<p>If it's not obvious, I like that Ben breaks the CPU into separate parts or
units. This way Ben can focus on one thing at a time, building up earlier
tools and units for re-use later, freeing up his ability to think on different
problems without having to hold the whole of the CPUs design in his head at
once.</p>
<p>He tends to build out units using bare circuits and simple transistors, first,
moving onto integrated circuits, or ICs, later when things become tedious. Ben
actively takes the time to build out initial circuits to demonstrate some
essential electrical patterns. The way Ben leads up to d and j-k flip-flops
based on his prior building and use of d and s-r latches informs a <em>lot</em> of the
circuitry across the whole of the computer. Knowing how data gets &quot;trapped&quot;,
forming the basis of memory, and how this plays into how memory is &quot;gated&quot; on
and off the bus, can help strip away whatever magic you might have left about
what is going on under the hood. He does have some supplemental videos on how
semiconductors, transistors, and diodes work that can help fill in fundamentals
beyond what the primary playlist covers.</p>
<p>On a number of units Ben goes out of his way to make driving the unit by hand,
such as allowing one to single-step the clock or program the RAM with
dip-switches. Doing this allows him to rapidly smoke test or make quick tweaks
on a given unit.</p>
<p>I like that he builds the EEPROM from scratch because it helps keep the attitude
that &quot;there is no magic&quot;.</p>
<p>At several points he uses a multimeter and oscilloscope to visualize what is
going on with the circuits. This really drives in the metaphor of the electrical
current analogy and that the transistors and complicated logic are the gates,
feeding data into and out of particular pools.</p>
<p>In the core explanation, I noted that the RAM and LED display weren't crucial
parts of a CPU, but it is good that Ben included them because they helped
facilitate the process of building up the CPU, testing it, and making it useful
beyond simply being a doormat that blinks.</p>
<p>Last, but not least, it's a small thing but I like that Ben actively goes out of
his way to build a number of programs for the CPU to run. This verifies the CPU
is, indeed, working as intended, and helps highlight a bit of a discussion
around &quot;what is Turing completeness?&quot;, or, more simply, how can a computer be
deemed of enough general-use such that it can be used to compute anything we
would generally hope for it to compute?</p>
<h2>Conclusions</h2>
<p>Ben's courses are great. He covers a number of other topics, such as building up
a basic computer using a 6502 chip, talking about networking internals, checking
for reliability on a transmitted message, and he also has a pair of videos
building up a video card from scratch.</p>
<p>The brilliance of all of this is that there isn't any magic behind modern
computers. There is a lot of complexity through many, <strong>many</strong> layers of
abstractions, responsibilities, code, data, hardware, and so on, but the overall
view of things need not be that complex. Ben's CPU isn't the fastest or
feature-complete, but it does give a mental model for a basis of a CPU.</p>
<p>Modern CPUs are a <em>beast,</em> but you don't need to know everything to feel like
you know enough of a subject to be dangerous. Many modern-day, bootcamp-trained
software developers have learned specific patterns that might aide them with
particular stacks of technology, but I have helped mentor a number of these
people who wish they could better understand just what is going on under the
hood. Holding the idea that there isn't any magic and you can continually
unravel the layers and components to gain a deeper understanding <em>is</em> possible
and it is thoroughly rewarding. You don't need to know everything at once,
either! Understanding how a CPU works and some fundamentals of electronics both
at a rudimentary level <em>will</em> help guide you with other understanding other
things, and with time you can refine that understanding.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Discovering Problematic Commits With Git Bisect</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/discovering-problematic-commits-with-git-bisect.html</link>
      <guid>https://justanotherdot.com/posts/discovering-problematic-commits-with-git-bisect.html</guid>
      <pubDate>Tue, 30 Jun 2020 08:50:46 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>A problem has happened due to some offending code landing on your main,
production branch. You use <code>git</code> and your best bet is to keep rolling back
commits until the system finds itself in a steady state. You come late into this
picture and you're unsure how far back you need to go.</p>
<p>Firstly, you ought to be using something that alleviates the need for running
through out an entire CI pipeline in order to produce a deploy. I've talked a
bit about this in the past on my <a href="https://www.youtube.com/playlist?list=PLG8S6YrJRoYI3CIUqvGX4NBSaMWZxe9in">screencasts about setting up a
CI</a>
regarding the distinction between a deployment and a release. If you have
something like this, rolling back a fair few number of releases is probably
trivial to attempt.</p>
<p>However, if you don't have this in place or you really do need to roll through
an entire CI pipeline, then you can still using something like <code>git bisect</code> to
find the first offending commit.</p>
<p><code>git bisect</code> runs a binary search across a span of commits. The general
framework for running a <code>git bisect</code> is the following:</p>
<ol>
<li><code>git bisect start FROM_COMMIT TO_COMMIT</code></li>
<li>Test the commit, determine if it is good or bad, and tell <code>git</code> with <code>git bisect good</code> or <code>git bisect bad</code>. You can also skip commits with <code>git bisect skip</code>.</li>
</ol>
<p>The trick to finding the first offending commit isn't to run the same steps your
CI pipeline would; you should have all those builds available for review and
they will tell you whether or not a build truly succeeded, unless you can't
trust your CI and, in that case, you have other issues on your hand. Crafting
your own test and running it each time in (2) will help guide you in the
decision to making a choice for whether or not the commit is <code>good</code> or <code>bad</code> in
light of what you are trying to find.</p>
<p>You can alleviate the tedium of (2) by using <code>git bisect run</code> and supplying a
program. If the script fails or you ever want to abandon your search midway, you
can always run <code>git bisect reset</code> and start over again. There are some tricks to
how you can craft the exit codes from the script you write for <code>git bisect run</code>
that really make this process a lot faster. To give a sense of the range of use
for <code>git bisect</code> as a general search tool, let's call our test script
<code>predicate</code>.</p>
<pre><code class="language-bash">#!/bin/sh -eux

# NB.
# exiting with 125 tells `git bisect run` to skip this commit.
# exiting with 0 means the commit is `good'.
# exiting with 1 means the commit is `bad'.

cargo build || exit 125 # skip failed builds.
target/debug/program &gt; /tmp/program.out
[ ! diff /tmp/program.out /tmp/program.snapshot ] &amp;&amp; exit 1
</code></pre>
<p>You'll need to place this script somewhere outside of the current git repository
as it will mess up checkouts between commits, and, as always, ensure it is
executable. Another pitfall that can hurt is how you structure your git history;
if you use merge styled commits, as is the default for GitHub, then you will
probably not care if the commits in between the merge commits fail. You can do
one of two things: output the list of all merge commits that match a particular
pattern, e.g., the way GitHub does it, or you could also, if your history is
clean enough, use <code>git show --no-patch --format=&quot;%P&quot; &lt;commit hash&gt;</code> to determine
if a commit has more than one parent; you'll see more than one hash noted in the
output. You can find quick version I hacked together filtering out commits with
the GitHub styled subject lines you can tweak at <a href="https://gist.github.com/justanotherdot/d587f5bea0f6937ef7f7bda53f23ac56">this
gist</a>.</p>
<p>In the above example I show testing against a snapshot given some program
output, but really the predicate could be <em>anything</em>. Using <code>git bisect</code> to
drive things like textual search has better alternatives like the &quot;pickaxe&quot; with
<code>-S</code> in <code>git log</code>, but if you want to find the first commit where something
happened and it isn't part of the data that git saves, such as program behavior,
then <code>git bisect</code> will let you find it far faster. I've also used this in the
past to whip up quick, minimal tests that I can inject after the checkout and
run some test suite against. <code>git bisect run</code> takes any binary, too, meaning you
don't <em>have</em> to use a shell script like I have in the example above. The real
aim is not to think of the <code>predicate</code> script or program as something that has
to be about failures; you can easily use it to discover first instances of any
kind of particular behavior a program may exhibit, as long as it is reproducible
locally.</p>
<p>Granted, a system may be so complex in it's operation that there is no way for
you to locally verify the offending commit. Mitigating or &quot;stopping the
bleeding&quot; is something that needs to happen quick. With that said, <code>git bisect</code>
might be a better tool for analysis later, when the pressure is low and you can
better craft a test or predicate to find where the fault first occurred, but if
you haven't spent a lot of time with release engineering or you are in a place
where it could use some improvements, running <code>git bisect</code> in this matter might
help save you precious time, and even if you do have good release engineering in
place, it might help save you a pulling out a lot of hair finding the place
where code has effectively broken down.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Feature Flag Cleanly With Blocks</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/feature-flag-cleanly-with-blocks.html</link>
      <guid>https://justanotherdot.com/posts/feature-flag-cleanly-with-blocks.html</guid>
      <pubDate>Mon, 29 Jun 2020 19:45:44 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I recently had to feature flag some code in JavaScript and felt myself wishing I
had Rust's expression-based blocks. In the JavaScript code, I didn't want to
break out the logic into its own function or module as the code was a proof of
concept and deciding on an interface early on would distract me. In languages
that aren't expression and block oriented, you have one of two choices:</p>
<ol>
<li>Use an immediately invoked function</li>
<li>Have 'unset' or 'default' variables on the outside of some scope and a scope that
potentially assigns to the variables</li>
</ol>
<p>Here's an example of the two cases in JS:</p>
<pre><code>// with an IIFE.
const featureFlaggedItem = (() =&gt; {
  if (!featureFlag) {
    return {
      // defaults without the flag.
      // the steady state of the system.
    };
  }

  // feature flagged code.
})();
</code></pre>
<pre><code>// or with an assignment, relying on side effect.
let featureFlaggedItem = {
  // defaults without the flag.
  // the steady state of the system.
};
if (featureFlag) {
  featureFlaggedItem = {
    // featureFlagContent
  };
}
</code></pre>
<p>The problem with the side effect approach is that we lose the nice benefit of
having <code>featureFlaggedItem</code> as <code>const</code> as well as keeping things nice and tidy
for easier deletion later on. I personally refuse to use the second approach.
Rust let's you easily write the above code as following:</p>
<pre><code>let feature_flagged_item = {
  if (!featureFlag) {
    return {
      // defaults without the flag.
      // the steady state of the system.
    };
  }

  // feature flagged code.
};

// or ...

let feature_flagged_item = if (featureFlag) {
  // feature flagged code.
else {
  return {
    // defaults without the flag.
    // the steady state of the system.
  };
};
</code></pre>
<p>But this is wildly useful for a lot of things beyond one-off changes. Maybe you
need a one-off value for a function argument but you don't want to immediately
invoke a closure or define a function to call. With blocks you can tuck all
sorts of code into places with or without assignment. If you have a lot of
&quot;identifier&quot; pollution going on in a given scope, say with a lot of temporary
variables, you can tuck them under the rug with blocks. I tend to have a lot of
assignments that break up code like a newspaper article but there is some
dispute around shadowing. I am pro shadowing in Rust as I feel it bars a class
of bugs, but you can understandably avoid shadowing if you so care; <a href="https://github.com/rust-lang/rust-clippy/blob/master/clippy_lints/src/shadow.rs">using
<code>clippy</code>s lint on the
matter</a>,
or ensuring the temporary shadowed variable(s) are only present for the inner,
temporary, scope.</p>
<p>The best part is that when the &quot;one-off&quot; value becomes less &quot;one-off&quot;, you can
easily take the block and dump the contents straight into a function and it will
work as-is, with or without the superfluous curly braces! Splitting up the
decisions around what is done to build up the value versus the surface area of
the value (its interface) is a great way to guarantee the interface is what you
really want it to be rather than what it had to be in order to figure out its
implementation. In general it's best to split up work in such a way that you can
focus on each piece in isolation without the other pieces distracting you.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>An Opinionated Guide To Structuring Rust Projects</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/an-opinionated-guide-to-structuring-rust-projects.html</link>
      <guid>https://justanotherdot.com/posts/an-opinionated-guide-to-structuring-rust-projects.html</guid>
      <pubDate>Fri, 12 Jun 2020 20:49:59 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Cargo's initial project layout is good for bootstrapping a project but as time
goes on there is a growing need to automate chores, wrestle with compile times,
and increase discoverability for maintainers and contributors. I've previously
written about <a href="https://www.justanotherdot.com/posts/structuring-rust-projects-with-multiple-binaries.html">how I personally orchestrate chores on a Rust
project</a>
but this article will focus on the latter two points.</p>
<p>The highest leverage act you can do with structuring a Rust project is to break
it into independent crates. Chunks of logic that have shown stability over a
window of time are immediate candidates for splitting into a crate, as well as
semantic boundaries between concepts in a given codebase. For example, you may
want to keep sets of types distinct from one crate to the next in the same
project or you might want to enforce that driver logic should be as minimal as
possible with only some glue tying together other core logic libraries. Keeping
things cleanly separated means clustered concepts are easier to locate while we
can aggressively cache crates that don't change much. In terms of naming I
prefer each crate to be in kebab-case and to use the project's name as the
prefix for the sub crate. If our project name was &quot;foo&quot; then each crate would be
prefaced with &quot;foo-*&quot;. You can tie all of these crates into a workspace for a
central place to build the entirety of the project. If our project had three
crates in it, we could put a <code>Cargo.toml</code> at the root of our project with the
contents:</p>
<pre><code>[workspace]
members = [
  &quot;foo-core&quot;,
  &quot;foo-cli&quot;,
  &quot;foo-benchmark&quot;,
]
</code></pre>
<p>The general advice for build times is to first use something like <code>cargo check</code>
and move to <code>cargo test</code> and finally some form of <code>cargo build</code> with or without
options. A way to drive down build times is to keep building all the time as you
make changes. I prefer to run multiple loops with various subcommands specified
while I code. You can get around locking issues on the same <code>.cargo</code> and
<code>target</code> directories by changing these with <code>CARGO_HOME</code> and <code>CARGO_TARGET_DIR</code>
respectively. This means you can spin up several <code>watchex</code>, <code>cargo-watch</code>, or
<code>entr</code> loops as shell jobs or in separate terminals. To give an example I will
sometimes do <code>cargo watch</code>, which does <code>cargo check</code> by default, and then will
specify <code>CARGO_HOME=/tmp CARGO_TARGET_DIR=/tmp/target cargo watch -x test</code> to
get test information as it shows up.</p>
<p>If you're using a CI and can afford it, pushing jobs off to a remote server to
build at the same is yet another extension to this &quot;build all the time&quot;
mentality. When you're happy with your changes you are closer to merging. If you
pair this with something like <code>sccache</code> for caching crates across projects, you
can see some nice gains on compile times across several build bots or, if you
run a similar environment to your build bots, you can even share crates from
both local development machine and build bots at the same time. Once <code>sccache</code>
is installed, you can export <code>RUSTC_WRAPPER=$(which sccache)</code> and check if it's
running across builds with <code>sccache -s</code>. I'm unsure what gains you'd see over
<code>cargo</code> on a single machine as I've yet to dive into the core of how <code>sccache</code>
works under the hood but it's harmless to run for a try.</p>
<p>You have the option to be disciplined and keep all crates on the same version to
make downstream consumption easier, such that if you want to install <code>foo</code> and
<code>foo-bar</code> you could know that version <code>2.0.0</code> is valid for both crates. You can
also setup the installation as a transitive thing from some 'central' crate that
could always install the &quot;right&quot; version of <code>foo-bar</code> given some build feature
flag. You may want more flexibility in what version of <code>foo-bar</code> you use,
however, and as long as <code>foo</code> doesn't also depend on <code>foo-bar</code> you shouldn't
have to do any juggling.</p>
<p>I've left some stray tricks for last in the possibility that they may help your
specific case. You can try linking with <code>lld</code> or <code>gold</code> instead of the standard
linker. You can do this with <code>RUSTFLAGS=&quot;-C link-arg=-fuse-ld=lld&quot;</code> to use <code>lld</code>
at least on linux but I don't always see speedups from this. Setting
<code>CARGO_BUILD_JOBS</code> to a number higher than the number of capabilities (cores)
you have on your system is likely to <em>increase</em> compile times, but you could
split your test, check, and build jobs across lower number of cores, such as two
cores for <code>test</code> and another two for <code>check</code> on a four-core machine. If you are
truly desparate you can try gimmicks like building to a less intense target.
I've written a shell script that will build all possible cross-compile targets
<code>rustc</code> can attempt and report build times in the event that they succeed. You
can find the gist
<a href="https://gist.github.com/justanotherdot/ca1f163754e9a90f6c6b9dfb25a0598f">here</a>
and can invoke it with <code>x-compile-test</code>. You can also narrow down which targets
you want to use with a regex by specifying <code>FILTER=x86_64 x-compile-test</code>.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Picking Variants or Fields Out Of Collections of Enums</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/picking-variants-or-fields-out-of-collections-of-enums.html</link>
      <guid>https://justanotherdot.com/posts/picking-variants-or-fields-out-of-collections-of-enums.html</guid>
      <pubDate>Wed, 10 Jun 2020 20:06:54 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Occasionally you want to pull out a specific field or variant out of one or more
enums. Doing this with pattern matching is tedious and verbose, but there's a
simple way to use methods and combinators to do exactly what you want.</p>
<p>There's a pattern I use to solve both of these. If you already have the
collection in hand, you can write a simple method that operates on the type
named something like <code>as_foo</code> where foo is the name of the variant or field name
you are after. There's a clippy lint that says <code>as_*</code> functions should always
take references so I've followed the lint in the following example but it
doesn't really matter what you name the method and it's fine to have the method
take ownership of the value, too, if that makes sense for your use case. When
you define a method you can use it either with the basic method syntax
<code>value.as_foo()</code> or you can access it as an associated function e.g.
<code>Type::as_foo(value)</code>. Then we can use either method in tandem with the
<code>filter_map</code> or <code>flat_map</code> methods of an iterator. I personally prefer the more
terse way of passing the associated function instead of the closure, which is
sometimes referred to as &quot;point free&quot; style where the arguments, or points, are
not mentioned:</p>
<pre><code>#[derive(Debug, Clone, PartialEq)]
enum E {
    A { x: i32 },
    B { x: i32 },
}

impl E {
    pub fn as_x(&amp;self) -&gt; Option&lt;i32&gt; {
        Some(match self {
            E::A { x } =&gt; *x,
            E::B { x } =&gt; *x,
        })
    }

    pub fn as_b(&amp;self) -&gt; Option&lt;&amp;E&gt; {
        match self {
            x @ E::B { .. } =&gt; Some(x),
            E::A { .. } =&gt; None,
        }
    }
}

pub fn main() {
    // Method access off type.
    let a = E::A { x: 1 };
    let b = E::B { x: 2 };
    assert_eq!(a.as_x(), Some(1));
    assert_eq!(b.as_x(), Some(2));

    // Associated function on impl.
    let a = E::A { x: 1 };
    let b = E::B { x: 2 };
    assert_eq!(E::as_x(&amp;a), Some(1));
    assert_eq!(E::as_x(&amp;b), Some(2));

    // In a collection.
    let a = E::A { x: 1 };
    let b = E::B { x: 2 };
    let as_and_bs = vec![a, b];
    let xs = as_and_bs.iter().filter_map(E::as_x).collect::&lt;Vec&lt;i32&gt;&gt;();
    assert_eq!(xs, vec![1, 2]);

    // Selecting a field as a dummy pattern match.
    let b = E::B { x: 2 };
    let xs = Some(&amp;b).and_then(E::as_b);
    assert_eq!(xs, Some(&amp;E::B { x: 2 }));
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5bb110c4a9981caa3bc3317b9e2350c3">Playground</a>.</p>
<p>Pattern matching is powerful but sometimes you can reduce the number of explicit
pattern matches you perform by taking advantage of functions and combinators and
keeping the logic small and simple, letting you reason about what the result
ought to be on the other end. In the last case using <code>and_then</code> above, we can
reason that whenever we call <code>as_b</code> we're sure to get a single pattern match if
we must simply checking for <code>Some(E::B { .. })</code> or <code>None</code>. The compiler may not
understand that, though, and you'll most likely have to include a wildcard case,
but the brilliance of combinators is that you can chain them together in a
pipeline similarly to the fluid interface that iterators present.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Building An Intuition for Pattern Matching</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/building-an-intuition-for-pattern-matching.html</link>
      <guid>https://justanotherdot.com/posts/building-an-intuition-for-pattern-matching.html</guid>
      <pubDate>Tue, 19 May 2020 20:08:50 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><em>What's the point of pattern matching if we already have conditionals and
variable assignment in a language?</em></p>
<p>Pattern matching helps tease apart values and construct control flow using the
shape of data rather than bespoke logic, methods on types, or special fields on
a struct. For example, in languages that don't have first-class support for sum
types, enums in Rust, you'd have to encode the variant as a unique tag on
something like a struct, e.g.,</p>
<pre><code>struct Option {
    tag: String, // maybe one of 'Some' or 'None'.
    // and so on.
}
</code></pre>
<p>Then the <code>tag</code> field can be checked by traditional control flow. This is precisely
how it is done in languages like TypeScript, but in Rust, where sum types are
supported, we have no unique <code>tag</code> field to check and, since the compiler hides
this information away from us, we can't write a method to describe which variant
we have in our hands. It would be a bit clumsy if the compiler generated methods
for us as we might want to have methods with the same name!</p>
<p>Any kind of syntactic sugar used to construct a value is known as a
<strong>constructor</strong>, such as building values for structs, enums, tuples, and so on.
Pattern matching gives us a way to describe the shape of data using constructors
to match on and what to do if the value matches. This analogy isn't perfect, but
I like to think of patterns as mirrors with outlines; if the reflection matches
the outline of a constructor, we go down that path of logic, possibly with some
new values drawn out of the data. Here are some common patterns for
constructors:</p>
<pre><code>pub struct S {
    field: i64,
}

pub enum E {
    FirstVariant,
    SecondVariant,
}

pub fn main() {
    // Tuples.
    let a = (&quot;Fizz&quot;, &quot;Buzz&quot;);
    match a {
        (p, q) =&gt; println!(&quot;{}&quot;, format!(&quot;{}{}&quot;, p, q)),
    }

    // Numeric literals.
    let b = 123;
    match b {
        std::i32::MIN..=99 =&gt; println!(&quot;under one-hundred&quot;),
        100 =&gt; println!(&quot;exactly one-hundred&quot;),
        101..=std::i32::MAX =&gt; println!(&quot;above one-hundred&quot;),
    }

    // Strings.
    let c = &quot;A string.&quot;;
    match c {
        &quot;A string.&quot; =&gt; println!(&quot;it's _the_ string.&quot;),
        _ =&gt; println!(&quot;some other string.&quot;),
    }

    // Enums.
    let x = E::SecondVariant;
    match x {
        E::FirstVariant =&gt; println!(&quot;first variant of E&quot;),
        E::SecondVariant =&gt; println!(&quot;second variant of E&quot;),
    }

    // Structs.
    let y = S { field: 100 };
    match y {
        S { field } =&gt; println!(&quot;field is: {}&quot;, field),
    }

    // Slices.
    let z = vec![1, 2, 3];
    match *z { // we need * to dereference Vec to a slice.
        [a, b] =&gt; println!(&quot;{} + {} = {}&quot;, a, b, a + b),
        [a, b, c] =&gt; println!(&quot;{} + {} * {} = {}&quot;, a, b, c, a + b * c),
        _ =&gt; println!(&quot;any other unmatched vector&quot;),
    }
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=483dd9713719d848f7e221047961e8c8">Playground</a></p>
<p><em>Where can we put patterns?</em></p>
<p><code>match</code> is the traditional way of doing pattern matching but not the only way.
Matches work top-to-bottom, and they ensure that every case is handled, known as
<strong>exhaustivity checking</strong>.</p>
<pre><code>enum Val {
    Integer(i64),
    Float(f64),
}

match {
    Val::Integer(x) =&gt; println!(&quot;It's an integer: {}&quot;, x), // one &quot;arm&quot; or &quot;case&quot;
    // without anything else, this is non-exhaustive; it doesn't include Val::Float!
}
</code></pre>
<p>which fails to compile with the following error:</p>
<pre><code>error[E0004]: non-exhaustive patterns: `Float(_)` not covered
 --&gt; src/main.rs:8:11
  |
1 | / enum Val {
2 | |     Integer(i64),
3 | |     Float(f64),
  | |     -- not covered
4 | | }
  | |_- `Val` defined here
...
8 |       match v {
  |             ^ pattern `Float(_)` not covered
  |
  = help: ensure that all possible cases are being handled, possibly by adding wildcards or more match arms

error: aborting due to previous error
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=65e0688f852c45e4c1712f2481ef2231">Playground</a></p>
<p>Pattern matches in <code>let</code>s and function arguments will also work but must be
<strong>irrefutable</strong>, which is a fancy way of saying that the pattern can never fail.
Any pattern that covers all possible values of a type is irrefutable. It could
be literal like a range or with a variable, which will always capture a value
and, therefore, match.</p>
<pre><code>// works.
pub fn f((x, y): (i32, i32)) -&gt; i32 {
    x + y
}

// does not work.
//pub fn g((1, 2): (i32, i32)) {
// fails on anything other than g(1, 2).
// the compiler rejects this as a refutable pattern
// which is in place where only an irrefutable pattern can be.
//}

pub fn main() {
    //let 12 = 12; // fails.
    let x = 12; // succeeds.
    f((x, x));
    let std::i32::MIN..=std::i32::MAX = 12; // succeeds, covers all values.
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d21343054645f8500c7701fdcc174171">Playground</a></p>
<p>With functions, this is a bit different from other functional languages like
Erlang or Haskell. In those languages, you can write multiple function
declarations, each with their pattern match, and the function that matches the
pattern will be the one that executes. You can think of this like match
expressions but for functions! Rust, unfortunately doesn't have this, but it's
still fine to take the full value from the argument and make the entire function
body a <code>match</code>. So this in Elixir:</p>
<pre><code>def f(1) do
  // first case.
end

def f(2) do
  // first case.
end

def f(x) do
  // final, irrefutable case.
end
</code></pre>
<p>could be expressed as:</p>
<pre><code>pub fn f(x) {
    match x {
        1 =&gt; , // first case.
        2 =&gt; , // second case.
        x =&gt; , // final, irrefutable case.
    }
}
</code></pre>
<p><em>Isn't this a bit tedious? What if you don't care about particular portions of a
shape?</em></p>
<p>Ignoring particular values is easy with the <code>_</code> variable, or we can prefix a
variable name with <code>_</code> if we want to keep the name but ensure it can't be used.
This is formally known as a <strong>wildcard</strong>, but informally known as the &quot;don't
care&quot; variable. The equivalent for structs is <code>..</code> where we can specify only the
fields we care about and ignore the rest. These two dots, a bit like an
ellipse, must be mentioned in the last place of the struct.</p>
<pre><code>struct S {
    field: i32,
    property: (i32, i64),
}

pub fn main() {
    let s = S {
        field: 42,
        property: (12, 13),
    };
    match s {
        S { property: (12, _), .. } =&gt; println!(&quot;{}&quot;, 12),
        S { field, .. } =&gt; println!(&quot;{}&quot;, field),
        // or `S { field, property: _ } =&gt; println!(&quot;{}&quot;, field),`
    };
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=5045a6587c31f5d954ab52467b735d67">Playground</a></p>
<p><em>What if I want to describe some nested shape, but match on the whole thing?</em></p>
<p>To do this you can use <code>@</code> in front of the pattern, known informally as the
&quot;as-pattern&quot;. As of this writing, binding both the whole pattern plus parts of
the pattern isn't allowed.</p>
<pre><code>#[derive(Debug)]
struct S {
    field: i32,
}

pub fn main() {
    let s = S { field: 42 };
    match s {
        S { field: x @ 10..=100, } =&gt; println!(&quot;{:?}&quot;, x),
        S { field } =&gt; println!(&quot;{}&quot;, field),
    };
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=c0afcb3477a7e0b66e2aaa1a1912abf8">Playground</a></p>
<p><em>What if you don't want to specify literals or bind to variables?</em></p>
<p>If you want to do more complicated checking on bound variables, you can use a
match guard. A guard is introduced with an <code>if</code> after the pattern, but before
the fat arrow <code>=&gt;</code>, and the resulting value must be a boolean value, as would be
the case for other conditionals. You can't use guards on <code>let</code> and function
argument patterns.</p>
<pre><code>#[derive(Debug)]
struct S {
    field: i32,
}

pub fn main() {
    let s = S { field: 42 };
    match s {
        S { field } if field % 2 == 0 =&gt; println!(&quot;only executes when field is even&quot;),
        _ =&gt; println!(&quot;all remaining values go here&quot;),
    };
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=41ed619c36361f7c9481df2c2c6b9ccc">Playground</a></p>
<p><em>What about cases where you might want to combine several possible patterns into
one match arm?</em></p>
<p>You can combine patterns using what is known as an <code>or-pattern</code> by using a <code>|</code>
to try several patterns in a row. This way you can compress several patterns
into one match arm.</p>
<pre><code>enum Enum {
    A,
    B,
}

pub fn main() {
    let x = Enum::A;
    match x {
        Enum::A | Enum::B =&gt; println!(&quot;matches&quot;),
    };

    // or possibly in an if/while-let pattern match.
    let x = Some(12);
    if let Some(13) | Some(12) = x {
        println!(&quot;works&quot;);
    }
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3727a37c698f041316e3b382b11fb3ab">Playground</a></p>
<p><em>What if I want to check a pattern but I don't want all of the machinery of a
<code>match</code> statement?</em></p>
<p>The <code>matches!</code> macro lets us write a test to see if a supplied pattern will
match a given value. The macro doesn't allow you to bind values, but it can
allow you to extend a pattern using guards which is another handy use I've found
for it (see the quirks later for more details on a precise application).</p>
<pre><code>pub fn main() {
    assert_eq!(matches!(12, std::i32::MIN..=100), true);
    assert_eq!(matches!(None, Some(42)), false);
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=f523c33e7bf8b893a4253b41c78eb7b3">Playground</a></p>
<h3>Conclusion</h3>
<p>And now you know the bulk there is to pattern matching in Rust! As a recap,
patterns are tested on a value, and if they line up, they will execute some branch of
logic or bind some values to identifiers, or both! You can check
complicated logic with guards, ignore portions of patterns with wildcards, bind
whole matches with as-patterns, combine patterns with or-patterns, and
test for pattern matches with the <code>matches!</code> macro. You also can use patterns in
a number of places outside of <code>match</code> and the relevant control flow expressions
such as in <code>let</code> bindings and function arguments.</p>
<h3>Quirks</h3>
<p>These quirks are more around ergonomic uses of patterns rather than any
dealbreakers for writing production-grade code. You can happily skip this
section if you are still processing the information from above.</p>
<p>First up, nested or-patterns or in other locations, such as function arguments,
are unstable and require the <code>#![feature(or_patterns)]</code> attribute. Another way
around the nested or-patterns is to use the <code>matches!</code> macro in a guard:</p>
<pre><code>#[derive(Debug)]
struct Container(Possibly);

#[derive(Debug)]
enum Possibly {
    A,
    B,
}

fn main() {
    let container = Container(Possibly::A);
    match container {
        // Container(Possibly::A | Possibly::B) =&gt; // won't work
        Container(inner) if matches!(inner, Possibly::A | Possibly::B) =&gt; {
            dbg!(inner);
        }
        _ =&gt; {
            dbg!(&quot;won't happen unless Possibly changes&quot;);
        }
    };
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3c204e9bed56ff5c292d1d84e80226bb">Playground</a></p>
<p>Exclusive ranges for matching against numbers that aren't literals can be
enabled with <code>#![feature(exclusive_range_pattern)]</code>. As it stands, you can
only express inclusive ranges:</p>
<pre><code>fn main() {
    let std::i32::MIN..=std::i32::MAX = 12; // works.
    //let std::i32::MIN..std::i32::MAX = 12; // refuses to compile.
}
</code></pre>
<p>And lastly, bindings after <code>@</code> aren't supported unless you turn them on with
<code>#![feature(bindings_after_at)]</code>. This is a bit tricky anyway given ownership
and borrowing semantics and how that plays into binding both the top-level value
and the values inside of them.</p>
<pre><code>#![feature(bindings_after_at)]

#[derive(Debug)]
struct S {
    field: (i32, i32),
}

fn main() {
    let x = S { field: (1, 2) };
    match x {
        S {
            field: tuple @ (ref a, ref b),
        } =&gt; println!(&quot;{:?}, {} + {} = {}&quot;, tuple, a, b, a + b),
    }
}
</code></pre>
<p><a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=2b605ec39ed4884bb4ab92b5c3cc69bc">Playground</a></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Benchmark It!</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/benchmark-it.html</link>
      <guid>https://justanotherdot.com/posts/benchmark-it.html</guid>
      <pubDate>Sat, 09 May 2020 15:22:46 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Performance is something users <em>feel</em>. Maybe you're in an organization that
doesn't care about performance, and you think they should. Perhaps there are
lofty claims that performance <em>does</em> matter, but loose arguments for one
approach over another filled with baseless claims like &quot;X is <em>fast</em>!&quot;. The only
way to get past this conjecture is with numbers, and we collect numbers with
benchmarks.</p>
<p><strong>Benchmarking</strong> is the act of measuring latency or throughput of some component
regardless of size. <strong>Profiling</strong> is a means to explore the constituent parts of
where code spends its time. There is a dizzying array of variables we can tweak
to impact the performance of user experience on modern platforms, but benchmarks
should come first before we begin profiling. With benchmarks, we get
reproducibility, enabling others to test our claims and verify our results for
their use cases. As we generate measurements for targets, we can store the
results away so we can visualize the data however we please or compare
particular runs tied to specific code changes. In this sense, benchmarks provie
us a historical context of performance.</p>
<p><strong>Latency</strong> is the duration of time the component in question takes to complete.
<strong>Throughput</strong>, on the other hand, is the amount of work completed in a window of
time. <strong>You cannot trust the first number you get out of a single trial.</strong> Hence, we
focus on arithmetic mean averages or medians from multiple measurements. A
server might have an average latency of 123 milliseconds, or a JSON parser might
boast an average of 2.6GiB processed per second.</p>
<p>Cargo does have built-in support for benchmarking, and it works in a pinch but
hides the influence of outliers on your results. This is handy if you want
something you can quickly run and glance at to make improvements, but tests are
<strong>reliable</strong> if they are low on noise.</p>
<p>To gain this insight, I strongly recommend
<a href="https://github.com/bheisler/criterion.rs">criterion.rs</a>. It does all the things
<code>cargo-bench</code> does, along with providing additional statistical metrics to help
us determine if a benchmark is worth trusting.</p>
<p>By default, criterion takes one-hundred measurements. It then tries to find a
line that fits these measurements using linear regression. How well this line
fits the data is designated by the metric <strong>R^2</strong>. The <strong>slope</strong> of this line,
along with the <strong>mean</strong> and <strong>median</strong>, offer ways of viewing &quot;center&quot; for the
data. If there are no outliers, then slope, mean, and median should be similar.
<strong>Standard deviation</strong> is the dispersal of values around the mean, and <strong>MAD or
Median Absolute Deviation</strong> is the same for the median. A high standard
deviation or MAD might indicate a higher than expected level of noise.
Similarly, if the R^2 is low, then the difference between timings is high. To
get reliable tests, we want each iteration to be the same. There is bound to be
<em>some</em> noise and differences between runs, but we are trying to find the values
we are confident aren't merely aberrations.</p>
<p>You can add it to a project by adding the dependency to your Cargo.toml:</p>
<pre><code>[dev-dependencies]
criterion = &quot;0.3&quot;
</code></pre>
<p>Then, in the same file we can add the benchmark:</p>
<pre><code>[[bench]]
name = &quot;benchmark_it&quot;
harness = false
</code></pre>
<p>benchmarks live in distinct files under the <code>benches</code> directory in the root of
your project, named the same as the name we gave in the manifest:</p>
<pre><code>$ fd benchmark_it
benches/benchmark_it.rs
</code></pre>
<p>Imagine our crate is called <code>crate</code> and it exposes a public function named
<code>function</code> that we want to measure, the most basic benchmark looks something
like the following:</p>
<pre><code>use criterion::{black_box, criterion_group, criterion_main, Criterion};
use crate::function;

pub fn criterion_benchmark(c: &amp;mut Criterion) {
    c.bench_function(&quot;benchmark_function&quot;, |b| b.iter(|| function(black_box(20))));
}

criterion_group!(benches, criterion_benchmark);
criterion_main!(benches);
</code></pre>
<p>and you can run this benchmark with <code>cargo bench</code>. After the tests run you get
output on the command line. If you have gnuplot installed you can also view
an HTML report generated at <code>target/criterion/report/index.html</code> you can open in
a browser. If you want access to the raw data, you can find that dumped as
CSVs under <code>target/criterion/benchmark_function/{base,change,new}/raw.csv</code>. As
you run the benchmarks, the base gets replaced with the last latest run, and the
latest run is compared to visualize improvements or regressions.</p>
<p>Criterion's reports include explanations in reports, along with <a href="https://bheisler.github.io/criterion.rs/book/index.html">fantastic
documentation</a>
outlining how the test harness works and how the statistics are calculated,
including diagnostics in the output about when different outliers are detected.</p>
<p>To further reduce noise, you want to run on a machine that matches your target
environment as much as possible, which depends on your domain in question. If
you are running a server on a specific configuration, test the code on that
hardware with that configuration. If you are building a CLI tool used by
developers, it might make sense to have many benchmarks from commodity hardware
that developers are using, such as laptops with little to nothing else running
on the system. We might also want to benchmark on the fastest, quality hardware
we can find to determine limits of what you could hope to attain given empirical
results.</p>
<p>Benchmarks can explain performance under various loads. Input now comes into the
picture. If you are a data-regulation compliant company, and I hope you are
self-compliant if not, then generating data that has the same characteristics
and cardinality of what you tend to expect is vital to feed into your benchmarks
as expected &quot;normal load&quot; under the system. You also want to try to record
pathological cases where the system is exceptionally slow under particular
input. This isn't to say these pathological cases need to be your primary target
for the profiling and optimization that comes later. You might want to improve
the life of 99% of your users rather than worrying about an edge case that
happens 0.001% of the time. However, pathological cases still give us insight
into the limits of the component under measurement.</p>
<p>Independent of where you store your benchmarks, having them recorded for every
commit, or possibly every master commit, can let you easily compare two changes
using something like Andrew Gallant's tool
<a href="https://github.com/BurntSushi/critcmp">cargo-critcmp</a>. If you have
different hardware to test, you can script checking out changes, running the
benchmarks, and comparing the results. When making comparisons, make sure to
minimize variables of change across the various measurements! You don't want one
to have programs running in the background while the other was on a totally
silent system, for example.</p>
<p>Andy Gavin talks about how they <a href="https://www.youtube.com/watch?v=izxXGuVL21o&amp;feature=youtu.be&amp;t=513">benchmarked the various display
modes</a> for
the Sony Playstation during the making of Crash Bandicoot. He remarks how if he
had not done this and taken the recommended mode at face value, it would have
been subpar for their situation! This is precisely the kind of speculation that
measurements help dispel. <strong>Performance matters and numbers help make
performance tangible.</strong> Write benchmarks that work for <em>your</em> data and <em>your</em>
setup! Arm yourself to the teeth with numbers and ensure they are <em>valid</em>
numbers to be confident in your fight against lofty claims. <em>Valid</em> measurements
are useful to the community, whether it's your local team or the open-source
community as a whole. Write more benchmarks!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Magnifying Glasses for Rust Assembly</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/magnifying-glasses-for-rust-assembly.html</link>
      <guid>https://justanotherdot.com/posts/magnifying-glasses-for-rust-assembly.html</guid>
      <pubDate>Sun, 03 May 2020 14:40:08 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Compilers are complicated beasts. Our high-level source code goes through many
transformations until it winds up becoming machine code that runs on real or
virtual hardware. Assembly is the final destination before machine code, and it
doesn't have to be menacing! <strong>Whether you intend to write assembly directly or
not, knowing how your code translates to assembly can drastically improve your
ability to analyze programs from the standpoint of performance.</strong></p>
<p>Yes, we need numbers to guide us towards improvements, and yes, that means
having benchmarks. <strong>Arguments over performance that don't include data are
conjecture but understanding assembly gives you a magnifying glass to help guide
you in your optimization adventures.</strong> With some experience, we can learn how to
look at assembly and determine such things as whether or not the assembly
contains efficient instructions, chunks of code are replaced with constant
values, and so on. Benchmarks and analyzing assembly can go hand in hand, but
how do you even get at the assembly in the first place?</p>
<p>If you want to look at Rust's assembly in your project using just <code>cargo</code>, there
are two ways. You can call</p>
<pre><code>$ cargo rustc --release -- --emit asm &lt;ARGS&gt;
</code></pre>
<p><code>--release</code> is optional here. The primary argument that's needed is <code>--emit asm</code>. <code>ARGS</code> is the list of arguments you want to pass to <code>rustc</code> that might
influence compilation. By default, <code>rustc</code> generates AT&amp;T syntax. Still, you can
change to Intel syntax if that's what you prefer by passing <code>-C llvm-args=--x86-asm-syntax=intel</code>, which may not matter to you if this is your
first foray into analyzing assembly, but it can be fun to see as an experiment!</p>
<p>If you want a good starting point for flags, try using:</p>
<pre><code>&lt;snip&gt;
-C target-cpu=native -C opt-level=3
&lt;snip&gt;
</code></pre>
<p>These two codegen options instruct the compiler to emit code specifically for
the processor it guesses you are running the compiler on as well as using all
optimizations. You can also pass <code>opt-level=z</code> or <code>opt-level=s</code> if you want to
optimize for total disk space, instead. <strong>As a note, fewer instructions doesn't
necessarily mean efficient code.</strong> A short set of instructions may end up taking
more cycles than the more verbose alternative.</p>
<p>If, instead, you want to call the standard <code>cargo build</code>, you can pass all these
arguments with the <code>RUSTFLAGS</code> environment variable. For example:</p>
<pre><code>$ RUSTFLAGS=&quot;--emit asm -C opt-level=3 -C target-cpu=native&quot; cargo build --release
</code></pre>
<p>When the build finishes, the assembly will live in a file with the suffix of<code>.s</code>
under <code>target/debug/deps/CRATE_NAME-HASH.s</code>
or<code>target/release/deps/CRATE_NAME-HASH.s</code>, depending on whether or not you
builtwith the <code>--release</code> flag. If I run the above command on a crate with the
name<code>project</code> I'll get something like the following:</p>
<pre><code>$ find . -name &quot;*.s&quot; -type f
./target/release/deps/project-1693e028130a9fa3.s
</code></pre>
<p>Keep in mind that there may be several of these outputs. If you are confused,
which is the latest, you can try <code>cargo clean</code> and building fresh. By default,
the names are going to look pretty weird in the output due to mangling!
<strong>Mangling</strong> ensures that names for identifiers are unique across the process of
compilation. You can try feeding the resulting assembly into
<a href="https://github.com/luser/rustfilt">rustfilt</a> to get cleaner names:</p>
<pre><code>$ find . -name &quot;*.s&quot; -type f | xargs cat | rustfilt
</code></pre>
<p>Ok, this is great if you have a project going, but maybe you have some transient
code in the <a href="https://play.rust-lang.org/">Rust playground</a> and want to know what
the assembly is there. You can emit assembly there, too! If you click on the
ellipses next to the <code>Run</code> button, you'll get a menu that has several options.
Select <code>ASM</code> for assembly output in another tab. There isn't much control over
compilation options with the Rust playground approach besides picking stable,
beta, or nightly. A more fully-featured web version for picking apart assembly
is <a href="https://godbolt.org/">godbolt</a>, describes itself as a &quot;compiler explorer&quot;
and provides a <em>lot</em> of features to aid you in exploration over the above
bare-bones approaches. Advantage of using godbolt include:</p>
<ul>
<li>Viewing highlighted segments of our source code and where they line up to the assembly</li>
<li>Access to a bevy of compilers from a wide variety of languages, even  selecting which version of Rust you want to use</li>
<li>Passing arbitrary flags to influence how the generated output is produced</li>
<li>Diffing changes in assembly between source code assembly</li>
<li>Looking up the documentation for instructions on the fly</li>
</ul>
<p>You now know three ways to emit assembly, whether it's on your machine, the Rust
playground, or godbolt! To the uninitiated, this can be overwhelming, but
opening the hood can be liberating and allow us to start exploring the various
instructions and how they all tie together.</p>
<p>To reiterate, you don't always have to look at assembly to guide performance
optimization. Benchmarks are crucial at guiding us towards real-world results.
Try to make it a habit to look at assembly when you're curious about what's
going on under the hood. If you start optimizing, it can be interesting to
compare how assembly changes as you make high-level changes. If things seem to
speed up, try to explore how the assembly itself has changed!</p>
<p><em><strong>Update May 4 2020, 2:12PM</strong></em></p>
<p><code>u/ibeforeyou</code> on
<a href="https://www.reddit.com/r/rust/comments/gd1wtd/magnifying_glasses_for_rust_assembly/fpf4grv/">Reddit</a>
mentioned <a href="https://github.com/gnzlbg/cargo-asm"><code>cargo-asm</code></a> to help alleviate a
lot of the pain of dumping out the raw assembly above with <code>cargo</code>. By default,
it will produce Intel syntax, and it can even overlay the rust code over the
lines of assembly. The twist is that you need to give a path to the assembly you
want to see dumped. If you want to see function <code>foo</code> of the crate <code>crate_name</code>,
you could specify the path:</p>
<pre><code>$ cargo asm --rust crate_name::foo
</code></pre>
<p>I did have to shuffle around the flags to get it to emit AT&amp;T syntax for me, in
the end, this ended up working:</p>
<pre><code>$ cargo asm --rust --asm-style att crate_name::foo
</code></pre>
<p>Running <code>cargo asm</code> dumps all the available paths that you can list, which is
pretty neat if you're confused about which path to put down. What I like about
this is you can jam it into a <a href="https://www.justanotherdot.com/posts/a-love-letter-to-feedback-loops.html">feedback
loop</a>
using something like <code>cargo-watch</code> or <code>entr</code>. This way you can make changes on
an individual function and watch how the benchmarks and assembly change without
having to invoke commands manually!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Cut Down On Clones With Cows</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/cut-down-on-clones-with-cows.html</link>
      <guid>https://justanotherdot.com/posts/cut-down-on-clones-with-cows.html</guid>
      <pubDate>Wed, 29 Apr 2020 18:59:34 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>At the start of a program, it is straightforward to <code>clone</code> data all over the
place to get things working, and, soon enough, the program is overrun by them.
Switching away from clones can be hard because it requires fighting with the
borrow checker, and <a href="https://www.justanotherdot.com/posts/four-ways-to-avoid-the-wrath-of-the-borrow-checker.html">alternative
solutions</a>
aren't quite right for the job. How do you cut down allocations from cloning as
if you were borrowing without winding up in borrow hell? Consider using a Cow.</p>
<p><code>Cow</code> stands for <strong>C</strong>lone <strong>o</strong>n <strong>Write</strong> and they are underrated for what
they can do in this regard. On their own <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ceab3b70e17fc69d254404ae3357d0b4">cows are usually larger than their
owned or borrowed
variants</a>,
but cutting down careless memory allocations may help improve performance.</p>
<p>Use a <code>Cow</code> when there is data allocated outside of a function or block, and
there is some runtime logic that determines whether a write takes place. <strong>Cows
are a useful mechanism for transferring ownership lazily by cloning data once
and only once</strong>.</p>
<p>Cows are like hybrids such that at runtime, they might be borrowing data that's
already been around, or they may be handing out borrows to an owned piece of
data that <em>they</em> own.</p>
<p>Consider a function that replaces values in a string that we already have
allocated outside of the function. Replacing characters might mean changing the
size of the string or it could be a case of soft failure where we replace
invalid characters with the unicode replacement character
<a href="https://doc.rust-lang.org/std/char/constant.REPLACEMENT_CHARACTER.html">U+FFFD</a>,
as is the case for
<a href="https://doc.rust-lang.org/std/string/struct.String.html#method.from_utf8_lossy"><code>String::from_utf8_lossy</code></a>.
<strong>We should strive to recycle what values are already hanging around if we can
help it</strong>. We can recycle in other ways, such as taking a reference to a default
rather than assuming it must always be allocated on the fly, or having a
collection lazily clone and take ownership of values as it needs to rather than
cloning the collection from the start. With a bit of imagination, there are
several places that Cows can be used to improve performance and cut down on
clones.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Untangle Your Errors With Enums</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/untangle-your-errors-with-enums.html</link>
      <guid>https://justanotherdot.com/posts/untangle-your-errors-with-enums.html</guid>
      <pubDate>Wed, 22 Apr 2020 21:04:16 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Do you find it far too easy to reach for panics or shoehorn pre-existing errors
to fit your needs? Is it unsatisfying that there are no exceptions in Rust and
challenging to adjust to handling errors with <code>Result</code>? Here is a fundamental
method for modeling data that will help untangle error handling in your
programs.</p>
<p>Programs laden with <code>unwrap</code>, <code>expect</code>, <code>assert</code>, and <code>panic</code> are quick to gain
momentum, but this approach is clunky. For those coming from languages where
exceptions are the norm for error handling, it can feel natural to reach for,
but also awkward to use, something as blunt as a panic. Exceptions have handlers
registered, whereas panics do not, which is the primary difference between the
two.</p>
<p>Panics are for critical situations where a program has no other option but to
commit suicide. These vital situations are why capturing a panic in Rust carries
a stigma. Recovering from a panic depends on how the panic unwinds or aborts,
which is not always under our control.</p>
<p>Handle errors in Rust with <code>Result</code>. However, for newcomers, it's not apparent
how to best design error types. <code>Result&lt;A, B&gt;</code> implies that <code>B</code> could be
anything and we don't want to put <em>anything</em> in there; ad hoc matching of
strings or cross-referencing integers for errors is the pits.</p>
<p>The problem with strings, integers, and other unrefined types is that the range
of values you can express with them is <em>vast</em>, and when it comes to errors, we
want to categorize them neatly, so the range of things we can express is
<em>concise</em>. Unstructured data is hard to check against, parse by a machine, and
find in a codebase. If you do not want to be caught in molasses later in your
project, error handling brevity and classification matter a lot.</p>
<p>Enter the endlessly useful enum. <strong>The beauty of enums is that we can refine
and, therefore, narrow down the range of things we can express</strong>. Enums expose a
handle other coders can rely on, whether they be consumers of your crate or
internal maintainers. Enums optimize for categorization and aggregation, which
makes errors easy to find in code.</p>
<p>To start, create a bare-bones <code>error</code> module in your project with a top-level
<code>Error</code> enum. I'll put some things in here for demonstration purposes, but I'm
sure you can extrapolate for your own needs:</p>
<pre><code>use std::fmt::Display;

mod error {
  pub enum Error {
    IoError(std::io::Error),
    SerdeError(serde_json::Error),
    // ... and so on.
  }

  impl Display {
    // display implementations for each variant.
  }
}
</code></pre>
<p>Once we have this top-level <code>Error</code>, keep pushing; Maybe <code>Error</code> is too much of a
grab bag. <a href="https://www.justanotherdot.com/posts/make-your-errors-clearer-by-splitting-them-in-half.html">Keep clarifying your error
types</a>.
In the above example, we only expressed external errors but we will inevitably
need to express errors relating to our concerns. I'll extend our top-level
error and even grow some new ones:</p>
<pre><code>use std::fmt::Display;
use crate::token::Token;

mod error {
  pub enum Error {
    pub Vendor(VendorError),
    pub StdError(StdError),
    pub Internal(InternalError),
  }

  pub enum InternalError {
    pub Lex(LexError),
  }

  pub struct LexError {
    pub path: Path,
    pub line: i64,
    pub column: i64,
    pub token: Token,
  }

  pub enum VendorError {
    pub SerdeError(serde_json::Error),
    // ... and so on.
  }

  pub enum StdError {
    pub IoError(std::io::Error),
    // ... and so on.
  }

  impl Display {
    // display implementations for each variant.
  }
}
</code></pre>
<p>From such small beginnings we have grown a relatively comprehensive error type
to use in a variety of situations for our program or library. With all of this
in place there is little reason to turn to a panic. The astute observer noted
that we had <code>Display</code> impls laying around. I've structured the output in the
&quot;NASA&quot; style of error reporting, showing a 'stack' of errors. Each layer of the
classification above might have nested descriptions with colons or some other
nested format, for example:</p>
<pre><code>&lt;snip&gt;
  //  top level, we start with [foo] to help describe things.
  impl Display for Error {
    fn fmt(error: Error, f: Formatter) -&gt; {
      match error {
        Error::Vendor(ve) =&gt; fmt.write(&quot;vendor: {}&quot;, ve),
        Error::StdError(se) =&gt; fmt.write(&quot;stdlib:  {}&quot;, se),
        Error::Internal(ie) =&gt; fmt.write(&quot;internal: {}&quot;, ie),
      }
    }
  }

  impl Display for InternalError {
    fn fmt(error: InternalError, f: Formatter) -&gt; {
      match error {
        Internal::Lex(e) =&gt; fmt.write(&quot;could not lex source: {}&quot;, e),
      }
    }
  }

  impl Display for LexError {
    fn fmt(le: LexError, f: Formatter) -&gt; {
      fmt.write(&quot;unrecognized token `{}' in {}:{}:{}&quot;, le.token, le.path, le.line, le.column),
    }
  }
&lt;snip&gt;
</code></pre>
<p>If we had a lexing error we could get a nice output like this:</p>
<pre><code>internal: could not lex source: unrecognized token `asdf' in src/main.rs:3:4
</code></pre>
<p>In a language with sum types, or as Rust calls them, enums, there's no excuse
not to use them liberally for data modeling of all kinds. Being meticulous about
how you design errors and using categorization as a guiding heuristic makes
error handling a snap rather than a grind.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Skip Unnecessary Allocations In Your Collections</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/skip-unnecessary-allocations-in-your-collections.html</link>
      <guid>https://justanotherdot.com/posts/skip-unnecessary-allocations-in-your-collections.html</guid>
      <pubDate>Tue, 14 Apr 2020 19:47:45 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Rust's standard library offers a lot of neat dynamically-sized data structures
for use in Rust programs. They are quite performant, but the allocations they
conduct behind the scenes to grow may add up and cause performance issues in
your programs.</p>
<p>Rust intentionally avoids costly uses of <code>new</code> in a program by having the
allocation be empty by default, including types outside of<code>std::collections</code>,
too, such as <code>String::new</code>.</p>
<p>The backing store usually grows with a doubling strategy, and the growth tends
to happen right as it is needed, as is the case for <code>Vec</code>, see
<a href="https://github.com/rust-lang/rust/blob/42abbd8878d3b67238f3611b0587c704ba94f39c/src/liballoc/raw_vec.rs#L462-L464">here</a>
and
<a href="https://github.com/rust-lang/rust/blob/42abbd8878d3b67238f3611b0587c704ba94f39c/src/liballoc/raw_vec.rs#L476-L540">here</a>
for references to code as of this writing, but it may not always be the same
story for other collections. I strongly encourage looking at the actual source
code for the standard library when you are curious. Rust uses the language of
<strong>capacity</strong> to designate the total possible amount of memory the backing store
has room for and <strong>length</strong> to designate the total number of actual values in
the data structure.</p>
<p>One of the core tenants of optimization is to avoid doing needless work. Putting
data on the heap isn't necessarily expensive if you've already paid the price
upfront for allocating it. Doing work in this way is called <strong>amortization</strong>.
Imagine I have to store 4096 things in a vector. By default, the vector grows in
powers of two with capacities of 0, 2, 4, 8, 16, 32, 64, and so on, in that
order. That's already six allocations I've mentioned and not done reaching the
final size. Avoiding unnecessary work is at the heart of performance
optimization and these are intermediate steps are unnecessary!</p>
<p>A fantastic part of the Rust standard library collections is they tend to have
common interfaces precisely for this sort of thing! You can avoid these
allocations by using<code>with_capacity</code> if you know the value or upper bound you
need initially. If you already have the data structure hanging around, you can
also call <code>reserve</code> to request additional capacity to avoid needless allocation.</p>
<p>The way allocation happens with the doubling strategy <em>is</em> a form of
amortization. As the collection grows in powers of two, the number of calls
reduces, but the cost of growing the vector increases. Each time the vector
grows, it will copy all the values over to a new backing store. In general, any
time you think you can use a big chunk of data up front, you should allocate the
full capacity, but if the exact amount you are requesting is unknown, isn't that
a bit wasteful? An alternative strategy where the amount may only be partly
known is to request a large chunk of memory and size it down either with
<code>shrink_to_fit</code> or <code>resize</code>, but be careful with <code>resize</code> as it may truncate the
collection if you aren't careful!</p>
<p>It is always best to get empirical data on how to reasonably size the collection
upfront or while the program is running. If we instead take a chaotic approach
to allocations we may do more harm than good. At the end of the day, the reason
why these data structures grow on their own is to avoid thinking about them
<em>until</em> performance is an issue and we reveal that spending our time on this is
important through profiling.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Ways Wildcards Hide Bugs</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/ways-wildcards-hide-bugs.html</link>
      <guid>https://justanotherdot.com/posts/ways-wildcards-hide-bugs.html</guid>
      <pubDate>Thu, 09 Apr 2020 20:42:10 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>We call the wildcard variable, denoted by an underscore (<code>_</code>), the &quot;don't care&quot;
variable to throw away values we don't care to keep. Wildcards don't bind any
values, so wildcards have specific support in the language, as opposed to other
languages where an underscore may be yet another variable name.</p>
<p>I'll discuss three ways bugs can lurk innocently behind wildcards. Wildcards are useful, but reckless use of them can lead to bugs! I'll discuss three ways this can happen and how to be a bit more vigilant with their use. The general principle across these fixes is to think twice when you find yourself writing a wildcard. Ask if it is a value you want to ignore?</p>
<h3>Throwing away values</h3>
<p>The <code>;</code> without a <code>let</code> statement in Rust means &quot;turn this thing into a <code>()</code>because I want to cast the value into something that isn't an error but isn't a useful value.&quot; If you write a <code>Result</code> in Rust and don't propagate the value with <code>?</code> and don't assign the value to a variable name, say something like this:</p>
<pre><code>pub fn main() -&gt; Result&lt;(), std::io::Error&gt;  {
    std::fs::remove_file(&quot;/hello&quot;);
    Ok(())
}
</code></pre>
<p>rustc whinges, stating:</p>
<pre><code>   Compiling playground v0.0.1 (/playground)
warning: unused `std::result::Result` that must be used
 --&gt; src/main.rs:2:5
  |
2 |     std::fs::remove_file(&quot;/hello&quot;);
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |
  = note: `#[warn(unused_must_use)]` on by default
  = note: this `Result` may be an `Err` variant, which should be handled

    Finished dev [unoptimized + debuginfo] target(s) in 0.61s
     Running `target/debug/playground`
</code></pre>
<p>However, we can entirely silence this with a wildcard:</p>
<pre><code>pub fn main() -&gt; Result&lt;(), std::io::Error&gt;  {
    let _ = std::fs::remove_file(&quot;/hello&quot;);
    Ok(())
}
</code></pre>
<p>It may seem like you don't care about the value, but if the value represents a failure in any way, including, say, an <code>Option</code>, you want to ensure that value is not lost. Silencing any error generally means failures aren't be handled or reported, and that can mean things <em>seem</em> fine on the surface, but might be broken.</p>
<p>To give a production example of this, I recently fixed a test bug that used a database transaction but discarded its <code>Result</code>. It did this with the same sort of wildcard binding we saw above. As such, when it encountered any failure in the transaction and ignored it, the test became a false positive because nothing would panic. Two ways to fix this are to turn the enclosing test as one that returns <code>Result</code>, which you can do and is excellent, and the other is to bind the value and <code>unwrap</code>, <code>expect</code> or even do an <code>assert</code> on it given what sort of ergonomics and output you like.</p>
<h3>Using wildcard cases in a match carelessly</h3>
<p>Not everyone coming to Rust is used to match expressions. The anatomy of a match expression is pretty straightforward; you <code>match</code> on the term of interest, and each 'arm' is a case that we consider. Match statements are exhaustive, which means we either check every possible value we are matching against or we slap in a wildcard because we don't care or can't feasibly match on every possible value. When things have high-cardinality of the values they can represent, a wildcard might be funneling unexpected value into the wrong logic.
rustc is great at calling you out on this sort of stuff. Consider this code that
matches on an integer:</p>
<pre><code>pub fn main() {
    let x: i32 = 12;
    match x {
      y if y == 12 =&gt; x,
      y if y &lt; 12 =&gt; x,
      y if y &gt; 12 =&gt; x,
    };
}
</code></pre>
<p>Integers have many values, and we're trying to narrow down the selection to an exact match, values that are greater than the exact match, and values that are lesser than the exact match. However, rustc wants to make sure we've genuinely considered every angle:</p>
<pre><code>   Compiling playground v0.0.1 (/playground)
error[E0004]: non-exhaustive patterns: `_` not covered
 --&gt; src/main.rs:3:11
  |
3 |     match x {
  |           ^ pattern `_` not covered
  |
  = help: ensure that all possible cases are being handled, possibly by adding wildcards or more match arms

error: aborting due to previous error

For more information about this error, try `rustc --explain E0004`.
error: could not compile `playground`.

To learn more, run the command again with --verbose.
</code></pre>
<p>If you make the first match case <code>12 =&gt; x</code> rustc tells us that we aren't considering full ranges:</p>
<pre><code>   Compiling playground v0.0.1 (/playground)
error[E0004]: non-exhaustive patterns: `std::i32::MIN..=11i32` and `13i32..=std::i32::MAX` not covered
 --&gt; src/main.rs:3:11
  |
3 |     match x {
  |           ^ patterns `std::i32::MIN..=11i32` and `13i32..=std::i32::MAX` not covered
  |
  = help: ensure that all possible cases are being handled, possibly by adding wildcards or more match arms

error: aborting due to previous error

For more information about this error, try `rustc --explain E0004`.
error: could not compile `playground`.

To learn more, run the command again with --verbose.
</code></pre>
<p>rustc is already nudging us towards a solution specifically here where guards aren't doing much for us by spelling out the exact patterns for us. With some changes, this builds cleanly:</p>
<pre><code>pub fn main() {
    let x: i32 = 12;
    match x {
      12 =&gt; x,
      std::i32::MIN..=11i32 =&gt; x,
      13i32..=std::i32::MAX =&gt; x,
    };
}
</code></pre>
<p>If possible, it's best to refine the type into something like a sum type (enum) so that we can match on exact variants. In our example above, if we were to <code>x.cmp(12i32)</code>, we'd have three cases to check for; <code>LessThan</code>, <code>MoreThan</code>, and <code>Eq</code>.</p>
<pre><code>use std::cmp::Ordering;

pub fn main() {
    let x: i32 = 12;
    match x.cmp(&amp;12i32) {
        Ordering::Equal =&gt; x,
        Ordering::Less =&gt; x,
        Ordering::Greater =&gt; x,
    };
}
</code></pre>
<p>If you still need to use a wildcard, a useful tool is to turn to property-based testing or fuzzing to ensure that the specific branches are covered. Property-based testing and fuzzing are ways to generate test input randomly. Many libraries have support for generating specific ranges of values, such that you could focus more on the &quot;black hole&quot; that the wildcard case is creating, which is &quot;everything that isn't the cases I <em>have</em> covered.&quot; For example, you might have three branches, where one of the branches is a wildcard. 10-20% could go into the first two known branches, and the remaining 60-80% could be left to generate data that isn't of those two known branches to shine a flashlight into the dark crevices. When values become known that have special treatment, you can add them to the branches of the <code>match</code> and adjust the percentages accordingly.</p>
<h3>Leaving arguments dormant in a function</h3>
<p>When we design public-facing interfaces, it can be tempting to try to keep them stable by softening some of their fields. One way to do this with functions is to treat arguments as &quot;unused&quot; by prefixing them with an underscore. The surface seems the same, but now the value isn't used.</p>
<p>Since the function ignores these values, callers may make incorrect assumptions about how arguments might change the output. Leaving arguments dormant is especially nefarious when the function is non-deterministic, and it can <em>feel</em> like the different arguments are making a change. We can't always trust that users of our code are going to, or even be able to, read the source code.</p>
<p>Ignoring arguments has a straightforward fix; don't ignore arguments. Favor deprecating the function for a new version as the alternative and signal new versions with some precise apparatus. This principle is the same across a lot of interface design: make a new thing and migrate over to it rather than trying to change the other thing in place. When you rig up the new version, you can delete the old one, but you need to keep the old one around until you finalize the transition. Changing things in place is fine if the consumption is still light, but the breathing room gained from doing it the immutable way far outweighs the 'ease' of trying to modify the pre-existing.</p>
<h3>Wildcards aren't bad!</h3>
<p>Don't get me wrong, wildcards <em>are</em> useful, but they are easy to abuse if you are new to them. Hopefully, these points come to mind next time you write a <code>match</code> expression or modify a function!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Make Your Errors Clearer By Splitting Them In Half</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/make-your-errors-clearer-by-splitting-them-in-half.html</link>
      <guid>https://justanotherdot.com/posts/make-your-errors-clearer-by-splitting-them-in-half.html</guid>
      <pubDate>Mon, 06 Apr 2020 20:26:18 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Are your errors type devolving into grab bags with varying degrees of
categorization? Frequently <em>who</em> is at fault is not clear, and that can be one
of the most ergonomically essential classifications. If a program makes it clear
that an error is due to a user mistake, an internal complication, or possibly a
bug, that dramatically eases the usability of the service/library/tool in
question.</p>
<p>Making this clear is trivial with a top-level enum. I'm going to pretend we an
interface with two sides in question; the program authors (the providers) and
the users of the program (the consumers). For simplicity, I've called these
<code>External</code> and <code>Internal</code>, but it could also be <code>Provider</code> and <code>Consumer</code> or
whatever makes the designation clear for your use case.</p>
<pre><code>mod error {
  pub enum ExternalError {
    // e.g. MalformedInput, MissingArguments, and so on.
  }

  pub enum InternalError {
    // e.g. IoError, SerdeError, and so on.
  }

  pub enum Error {
    External(ExternalError),
    Internal(InternalError),
  }
}
</code></pre>
<p>A sum type lets you easily pattern match and analyze the error itself
without fickle operations or messy validation logic. Sum types (enums) are
fantastic, and you should be looking for ways to leverage them whenever
possible.</p>
<p>A mental model for this I like is thinking every error has an owner. Then you
can write functions that have clear offenders. Then, by signature, you are
assured that a module only deals with internally related failures or externally
associated concerns.</p>
<p>I love this approach because when you get to debugging, you can quickly
ascertain if an error is from mishandling, some operational concern, or perhaps
a bug. The user sees each diagnostic without any ambiguity to who is at fault.
In the same vein that a bug may be breaking an invariant, we might have an
<code>Invariant</code> case, which stipulates that an invariant has been breached, without
necessarily having to reach for assertions, or hoping that <code>debug_assertion</code>s
will fire in tests. And by all means, if there are more than two offenders, it
is best to define them clearly!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How to pick stable, beta, or nightly Rust</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/how-to-pick-stable-beta-or-nightly-rust.html</link>
      <guid>https://justanotherdot.com/posts/how-to-pick-stable-beta-or-nightly-rust.html</guid>
      <pubDate>Thu, 02 Apr 2020 20:04:29 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It would seem natural to always pick stable Rust, but how much awesome new
stuff do beta and nightly have and how unstable are they? It can be confusing
that such a plethora of feature flags sits in nightly but we don't want to
sacrifice stability.</p>
<p>Stability is a guarantee that something won't change. With that said, unstable
features theoretically have no guarantees, but in practice there is generally a
modicum of acceptable change and stability in nightly releases for most
purposes.</p>
<p>Before we begin, I've condensed my thought process into a simple diagram:</p>
<figure>
  <img
    src="/assets/images/pick-rust-toolchain-flowchart.png"
    alt="a flowchart describing how to choose between rust toolchains"
    title="A flowchart for choosing between stable, beta, and nightly toolchains">
  </img>
</figure>
<p><strong>If you don't need anything specifically from nightly or beta, stable should be
your default option.</strong> Cargo is quite good at mentioning what features you can
possibly turn on to help guide you into nightly. If you want a full guide on all
current and prior unstable features you can check out the <a href="https://doc.rust-lang.org/unstable-book/index.html">unstable
book</a>.</p>
<p>A way to run nightly with a slightly increased sense of stability is to use a
pinned variant. Cargo supports finding <code>rust-toolchain</code> files (the <a href="https://github.com/rust-lang/rustup#the-toolchain-file">toolchain
file</a>, as it's called)
at the root of crates which specify the specific version to use when building
the project. You can pin a nightly with a date, so something like</p>
<pre><code>nightly-2020-01-01
</code></pre>
<p>will ensure the nightly released on January 1st, 2020 will be the toolchain
picked.</p>
<p>In my own experience, when I encounter a bug in a pinned nightly I am using, I
can usually bump the pinned version to the latest nightly and go on with my
life. Although nightly Rust is still a moving target but in my experience it is
a remarkably sturdy moving target! Having run Rust for work and personal uses,
I've used pinned nightlies in both cases to great effect.</p>
<p>What's the difference between beta and nightly? <code>beta</code> is the first step before
a stable release. Beta is continually improved as nightlies progress and
regressions and features are discovered. The flow goes from nightly, to beta, to
stable, as you can see here in the <a href="https://doc.rust-lang.org/book/appendix-07-nightly-rust.html">Rust Programming Language Book Appendix
G</a>. As stated in
the same appendix:</p>
<blockquote>
<p>Most Rust users do not use beta releases actively, but test against beta in
their CI system to help Rust discover possible regressions.</p>
</blockquote>
<p>Or, another way of putting it; if you use stable, having beta and nightly builds
can help point out failures to be raised with the Rust core team, i.e. beta
should do everything stable does, and more. In the same vein, nightly should do
everything beta does, and more, but with the caveat that unstable APIs are
subject to change. Technically, one could try stable, then go to a pinned beta,
then go to a pinned nightly if they really want to tracking changes to specific
features.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Catching panics in dependencies</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/catching-panics-in-dependencies.html</link>
      <guid>https://justanotherdot.com/posts/catching-panics-in-dependencies.html</guid>
      <pubDate>Fri, 27 Mar 2020 12:55:26 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Having crates panic on you feels random because the specific conditions that
trigger the panic may not seem clear. Having external crates bring down your
program is a pain, but there is currently no static analysis tool to help us
easily find panics in external crates. You can get pretty close with fuzzing,
though!</p>
<p>The format for fuzzing is generally:</p>
<ol>
<li>Get some random bytes</li>
<li>Shape them into the right shape needed for our interface</li>
<li>Run the interface with the random data and see if it blows up</li>
</ol>
<p>Some fuzzing libraries take the input that leads to a crash and continually mutates it
to find other cases where it might crash as well. I'm going to use <code>cargo fuzz</code>
to reproduce finding a panic in an external dependency. <a href="https://github.com/rust-num/num/issues/268">Here's a
case</a> taken from the <code>rust-fuzz</code>
organisations <a href="https://github.com/rust-fuzz/trophy-case">trophy case</a>. I'll use
<code>num</code> v0.1.31 which panics when parsing <code>BigInt</code>s as per the linked issue. I'll
add it to the <code>Cargo.toml</code> of our project:</p>
<pre><code>[dependencies]
num = &quot;=0.1.31&quot;
</code></pre>
<p>Then, I'll install <code>cargo fuzz</code>.</p>
<pre><code>cargo install cargo-fuzz
</code></pre>
<p>then in our project, we can initialize cargo fuzz.</p>
<pre><code>cargo fuzz init
</code></pre>
<p>Which gives me a single fuzz target which I can rename if I want. I'll rename it
to <code>parse.rs</code> so the name is <code>parse</code> when listed but you can call it anything
that fits. To do this I will change the <code>fuzz</code> subdirectories <code>Cargo.toml</code>. A
<code>[[bin]]</code> key is in there that designates what targets are available. Since
we're moving <code>fuzz_target_1</code> to <code>parse</code>, we have to do this in the TOML file
since the <code>cargo fuzz</code> subcommand doesn't have this ability, but we <em>can</em> use
the <code>cargo fuzz add</code> subcommand to add extra targets with custom names in the
future.</p>
<p>The new key should look like this</p>
<pre><code>[[bin]]
name = &quot;parse&quot;
path = &quot;fuzz_targets/parse.rs&quot;
</code></pre>
<p>with the rest of the <code>fuzz/Cargo.toml</code> left as-is.</p>
<p>Then we can write a simple case for our function. This can be a little tricky
because the fuzzer will hand us raw bytes and it's up to us to shape them into
the right format, whether that be a struct, i64, f32, and so on. For this case
I'll make a string from the random bytes and feed it into our project's <code>parse</code>
function:</p>
<pre><code>#![no_main]
use our_project::parse;
use libfuzzer_sys::fuzz_target;

fuzz_target!(|data: &amp;[u8]| {
    if let Ok(s) = std::str::from_utf8(data) {
        let _ = parse(&amp;s);
    }
});
</code></pre>
<p>the <code>parse</code> function might look something like this (lifted from the issue):</p>
<pre><code>use num::Num;

pub fn parse(str: &amp;str) {
    num::BigUint::from_str_radix(str, 10);
}
</code></pre>
<p>and then I'll run the fuzzer for this target:</p>
<pre><code>cargo fuzz run parse
</code></pre>
<p>This finds offending strings similar to the trophy case example quite quickly:</p>
<pre><code>&lt;snip&gt;
────────────────────────────────────────────────────────────────────────────────

Failing input:

        fuzz/artifacts/parse/crash-00a78c613a00b21ea723de12e16f32a0385d9bdc

Output of `std::fmt::Debug`:

        [48, 43, 49]

Reproduce with:

        cargo fuzz run parse fuzz/artifacts/parse/crash-00a78c613a00b21ea723de12e16f32a0385d9bdc

Minimize test case with:

        cargo fuzz tmin parse fuzz/artifacts/parse/crash-00a78c613a00b21ea723de12e16f32a0385d9bdc

────────────────────────────────────────────────────────────────────────────────

Error: Fuzz target exited with exit code: 77
</code></pre>
<p>Hang on, that debug output doesn't look like the offending case mentioned in the
issue we linked above from the trophy case, does it? Again, this is because it's
the raw bytes. A handy trick I use when running regressions is to utilize the
stored failing input. In this case it's stored at
<code>fuzz/artifacts/parse/crash-00a78c613a00b21ea723de12e16f32a0385d9bdc</code> per the
output above. We can write a regression that uses this directly with the macro
<code>include_bytes!</code>:</p>
<pre><code>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn fuzz_regression_01() {
        let data = include_bytes!(
            &quot;../fuzz/artifacts/parse/crash-00a78c613a00b21ea723de12e16f32a0385d9bdc&quot;
        );
        let s = std::str::from_utf8(data).expect(&quot;should be able to make test input&quot;);
        dbg!(&amp;s);
        parse(&amp;s);
    }
}
</code></pre>
<p>You can run the tests the usual way with <code>cargo test</code> which should panic. I've
lobbed a <code>dbg!</code> in there of the transformed raw bytes into the string. When the
test panics we see:</p>
<pre><code>running 1 test
test tests::fuzz_regression_01 ... FAILED

failures:

- tests::fuzz_regression_01 stdout -
[src/lib.rs:17] &amp;s = &quot;0+1&quot;
thread 'tests::fuzz_regression_01' panicked at 'called `Result::unwrap_err()` on an `Ok` value: 1', /home/rjs/.cargo/registry/src/github.com-1ecc6299db9ec823/num-0.1.31/src/bigint.rs:388:25
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

failures:
    tests::fuzz_regression_01

test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out

error: test failed, to rerun pass '--lib'
</code></pre>
<p>And there is our offending input, <code>&quot;0+1&quot;</code>! Beautiful. I can use these specific
cases in issues for upstream projects instead of wasting time trying to find
exact cases on my own. Switching to <code>num</code> v0.1.41 avoids the panic and we can
run <code>cargo fuzz</code> again. We can also change the fuzzing library used, such as
libfuzz or <a href="https://rust-fuzz.github.io/book/afl.html">afl</a>, which <code>cargo fuzz</code>
supports. You can also use <code>honggfuzz</code> via the <code>honggfuzz.rs</code> library over at
<a href="https://github.com/rust-fuzz/honggfuzz-rs">honggfuzz-rs</a>. Different fuzzers
have different features and guarantees but most are relatively easy to write
targets for and get fuzzing so it can pay to try a few alternatives to see if
other cases are lurking around.</p>
<p>Before I go, I want to talk about a related concept known as &quot;property based
testing&quot; where we define how random data should be generated. We'll dig more
into that another day but for now, it suffices to say that both approaches help
to drive out test cases you might not have imagined! I am in the habit of making
my own regression suites from cases I find from either method, but you don't
<em>have</em> to do this as some fuzzers and property-based testing libraries will keep
a &quot;corpus&quot; of data that has failed that it can try again on future runs, but I
find it helpful to have the regressions as unit tests so there is a fast way to
verify earlier failures aren't still happening.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How To Return An Iterator From a Function</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/how-to-return-an-iterator-from-a-function.html</link>
      <guid>https://justanotherdot.com/posts/how-to-return-an-iterator-from-a-function.html</guid>
      <pubDate>Thu, 19 Mar 2020 20:40:42 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Iterators are handy ways to describe the potential for looping over data but
without eagerly evaluating it. In other words, we can describe the shape of
looping over things but we only do work when we call something like <code>collect</code>.
The compiler can do several optimizations around iterators and each <code>collect</code>
needs to allocate memory to store our results. To avoid a lot of needless
allocations it pays to returns iterators sometimes from functions.</p>
<p>Alright, that's great but you're hitting a wall returning one from a function.
I'll provide two ways you can do just that:</p>
<p>We could use the <code>impl Trait</code> syntax where <code>Trait</code> is the trait in question we
want to return. In this case, we'd have:</p>
<pre><code>fn unboxed_iterator() -&gt; impl Iterator&lt;Item = usize&gt; {
  (0..3).into_iter()
}
</code></pre>
<p>This is where the compiler will determine the exact type that is being returned
and substitute that in. This approach, albeit with static dispatch, has its
limits. You can read Bodil Stokke's <a href="https://bodil.lol/parser-combinators/">wonderful introduction to parser
combinators</a> as an example where the
<code>impl Trait</code> approach starts to get too complex and require turning to our next
approach of <code>Box</code>ing the iterators. This static approach also doesn't work when
we have anonymous types, such as with async functions. Marking a function as
<code>async</code> has support from the compiler to return the right type.</p>
<p>We could also use a <code>Box</code>. It is the simplest way to package up an iterator but
at the cost of allocation. Anything behind a <code>Box</code> is allocated on the heap.</p>
<pre><code>fn boxed_iterator() -&gt; Box&lt;dyn Iterator&lt;Item = usize&gt;&gt; {
  Box::new((0..3).into_iter())
}
</code></pre>
<p>We have to pay the price of dereferencing a pointer each time we want to deal
with this specific boxed iterator. We can manipulate both iterators in the same
way because <code>Box</code> implements <code>Deref</code> which lets you access the methods
underneath. So we can call <code>map</code> and friends on the resulting <code>Box</code>. Both of
these forms can usually be used with one another, as, for example, we can chain
both together since they both can be turned <code>IntoIterator</code>s.</p>
<pre><code>fn unboxed_iterator() -&gt; impl Iterator&lt;Item = usize&gt; {
  (0..3).into_iter()
}

fn boxed_iterator() -&gt; Box&lt;dyn Iterator&lt;Item = usize&gt;&gt; {
  Box::new((0..3).into_iter())
}

fn main() {
  dbg!(boxed_iterator().chain(unboxed_iterator()).collect::&lt;Vec&lt;_&gt;&gt;());
}
</code></pre>
<p>The above prints:</p>
<pre><code>[src/main.rs:10] boxed_iterator().chain(unboxed_iterator()).collect::&lt;Vec&lt;_&gt;&gt;() = [
    0,
    1,
    2,
    0,
    1,
    2,
]
</code></pre>
<p>You preferably want to use the <code>impl Trait</code> approach as much as you can and
fall back to the <code>Box</code> approach when that starts to fail or become inordinately
slow to compile. With time this restriction may go away as work on the compiler
continues.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Working From Home Without Clawing At The Walls</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/working-from-home-without-clawing-at-the-walls.html</link>
      <guid>https://justanotherdot.com/posts/working-from-home-without-clawing-at-the-walls.html</guid>
      <pubDate>Mon, 16 Mar 2020 19:49:21 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Swaths of people are shifting into working from home, otherwise acronymized as
&quot;wfh&quot;, and for some, it's a drastic shift. For those with kids, the reduced
productivity will be a change even for those that are used to working from home.
I have a fair amount of experience working from home and I thought I'd share
some thoughts from my experience of key things that have helped keep me
productive but also, more importantly, sane. After all, long periods locked up
inside can easily drive one to cabin fever; how do all the introverts and remote
workers keep from space madness?</p>
<p>Welcome. You are now working from home. You have a computer, a brain, and a way
to link the two, and this is (for purposes of simplifying this article) largely
what you need to get your job done.</p>
<p>Parts of what I'm to recommend are peppered with caveats due to the nature of
why people are coming in droves to the practice of working remotely. Take what I
say with a grain of salt.</p>
<p><em>NB. I started writing this on a Sunday and became aware of Alice Goldfuss's
<a href="https://blog.alicegoldfuss.com/work-in-the-time-of-corona/">fantastic article</a>
on the same subject the Monday after. It is far more thorough and I strongly
recommend it.</em></p>
<ul>
<li>
<p>Wear your work clothes. This is going to set off the &quot;I'm wearing work
clothes, I'm going to work&quot; cells firing. Resist the urge to wear the same
clothes you wore last night post-shower and relaxation. There are a few points
here that are about separation</p>
</li>
<li>
<p>Get outside. If you have a morning routine, say with tea or coffee, it can
help to simply exit your house for a brief period in the morning. Getting some
time outside the confines of the walls of your home may seem inconsequential
but I've found it to have a profound impact on my psyche. Lots of these points
may seem like &quot;well of course!&quot; but the reality is that working from home
tends to get you into tunnel vision about work.</p>
</li>
<li>
<p>Dedicate space to where you work if you can. Productivity can dwindle, and
your ability to relax, if you mix and match places for use of work and
leisure. If you don't have the space to dedicate as an office you can always
pick strategic places in your house that are more for business than they are
for recreation. For example, it might be cool to work at the dining table or
the kitchen island but not in bed and on the couch.</p>
</li>
<li>
<p>Do a bit of exercise. Doing a small run or bike around the block, some yoga at
home, or doing pushups, situps, and jumping jacks throughout your day in small
spurts can add up. Lack of exercise can lead to a reduction in energy and
alertness but you don't need to be doing really heavy exercise to get back
into the habit.</p>
</li>
<li>
<p>Do a bit of stretching at regular intervals. I make this a separate point
because if you work at a computer like me, it's vital to remember to get up
and move around constantly and stretch out all the muscles attached your hands
are up in your back, neck, and so on. I also own a foam roller and massage
ball for my back and neck that I use for about ten minutes at the end of each
day.</p>
</li>
<li>
<p>If you work in a high-pace work environment, having lunch at the ready so you
don't have to cook it helps reduce stress. Ditto breakfast and dinner
although, in another point we'll cover, hours of work can sort of distort a
bit.</p>
</li>
<li>
<p>Depending on what you can do, if you do anything with a deep work effort, be
free with how you organize your day during the &quot;deep work&quot; period. There might
be the time where everyone catches up, perhaps with a standup or quick chat,
but the idea is that you should feel a bit freer to, say, read a bit for
fifteen minutes or take a bike ride. This is important because it breaks up
your day and time you reclaim from the commute shouldn't all be poured into
work. You need to structure the demarcation of you-time and work-time
responsibly because things like getting on the train won't be present.</p>
</li>
<li>
<p>If you are a social person, it can help to have things like podcasts and
audiobooks at the ready so you can get an influx of other humans talking into
your daily psyche diet. I call this the morning-television effect. When people
put on morning television it is more to create the presence of people than the
content they broadcast. It can pay to have a little bit of human communication
on a quick ten to fifteen-minute call per day either for work or with a
friend, too.</p>
</li>
<li>
<p>In some ways, we are basically like plants in that we need water, oxygen, and
light. Getting plenty of these in some form or another improves quality of
life while working from home. This is a bit more than the &quot;get out of the
house&quot; point because it can be done without leaving the home, although getting
out of the house also makes you feel a bit &quot;free&quot; from the confines of your
home as well. Physiological needs can be the first place to try to supplement
and repair when things are feeling funky at home.</p>
</li>
<li>
<p>When you feel flustered, shut off the computer for a bit. I know not everyone
can do this as some people's jobs, say in software operations, need to be
present for most of their clocked time when on duty. If you do have the
option, it is powerful to push your laptop away and go someplace with just
your journal. Once there, try to de-stress a bit and forget work. Once your
head is a bit clear, focus on whatever problem you are dealing with on paper
first. Being indoors is claustrophobic. Being indoors inside the mental space
of a computer is doubly so.</p>
</li>
</ul>
<p>A majority of these boil down to a couple of heuristics:</p>
<ol>
<li>Focus on your physiological needs when things seem off</li>
<li>Ensure you have routines and structure in place to keep your mind focused</li>
<li>Be flexible with your expectations on yourself and on your work,
particularly concerning where hours are dedicated</li>
</ol>
<p>As I write this it's a stressful period. Go easy on yourself and those around
you while everyone adjusts. I hope some points here help ease the adjustment.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Why Are There Two Types of Strings In Rust?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/why-are-there-two-types-of-strings-in-rust.html</link>
      <guid>https://justanotherdot.com/posts/why-are-there-two-types-of-strings-in-rust.html</guid>
      <pubDate>Sat, 14 Mar 2020 11:45:46 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Understanding the distinction between <code>str</code> and <code>String</code> can be painful if you
need to get something done in Rust <em>now</em>. Rust doesn't sugar coat a lot of the
ugliness and complexity of string handling from developers like other languages
do and therefore helps in avoiding critical mistakes in the future.</p>
<p>By construction, both string types are valid UTF-8. This ensures there are no
misbehaving strings in a program. A <code>char</code> is always four-bytes in Rust, but a
string doesn't have to be composed of just four-byte chunks (that would be a
UTF-32 encoding!). Being UTF-8 means that Strings can be encoded with
variable-width code points, but you can iterate across the <code>char</code>s if you want
without them being stored as such.</p>
<p>I'll cover the remaining difference between a <code>String</code> and a <code>str</code> through
arrays, vecs, and slices.</p>
<p>An array is a contiguous chunk of memory where every element is the same type
and adjacent. Arrays are, however, of a fixed size. If we want to actually grow
or shrink an array we can turn to a <code>Vec</code> which is sometimes known as a
&quot;resizable array&quot;. This type abstracts away the housekeeping around allocating
bigger or smaller arrays.</p>
<p>A vec grow as elements fill the backing memory near or at capacity. Without
getting too distracted, a vec doesn't quite use an array but it does use a
contiguous chunk of allocated memory that is similar to an array. Vecs also
shrink to size if requested. The perks of ownership in Rust mean we, the vec,
can do whatever we please to the data we own. We can always borrow owned things
to temporarily read or change data. Why do you need more?</p>
<p>A slice is a view into a portion, or <em>slice</em>, of owned, contiguous memory.
Whenever we have a slice we know we can access its elements safely without
exposing any elements outside of the portion described by the slice and without
copying any data over to a new owner. Slices give us the capacity to provide
entire views of the original data rather than just a segment.</p>
<p>This relationship between an owned piece of data and a view into an owned piece
of data is pervasive in Rust. Not every view may exclude access outside of its
elements but it may provide a copy-free access such as an <code>Entry</code> for a
<code>BTreeMap</code> or a <code>Cursor</code> to a <code>File</code>.</p>
<p>This is the same relationship between <code>String</code> and <code>str</code>. A <code>String</code> is the
<code>Vec</code> and <code>str</code> is the slice. Since a slice is its own type, we can borrow it to
change or read as we please. This is the difference between <code>str</code> and <code>&amp;str</code> in
that you will only ever manipulate a <code>&amp;str</code> but it's technically a borrowed
&quot;string slice&quot; <code>str</code>.</p>
<p>There is one bit of &quot;magic&quot; that Rust allows which is that taking a borrow to an
owned string to a function will cast it to a string slice for you.</p>
<pre><code>let s = String::new();
fn takes_a_string_slice(the_string: &amp;str) {
  // reads the_string.
}
takes_a_string_slice(&amp;s);
</code></pre>
<p>This is a convenience so that you don't have to describe the bounds as you would
for an array or vector slice, a la <code>&amp;xs[0..n]</code>, although you <em>can</em> use the same
syntax to create a slice into a portion of a string if you want.</p>
<p>As a final point, the backing store of a <code>String</code> is actually <code>Vec</code>; <code>String</code>
just brings along the requirement that the contents are valid UTF-8 and heaps of
convenience functions, as does <code>&amp;str</code>. A slice is what we commonly call a &quot;fat
pointer&quot; which consists of two machine words: one pointing to the start of data
and another dictating the length. In this sense casting between a slice and back
is cheap in the sense that we do not copy any data besides creating a fat
pointer which is possibly reused it when we borrow.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>When to move, copy, or clone?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/when-to-move-copy-or-clone.html</link>
      <guid>https://justanotherdot.com/posts/when-to-move-copy-or-clone.html</guid>
      <pubDate>Mon, 09 Mar 2020 20:35:28 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Do you understand ownership and borrowing in theory but find it hard in
practice? Do the differences between things like <code>iter</code> and <code>into_iter</code> still
confuse you? Maybe the difference between <code>Copy</code> and <code>Clone</code> is still unclear? I
will shed some light on practical examples that should help you gain a better
grip on owning and borrowing values.</p>
<p>As you may know, all values in Rust need an owner. <em>Owners are about
responsibility</em>; some resource, usually, but not always, memory, is allocated and
the responsibility for releasing it is up to the owner. Ownership, or rather,
responsibility, is only transferred on a move, hence borrowing, not counting
towards releasing resources, is a <em>view</em> into the data. Rust's borrowing rules
mimic the solution to the readers-writers problem of concurrency; there may be
any number of readers but no writers and only ever one writer and no readers.
These two states are the same as immutable borrows and mutable borrows,
respectively.</p>
<p>When we rebind values that aren't <code>Copy</code>, by default we use move semantics and
transfer ownership to the new identifier. However, if somthing is <code>Copy</code> this
action now performs a bit-wise copy of the contents. An <code>i64</code> that is
re-assigned to a new variable will be a bit-wise copy. Contrast this to <code>Clone</code>
where the copying is explicit with the call to <code>Clone</code>. <em>Thus both <code>Clone</code> and
<code>Copy</code> signify copying of some kind, whether cheap or expensive, but the choice
is dependent on when the copying is preferred.</em></p>
<h3>tl;dr</h3>
<ol>
<li>
<p>I borrow values to avoid producing new values. In other words, I re-use
values that are already hanging around so as not to be wasteful with
allocations.</p>
</li>
<li>
<p>I copy/clone based on how I want to reduce allocations in the face of needing
to duplicate data, picking to allocate automatically or explicitly respectively.</p>
</li>
<li>
<p>I own values when I want total control of the data in question. I like to
think of this as <em>data recycling</em>.</p>
</li>
</ol>
<p>Here's some examples centered around iterators to give you a sense in practice.</p>
<h3>When to borrow</h3>
<p>Firstly, we can make a Vec of references, since the owner still lives while the
references live we don't need to allocate any new data.</p>
<pre><code>struct Wrapper {
  id: i64,
}

let values: Vec&lt;Wrapper&gt; = vec![
    Wrapper { id: 1 },
    Wrapper { id: 2 },
    Wrapper { id: 3 },
];
let xs: Vec&lt;&amp;Wrapper&gt; = values.iter().collect();
</code></pre>
<h3>When to clone or copy</h3>
<p>Next up, we might want to keep two copies of our <code>values</code>, if we change the
<code>Wrapper</code> to derive <code>Clone</code> we can use <code>cloned</code> on our iterator which is
functionally the same as <code>.map(|x| x.clone())</code>:</p>
<pre><code>#[derive(Clone)]
struct Wrapper {
  id: i64,
}

let values: Vec&lt;Wrapper&gt; = vec![
    Wrapper { id: 1 },
    Wrapper { id: 2 },
    Wrapper { id: 3 },
];
let xs: Vec&lt;Wrapper&gt; = values.iter().cloned().collect();
</code></pre>
<p>By default, most primitives are <code>Copy</code> because it's easy enough and usually
performant for the compiler to bitwise copy them. Since our <code>Wrapper</code> type in
the previous examples is just wrapping up a primitive <code>i64</code> integer, we can make
it also derive <code>Copy</code>:</p>
<pre><code>#[derive(Copy)]
struct Wrapper {
  id: i64,
}

let values: Vec&lt;Wrapper&gt; = vec![
    Wrapper { id: 1 },
    Wrapper { id: 2 },
    Wrapper { id: 3 },
];
let xs: Vec&lt;Wrapper&gt; = values.iter().copied().collect();
</code></pre>
<p>You might use <code>copied</code> if you don't want to write <code>.map(|x| *x)</code> if you happen
to have a collection of borrowed values at your disposal (imagine you are passed
a <code>Vec&lt;&amp;Wrapper&gt;</code>), this could be handy. The same logic stands for <code>cloned</code>. The
case is a little bit different for <code>Copy</code>, though. If we can own the iterator
with <code>into_iter</code> then any move of the values will result in a bitwise copy. This
is why you will sometimes see the rust compiler complain that a value is moved
and doesn't implement the <code>Copy</code> trait: it can't make a copy for you and it also
can't re-use a moved value.</p>
<h3>When to own</h3>
<p>Ownership is the basis of why we don't need garbage collection in Rust. Passing
an owned value across several different method calls could make copies or pass
pointers depending on what optimizations the compiler wishes to perform, hence
they could be <code>memcpy</code>s or as copied pointers. Regardless of how they work under
the hood, they prevent a host of bugs by ensuring <em>only one thing</em> has the
responsibility of finalizing the release of memory.</p>
<p>Expecting owned values is a nice way to push the decision to allocate on the
caller. If the caller wants to keep the value it owns, it must clone the value
itself, instead of guessing if a clone is happening elsewhere. More importantly,
writing code that expects values to be owned exposes the intent that I want to
have full control over the memory to do as I please, rather than trying to work
around what may already exist. This is why anytime you want to transform
something from one shape to another and don't care much or at all about the
original shape, taking ownership is the right choice.</p>
<pre><code>#[derive(Copy)]
struct Wrapper {
  id: i64,
}

let values: Vec&lt;Wrapper&gt; = vec![
    Wrapper { id: 1 },
    Wrapper { id: 2 },
    Wrapper { id: 3 },
];
let values: Vec&lt;Wrapper&gt; = values.into_iter().collect();
</code></pre>
<p>Note how I re-assign <code>values</code> after the transformation; albeit not necessary, it
does let me think a bit less about re-naming the original binding that I can't
use anymore.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Four Ways To Avoid The Wrath Of The Borrow Checker</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/four-ways-to-avoid-the-wrath-of-the-borrow-checker.html</link>
      <guid>https://justanotherdot.com/posts/four-ways-to-avoid-the-wrath-of-the-borrow-checker.html</guid>
      <pubDate>Sun, 01 Mar 2020 17:07:25 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Maybe you've tried to write a simple program in Rust using references that would
normally have taken less than an hour in C only to find yourself <em>hours</em> later
still fussing about with the Rust compiler. If the borrow checker seems too
restricting, here are four ways to loosen its grip.</p>
<h3>Shared Ownership with <code>Arc</code> or <code>Rc</code></h3>
<p>Shared ownership is what most garbage collected languages support. This is done
using a reference counting to objects in memory. We can mimic this in Rust with
the wrapper type <code>Rc</code>. If you plan on reference counting in multi-threaded code
you can use <code>Arc</code> where the <code>A</code> stands for <code>Atomic</code>.</p>
<p>Passing around an <code>Rc</code> means that if someone wants to jointly own the data, they
can simply call <code>clone</code> on the <code>Rc</code>. This can be used as a drop-in replacement
for places where you would borrow. Since these reference bumps count as new
owners there is no borrowing at all. However, now that we can express shared
ownership we also express a <em>graph</em> and graphs can have cycles (place where
pointers loop back on themselves). A cycle in a graph means a value may never be
deallocated, hence any self-referential structure poses a memory leak in our
program. You can avoid this by having the pointer that &quot;ties the knot&quot; be a
<code>Weak</code> pointer, which just means it's a non-owning pointer. A classic example is
with a cache: you want to have entries in the cache to objects owned outside of
the cache but you don't want the entries to count towards owning anything,
otherwise keeping the cache around means keeping all of the memory! Also, <code>Rc</code>
means you won't be able to take mutable borrows to the contents. This is easily
remedied with the use of <code>Rc&lt;RefCell&lt;T&gt;&gt;</code> or even <code>RwLock</code> as we'll see later.</p>
<p>&quot;When in doubt, reference count&quot; is appropriate for places where laying out
borrows and static lifetimes can be a pain and you want to get things passing
quick. Places where you temporarily use an <code>Rc</code> can easily be targeted for
borrows, so going back to fix things is clear. It may take some jiggling to get
things into place but at least it can happen later down the line when you've got
the breathing room, perhaps.</p>
<p><code>Rc</code> can be particularly handy when you want to pass around function references
in all sorts of ways. I used <code>Rc</code> extensively when porting a functional library
from F# and Haskell directly into Rust and needed to easily get mutual recursion
working quickly where using direct references or owned trait generics (e.g. F:
Fn(A) -&gt; B). I was later able to swap out the calls to references, which meant
the ergonomics of the first call simplified to borrowing rather than wrapping
the closures in question in <code>Rc</code>s.</p>
<h3>Interior Mutability with Cell or RefCell</h3>
<p>Exterior mutability (otherwise known as &quot;inherited mutability&quot;) is great because
it lets us know what things are actually changing beneath us. But sometimes
clients don't care that some housekeeping state is changing underneath an
operation. Perhaps we memoize the result of a function or manipulate a counter;
in both of these cases, with exterior mutability, the function wrapping this
action would have to be marked as mutable in some way, but if we want to keep
things looking immutable on the surface, we can use <code>Cell</code> or <code>RefCell</code> instead.</p>
<p>To give a concrete example with memoization, you might have an expensive
computation that you only want to do once and stash the result. As such, you
have a function that only need to be mutable for this one time and can be
immutable the rest of the time, so it doesn't make sense to have it marked as
<code>mut</code>. Whatever the result type is, we can wrap that in a <code>Cell</code> or <code>RefCell</code>,
depending on type: <code>Cell</code> is generally for things that support <code>Copy</code> and
<code>RefCell</code> for the rest.</p>
<p>As <code>RefCell</code> uses dynamic borrow checking, it can panic if multiple mutable
borrows are taken to the contents. <code>Cell</code> doesn't suffer from the same issue as
it moves the values in and out of the internals of the <code>Cell</code>. As such, you may
want to use something like <code>RwLock</code> if you are using an <code>Rc&lt;RefCell&lt;T&gt;&gt;</code> for
multi-threaded code. <code>Rc&lt;RefCell&lt;T&gt;&gt;</code> is a common way to have a shared object,
such as a <code>HashMap</code>, across several owners, but still mutate it. If one used
<code>Rc::get_mut</code> one would need to mark the <code>HashMap</code> itself as <code>mut</code>.</p>
<h3>Duplicate the data</h3>
<p>Often people think that coming to Rust means programs should be completely
devoid of <code>clone</code> but if you think about the language you may be coming from,
whatever <code>clone</code>ing you are doing is in Rust probably pales in comparison.</p>
<p>You don't need to feel bound to a <code>clone</code>less program by default. By abandoning
this idea of a slim program from the outset and move towards something far more
flexible. This generally means having duplicate formats of a data structure for
varying purposes such as one be a game map where walls are located whereas
another could be where someone has explored and yet another could contain items
on each tile, etc. It can also mean having a duplicate you want to make changes
to, leaving the original in-tact. This is more of the pure approach functional
programming languages tend to take, but these languages can also make particular
optimizations around immutability such as persistent data structures or
&quot;sharing&quot; of data since <em>everything</em> is immutable by default. Here's an example
that having some duplication of data might help. Perhaps you are trying to
iterate over a collection and mutate it:</p>
<pre><code>let mut xs = vec![1,2,3];
for x in xs.iter_mut() {
  if x % 2 == 0 {
    xs.push(x+1);
  }
}
</code></pre>
<p>In fact, this fails because we are borrowing to <code>xs</code> mutably twice! Once when we
construct the iterator and another time when we push to the <code>Vec</code>. This is a
classic &quot;modify a data structure while you iterate over it&quot; issue. However, we
could easily do this:</p>
<pre><code>let mux xs = vec![1,2,3];
let ys = xs.clone();
for x in xs.iter() {
  if x % 2 == 0 {
    ys.push(x+1);
  }
}
</code></pre>
<p>and hum along. In fact, we can keep these allocations to be short-lived, which
may or may not be a performance issue but that can always be addressed later
with proper profiling.</p>
<h3>Single ownership and data pipelines</h3>
<p>Lastly, you can try to go away from references entirely. Ownership is ideal for
the kinds of problems best described as transforming values into other kinds of
values.</p>
<p>Pipelines have stages or steps. Steps may build up required changes or apply
earlier changesets. Pipelines are useful for a variety of solutions. Parsers,
compilers, streaming analysis, and so on, however that isn't the end of it.
Configuration could be seen as a stream that updates when new values are added.
This isn't to say all pipelines are pull models but it is to say the solutions
are broad.</p>
<p>If the use of borrows is for performance (want to ensure a large structure isn't
<code>memcpy</code>d to a function), this type of optimization should already happen behind
the scenes with move semantics; an owned type will typically get passed by
<code>memcpy</code> for smaller sized objects and may be passed as a pointer for larger
objects. This means that there is no mechanical difference for borrows besides
marking the earlier variable as uninitialized meaning we always have no more
than one owner of a value at a time.</p>
<p>When data flows through a pipeline, doing it all by mutable reference can
achieve the same effect and the owner doesn't relinquish control. Although this
<em>might</em> mean there are less allocations, it will mean each step of the way we
are passing a pointer where copying an object might have actually been faster.
This is also limiting in that we can only have on mutable borrow at a time! If
callers allocate the objects they own and request they be transformed into a new
shape, any number of threads could pass values into this pipeline for changes
and be content that value changes will be isolated from one-another.</p>
<h3>Recap</h3>
<ol>
<li>Use <code>Rc</code> or <code>Arc</code> when you want to quickly get past tricky borrow issues and
want to convert back to borrows possibly later in time to reap the benefits
they offer</li>
<li>Use <code>Cell</code> or <code>RefCell</code> when you have an API that doesn't need to expose
mutability to a client</li>
<li>Get comfortable with cloning data for multiple purposes to avoid conflicting
borrows</li>
<li>Don't borrow at all and create pipelines that pass ownership from step to
step, producing a final, desired result</li>
</ol>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Bindings Are Cheap: Managing Rightward Drift</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/bindings-are-cheap-managing-rightward-drift.html</link>
      <guid>https://justanotherdot.com/posts/bindings-are-cheap-managing-rightward-drift.html</guid>
      <pubDate>Thu, 20 Feb 2020 21:34:34 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>How do you avoid deeply nested <code>if let</code> or <code>match</code> statements when you're first
coming to Rust? Rightward drift is a pain to decipher in any language, but the
good news is you can manage rightward drift in Rust with a few techniques and
some mental shifting. Maybe this is the code you're writing which has a lot of
if-let chaining:</p>
<pre><code>if let Some(x) = some_func() {
    // do stuff with x
    if let Some(y) = some_func2() {
        // do other stuff with y
        if let Some(z) = some_func3() {
          // and so on
        } else {
          reticulating_splines()
        }
    } else {
        engage_thrusters()
    }
} else {
    launch_the_missiles()
}
</code></pre>
<p>In Rust, everything is an expression, and every expression has a value. For
control flow, that means all branches must return values of the same type. If
you look at the code above you ought to see that whole thing as <code>()</code>, assuming
the functions in the <code>else</code> blocks above return <code>()</code>. When I look at the above
code snippet I think &quot;this code is always meant to succeed but with different
results on the types of success&quot;. This code is always mapping <code>Some</code> and <code>None</code>
to <code>()</code>, which doesn't tell the caller much besides &quot;I might have done
something.&quot;</p>
<p><strong>A <code>None</code> implies the absence of something. If we want more information for
<em>why</em> the data we want isn't there we can use the <code>Err</code> variant on <code>Result</code></strong>.
The intent with the <code>try</code> (<code>?</code>) operator is to always allow a way to express
this 'failure' back to the caller when it first happens; we should not assume we
can go ahead safely with the subsequent code and return from the current
function.</p>
<p>A style I like to recommend to people is known by some as &quot;newspaper article&quot;
style. Since Rust is an expression-oriented language we can <code>let</code>-bind to almost
anything! This means we can write our fix as:</p>
<pre><code>let x = some_func()
  .or_else(|| { launch_the_missiles(); None } )?;
let y = some_func2()
  .or_else(|| { engage_thrusters(); None } )?;
let z = some_func3()
  .or_else(|| { reticulating_splines(); None } )?;
// and so on.
</code></pre>
<p>If we wanted to only give the caller the sense that nothing bad happened,
we could wrap the whole thing in a block and discard the result (NB. the
semicolon at the end of the block):</p>
<pre><code>fn top_level() {
    fn go&lt;T&gt;() -&gt; Option&lt;T&gt; {
        let x = some_func()
          .or_else(|| { launch_the_missiles(); None } )?;
        let y = some_func2()
          .or_else(|| { engage_thrusters(); None } )?;
        let z = some_func3()
          .or_else(|| { reticulating_splines(); None } )?;
        // and so on.
    }
    go(); // throw away the result for the caller.
}
</code></pre>
<p><strong>But this is weird</strong>. Giving callers control is at the crux of good error
handling, especially when it comes to something as powerful as errors as values!</p>
<p>What I absolutely love about the rampant <code>let</code>-bindings approach is that it
provides a lot of flexibility for modification; with <code>let</code> bindings we can
remove or modify the offending assignments exactly, rather than mangling a
rather delicately constructed expression.</p>
<p>Rust also lets us shadow variables and with its move semantics we can avoid
unexpected allocations when doing things like expressing data as it changes
throughout various steps but under the same name:</p>
<pre><code>struct Json {
  property: i64,
}

struct Error {
  SerdeError(serde::Error),
  IoError(std::fs::IoError),
}

fn update_json() -&gt; Result&lt;(), Error&gt; {
  let json = include_str!(&quot;../some.json&quot;);
  let json: Json = serde_json::from_str(&amp;json);
  json.property = 42;
  let json = serde_json::to_string(&amp;json);
  fs::write(&quot;../some.json&quot;, json)?;
}
</code></pre>
<p>Use <code>let</code> bindings and the <code>try</code> operator liberally and you'll make your code
easier to modify and read. If you have custom types you've written yourself you
might,</p>
<ul>
<li>one day be able to write an implementation for the <code>Try</code> trait yourself when it stabilizes (its currently experimental)</li>
<li>take a cue from <code>Option</code> and <code>Result</code> and write similar combinators that let you get at internal data for your type</li>
<li>merely wrap things in <code>Option</code> and <code>Result</code> and use the bevy of methods they expose</li>
</ul>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Idiomatic Argument Passing in Rust</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/idiomatic-argument-passing-in-rust.html</link>
      <guid>https://justanotherdot.com/posts/idiomatic-argument-passing-in-rust.html</guid>
      <pubDate>Fri, 14 Feb 2020 07:05:17 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you're coming from a language that supports automatically taking references
to arguments you may wonder why Rust can't do the same. Rust is all about giving
developers a better control of the memory layout of the data in their programs.
Since Rust has the notion of ownership, we don't have to worry about large
objects being copied into a function when the arguments to a function are owned.
Instead, they are moved (pass-by-move), and when I first started writing Rust I
assumed the idiom was to always use owned types for function arguments. For
clarity, we call something an &quot;owned&quot; typed when it isn't behind a reference
(<code>&amp;</code>). When an argument is behind a reference, we call that a &quot;borrowed&quot; type.</p>
<p><strong>tl;dr</strong>
<em>Idiomatic Rust functions ought to borrow arguments unless a function needs to
completely own an argument for ergonomics (say, method chaining) or allocation
(the caller won't need to re-use the data, perhaps).</em></p>
<p>What's the case against owned types for function arguments as the default? All
data in Rust must have an owner and that owner is a variable. Function arguments
are variables. This means that when you give a function an owned type, you force
a caller to give away ownership of the data it has allocated and probably wanted
to use further down the line. If the function doesn't give back the value, it is
lost to the caller, and the memory will be de-allocated at the end of the call's
scope. Often callers <em>do</em> want to keep ownership of the values they pass into
functions.</p>
<p>Immutable borrows let functions decide if they want to make selective
allocations but that does mean a function may be allocating when the caller may
want to know all allocations upfront. Owned types are a good fit for this as it
is the caller's responsibility to allocate and give up ownership to the function
for its use. <em>Alternatively</em>, if a function wants to make a change (mutate) an
argument, it will be clear to the caller that data may change signaled by adding
<code>mut</code> after the <code>&amp;</code>. The common practice in C is to take pointers to
non-primitive values. This is done so large objects don't get copied on each
function call. However, with this approach of using raw pointers there is no way
to clarify when a pointer is simply going to read data and when it is going to
change it. With borrowed types in Rust we get this clarity at the syntactic
level.</p>
<p><strong>Idiomatic Rust functions borrow arguments unless it truly needs to own the
values or they are primitives.</strong> Rust copies primitive values as they are part
of the <code>Copy</code> trait. And this isn't to say you should <em>never</em> take owned
arguments. The <a href="https://doc.rust-lang.org/1.0.0/style/ownership/builders.html">builder
pattern</a>
explicitly takes ownership and gives it back at each method call, allowing us to
chain together calls prior to a <code>let</code> assignment. If we used <code>&amp;mut self</code> instead
we'd need to first assign the value with <code>let mut</code> and make the calls
separately.</p>
<p>This leads us to an interesting example: How would we write the inside of this
function?</p>
<pre><code>fn thin_air() -&gt; &amp;Vec&lt;i32&gt; {
    unimplemented!()
}
</code></pre>
<p>We could try to allocate and take a reference to the allocation?</p>
<pre><code>fn thin_air() -&gt; &amp;Vec&lt;i32&gt; {
    &amp;vec![]
}
</code></pre>
<p>But the borrow checker will refuse this program because our <code>Vec</code> only exists
for the scope of <code>thin_air</code> and if we held a reference after the point it was
dropped (its memory is freed) we'd be holding a pointer to garbage which is not
safe to read or write to. Thus, if we want to return a borrowed type, we must
also take a borrowed type or something that holds a borrowed type.</p>
<pre><code>struct&lt;'a&gt; Data {
  integers: &amp;'a Vec&lt;i32&gt;
}

fn thin_air(data: Data) -&gt; &amp;Vec&lt;i32&gt; {
  data.integers
}
</code></pre>
<p>To recap, Rust cares about memory safety and layout a fair amount and puts the
work on the programmer to decide when references to arguments should be taken.
Choosing immutable borrows by default means you won't cause any unintended
consequences besides maybe some stray allocations. If you want to change the
content that the caller owns and, hence, has allocated, switch to a mutable
borrow. Lastly, if you know the caller won't need the argument anymore or if it
wants to return an owned type in exchange of the passed in argument(s), the
function ought to take ownership.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Error Handling With Closures In Iterators</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/error-handling-with-closures-in-iterators.html</link>
      <guid>https://justanotherdot.com/posts/error-handling-with-closures-in-iterators.html</guid>
      <pubDate>Mon, 03 Feb 2020 20:38:54 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Iterators give us a wonderful array of functional-style combinators. Past
readability, the rust compiler can occasionally optimize iterators better than
it can for-loops, too. However, as iterators work by taking closures it can be
confusing on how to best handle them compared to using classic for-loops. Here's
a toy example:</p>
<pre><code>fn parse_str_of_i32(input: &amp;str) -&gt; Vec&lt;i32&gt; {
    input.split(&quot;,&quot;)
        .map(|char| char.parse().unwrap()) // `unwrap`!
        .collect()
}

let input = &quot;1,2,3,4,5,6,7,8,9,0&quot;;
let numbers = parse_str_of_i32(input);
assert_eq!(numbers, vec![1,2,3,4,5,6,7,8,9,0]);
</code></pre>
<p>This works but it has an <code>unwrap</code> which means that if callers pass invalid
strings, such as <code>&quot;oh boy, here we go again&quot;</code>, it will panic, which gives
callers of this code little control when things go wrong. How can we convert
this to use <code>Result</code> and be more ergonomic? Consider the for-loops variant,
first:</p>
<pre><code>use std::num::ParseIntError;

fn parse_str_of_i32(input: &amp;str) -&gt; Result&lt;Vec&lt;i32&gt;, ParseIntError&gt; {
    let mut numbers = vec![];
    for char in input.split(&quot;,&quot;) {
        numbers.push(char.parse()?)
    }
    Ok(numbers)
}

let input = &quot;1,2,3,4,5,6,7,8,9,0&quot;;
let numbers = parse_str_of_i32(input).unwrap();
assert_eq!(numbers, vec![1,2,3,4,5,6,7,8,9,0]);
</code></pre>
<p>You might think this means if you want to use error handling while iterating you
need to have a for-loop instead of using Iterator but you can still have an
Iterator and have get the same type signature above for our parser!</p>
<pre><code>use std::num::ParseIntError;

fn parse_str_of_i32(input: &amp;str) -&gt; Result&lt;Vec&lt;i32&gt;, ParseIntError&gt; {
    input.split(&quot;,&quot;)
        .map(|char| char.parse())
        .collect()
}

let input = &quot;1,2,3,4,5,6,7,8,9,0&quot;;
let numbers = parse_str_of_i32(input).unwrap();
assert_eq!(numbers, vec![1,2,3,4,5,6,7,8,9,0]);
</code></pre>
<p>How does this work? <code>collect</code> knows how to take an Iterator of <code>Result</code>s and
turn it into an <code>Result&lt;Vec&lt;A&gt;, B&gt;</code>. At the first sight of an <code>Err</code> the whole
expression will become the <code>Err</code> case but if everything works out with <code>Ok</code> then
the Iterator will take all the values into their own <code>Vec</code> and return <code>Ok</code> of
the enclosing <code>Vec</code>. This is sometimes referred to as a &quot;transpose&quot; and you can
see similar 'inside-out' behaviour elsewhere, including <code>Result</code>
<a href="https://doc.rust-lang.org/std/option/enum.Option.html#method.transpose">itself</a>.</p>
<p>You can also specify collections other than <code>Vec</code>. If <code>A</code> is something that can
be <code>collect</code>ed into some container <code>V</code>, then an <code>Iterator&lt;Item=Result&lt;V, B&gt;&gt;</code> is
possible. Have a poke around the <code>FromIterator</code> <a href="https://doc.rust-lang.org/std/iter/trait.FromIterator.html">trait
docs</a> to get a
better sense of what <code>collect</code> can roll up!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Structuring Rust Projects With Multiple Binaries</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/structuring-rust-projects-with-multiple-binaries.html</link>
      <guid>https://justanotherdot.com/posts/structuring-rust-projects-with-multiple-binaries.html</guid>
      <pubDate>Thu, 30 Jan 2020 17:31:53 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>How do you organize Rust projects with multiple binaries so that the build
output winds up in a common subdirectory? Should you be looking for a solution
other than cargo? Regardless of whether you are using nested crates within a
workspace or simply a mixture of <code>.rs</code> files under <code>src/bin/</code>, <strong>you absolutely
should be looking for something other than cargo.</strong> What you need is a proper
task runner and the most portable task runner ships with every unix
flavored operating system; <code>sh</code>.</p>
<p>People seem to conflate task runners with build tools. Build tools generate
artifacts such as binaries or libraries whereas task runners act as the glue for
teams to share ways to achieve particular chores. Some people use tools like
<code>make</code> to do both jobs and the crossed responsibility brings a lot of pain and
maintenance burden. People need to be aware of the many nuances of <code>make</code> such
as the fact that tabs for indenting are semantic, rules for tasks need to be
marked as <code>.PHONY</code> if there is a target they relate to, and so on. Others end up
using scripting languages such as python or javascript or they may use some
hybrid domain specific language that mixes a bit of programming and
configuration to specify how tasks are run, e.g. <code>gulp</code>. You don't need any of
these options.</p>
<p>I'll call this script <code>bin/build</code>. We will assume there are several crates in a
workspace for this example and that we use <code>git</code> since cargo bootstraps projects
with it by default.</p>
<pre><code>#!/bin/sh -eux

ROOT=$(git rev-parse --show-toplevel)
cd &quot;$ROOT&quot;
mkdir -p dist/bin
for crate in crate1 crate2 crate3; do
  cd &quot;$crate&quot;
  cargo build --release
  cp target/release/$crate &quot;$ROOT/dist/bin/&quot;
  cd &quot;$ROOT&quot;
done
</code></pre>
<p>This script is dead-simple. It shoots to the root of the project, makes the
directories <code>dist</code> and its subdirectory <code>bin</code>. We have a list of crates in a
loop we iterate across but we could make this dynamic, as well. Then, in each
crate we create a release build and copy the binary from the project up to the
common subdirectory. Then, we shoot back to the root directory again and repeat.
All we have to do now to do now is make the script executable and call it:</p>
<pre><code>$ chmod +x bin/build
$ bin/build
</code></pre>
<p>You don't need to let scripts grow out of control, either. What's awesome about
keeping scripts, and, more generally, programs small means you can compose
things like this:</p>
<pre><code>
bin/init
bin/run
</code></pre>
<p>Where <code>init</code> might do some stubbing or setup work and <code>run</code> might launch a
service, whatever those tasks may be.</p>
<p><code>sh</code> is POSIX compliant, which means it allows us to write highly portable, and
therefore shareable, scripts. Like anything there are ways things can go wrong
but you can address this by using the linter
<a href="https://github.com/koalaman/shellcheck">shellcheck</a>. Every shell script you
write should have the following</p>
<pre><code>#!/bin/sh -eux
</code></pre>
<p>Which says to use <code>sh</code> instead of, say, <code>bash</code>. shellcheck will actually
recommend things intelligently based on which shell you specify. <code>bash</code> is not
ideal here because support for particular features differs between versions and
we are aiming to have something pretty much anyone on a team can use at a
moment's notice so long as they are using linux, bsd, darwin, or any other *nix
flavor. This prelude also turns on some common flags.</p>
<ol>
<li><strong>e</strong> to stop on the first <strong>e</strong>rror</li>
<li><strong>u</strong> to stop if a variable is <strong>u</strong>nset</li>
<li><strong>x</strong> to print tracing output of each e<strong>x</strong>ecuted statement</li>
</ol>
<p>(3) can be optionally dropped if you don't want to expose details or want
cleaner output.</p>
<p>The last convention is to keep scripts in a common <code>bin</code> directory at the root
of your project which enhances discoverability of scripts for others. Allowing
people to make less guesses about which directory is the single source of truth
for automation scripts helps people move faster. If they want a chore done, they
can see what's present under <code>bin</code>, or if they need to add a chore they know
exactly where it's added for every project. The reason for why its called <code>bin</code>
is that they are executables!</p>
<p>In summary, for shell script success all you need is:</p>
<ol>
<li>A common prelude that uses <code>sh</code> and some options set</li>
<li>Using shellcheck to ensure you're writing sensible and POSIX compliant scripts</li>
<li>A common directory for scripts that is the same for all projects</li>
</ol>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Peer Pressure Is The Sign Of Ownership</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/peer-pressure-is-the-sign-of-ownership.html</link>
      <guid>https://justanotherdot.com/posts/peer-pressure-is-the-sign-of-ownership.html</guid>
      <pubDate>Tue, 21 Jan 2020 20:27:28 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I'm nearing 6,000 contributions on GitHub. I mix personal and private
contributions but I estimate a large portion of of the changes are from work.
This is up from about 1,700 contributions back in 2018. Not all valuable
contributions to a tech business are GitHub related; people write documentation,
sketch out strategic plans for the business, spend time researching technical
details, design user experiences, analyze user data for purposes of sales and
marketing, and so on. All of these things don't wind up being issues,
pull-requests, code review, or direct commits into a codebase and yet
nonetheless help keep the company move forward.</p>
<p>Nonetheless, a tech company has healthy codebase(s) when there is a fervor of
activity. Likewise, if your codebase(s) are highly active they may suffer a lot
of churn, where changes that are introduced are swiftly eased back out only to
be re-introduced again in rapid cycle. The true mark of health on codebase(s) is
a mark of steady progress despite the occasional setback. Sure, <strong>stable systems
do exist</strong> where activity is but the occasional patch but when something is
deemed stable can vary drastically depending on the complexity of the project
and the requirements that steer its growth.</p>
<p>Giving ownership away can be scary. What if we render control to members of the
team and all we get in return is a product running in circles? What if nothing
of value gets built and the product stagnates and rots?</p>
<p>Top down management makes sense in a factory setting; if you have a large room
full of people manually slaving away building widgets, you can treat them and
their process much like a chariot driver and its steeds; whip and hurl insults
at will and the workers will move faster. There is enough generality to
optimizing assembly lines that the illusion of optimizing <em>all</em> forms of
business or systems are alike. <em>Wrong</em>. This management by microscope devolves
into micromanagement and creative output can't be managed in the same way. Jobs
that require a lot of thought in addition to output require a more hands-off
approach to achieve excellent results. Optimizing for output for creative output
means giving away the keys.</p>
<p>You can't create an illusion of ownership, either. Managers sometimes think they
can retain authority over decisions on a project while paying lip-service to
'owners'. This is a recipe for disaster when the truth is unveiled. Allowing
others to own systems (or projects) or subsections of systems (or projects)
allows them to thrive. Ownership leads one towards <a href="https://www.justanotherdot.com/posts/make-a-home.html">home
making</a> where members
focus on the pursuit of an ideal lifestyle and surroundings.</p>
<p>A product rarely has a single project as a neighborhood rarely has a single
home. Imagine a partially isolated village with inhabits of various skills; for
the village to prosper everyone has to have a certain level of interdependence
for their survival and happiness.</p>
<p>And here's the rub, owners of projects put demands on other owners, without
expecting some central authority to put the demands on them instead. The sign of
ownership is peer pressure. Work gets queued up and prioritized in manageable
pools. All of our goals needed to be aligned to the business, sure, but it is up
to these owners to figure out how to best accomplish that goal within reason. A
central authority can still keep watch that things are on theme. People can
still move across a system and do changes where they need to with respect to
owners of the system the proposed changes are occurring. Ownership imbues a
sense of care on all parties. If no one owns anything, then everyone will do as
they please with little regard to 2nd or 3rd order consequences.</p>
<p>If you can't start fresh with respective owners at the get-go, try starting
small and handing out keys to smaller chunks of the larger system. You need
trust to make this work otherwise you'll wind up with a self-fulfilling
prophecy. The experiment will change you and your teams for the better.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Consistent Date Handling</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/consistent-date-handling.html</link>
      <guid>https://justanotherdot.com/posts/consistent-date-handling.html</guid>
      <pubDate>Sat, 18 Jan 2020 20:27:50 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Date handling is the kind of funny where you sob from of the ways it can
horribly cut you when you least expect it. Developers either pretend that <em>all</em>
date handling concerns can be shoved onto third-party libraries or that they
don't exist at all. Here's a short, incomplete primer.</p>
<p>There are two common formats for storage; as strings or as integers. Although
integers have a history of heavy optimization on modern CPUs and compilers,
strings can have reasonable performance with the right memory structure. This
integer format is typically known as <a href="https://en.wikipedia.org/wiki/Unix_time">Unix Epoch
Time</a> and the start of the world for
this format is January 1st, 1970; the birth of Unix. A 32-bit integer,
expressing seconds since January 1st, 1970, ends at 19 January 2038. 64-bit Unix
Epoch's will end <a href="https://en.wikipedia.org/wiki/Year_2038_problem#Possible_solutions">292 billion years from now, at 15:30:08 UTC on Sunday, 4
December 292,277,026,59</a>.
This is far after the estimated death of the universe and if your system is
still running after that I think that deserves a nice pat on the back.</p>
<p>Where you are is your <em>time zone</em> based on wobbly, vertical slices of the world.
UTC is the &quot;base&quot; time zone and is such because it's on the prime meridian (zero
degrees longitude). Think &quot;base time zone&quot; where the offset is <code>00:00</code>. Let's
pretend we are sitting in a lawn chair in this time zone, which is the same as
GMT or &quot;Greenwich Mean Time&quot;, so, not a sunny day.</p>
<p>As you recline in the lawn chair time passes by but the earth's rotations and
the solar orbits are complicated things. Time doesn't <em>just</em> pass bit-by-bit. It
does in a mathematical sense, sure, but time is a construct with ideas such as
days, weeks, and years. To fit time into these relatively standard quantities,
such as the number of days per given month or total days in year, we must make
small adjustments to time, such as leap years, leap seconds, and daylight
savings. Each of these 'correct' some kind of drift. However, the Unix Epoch
format doesn't encode leap seconds, which is one type of correction.</p>
<p>Enter <a href="https://en.wikipedia.org/wiki/ISO_8601">ISO8601</a>. Among several nice
properties but for starters, humans can read it! The timestamp,</p>
<pre><code>&quot;2005-01-01T00:00:00&quot;
</code></pre>
<p>is equivalent to the Unix Epoch,</p>
<pre><code>1104537600
</code></pre>
<p>It's far easier to quickly determine if an ISO8601 is suffering corruption than
eyeballing integers. It's also much easier to tell what ballpark of dates an
ISO8601 is in. If you have a bunch of ISO8601 timestamps, you can sort them with
default strings comparison (lexicographic) and they will naturally be in
ascending order. I love this feature about them because it means I don't need to
rely on a library to order a bunch of well-formed ISO8601s. Opposed to our
fixed-precision Unix Epoch integers, ISO8601 allows for variable granularity.
You <em>can</em> get finer granularity for time on Unix systems but I won't go too far
into that here. You can run <code>man 2 gettimeofday</code> and <code>man 2 clock_gettime</code> for a
slightly deeper understanding of some options on Linux.</p>
<p>Back to our lawn chairs someplace in Sunny England, time zones are expressed
officially as strings describing two parts separated by a forward slash, e.g.
<code>Australia/Sydney</code> or <code>America/Los_Angeles</code>. If you have any formatting you need
to do for a client reading data, you need to encode time zones. However, it is
OK to not deal with time zones if you are dealing with an &quot;absolute time&quot; for a
given data set that is fixed to a place. You then have a direct link between a
set of timestamps and time zone.</p>
<p>ISO8601 has an optional time zone specifier. RFC3339 enforces that the timezone
be specified but for the case of UTC you don't need to specify the specific
offset as it is implicit. Time zones tend to be exposed to many odd political
changes. Offsets assume a timezone will <em>always</em> be a particular amount, but
this isn't quite true. As recent as 2011, Samoa changed their time zone for
trade reasons. Originally, <code>Pacific/Apia</code> had an offset of UTC-11 (note the
minus) but it changed for trade reasons and went to UTC+13 (note the plus).
That's a big jump! Thus, the timestamp:</p>
<pre><code>2013-01-02T12:00:00Z-11:00
</code></pre>
<p>is invalid for Somoa. However, if you didn't specify the offset as part of the
stored data, you could get away with looking up the time zone for <code>Pacific/Apia</code>
indexed by some granularity, say, year. This way you can record both offsets
before and after 2011. We could encode the timestamp simply as</p>
<pre><code>2013-01-01T01:00:00Z // UTC
</code></pre>
<p>and the lookup for the year 2013 would reveal that <code>Pacific/Apia</code> is <code>+13:00</code>
meaning we can now shift this date over properly for Samoans. In fact, you don't
even need to store the specific index as most time zone databases that third
party libraries provide already extensively document this information.</p>
<p>Its important to keep your timestamps in a canonical timezone. In the case of
Unix Epoch, by definition the seconds from midnight January 1st 1970 needs to be
in UTC, similar to the default timezone for ISO8601. Picking a canonical time
zone, specifically UTC, will save you a lot of time from painful sleuthing as
two dates without timezone information <em>could</em> be from different timezones.</p>
<p>Chat applications have to pay particular care to this. If someone is sending
messages from San Francisco but another person is replying from Beijing, the
time difference is an important part of the UI. How many hours are we off? When
did they really read my message? Did they send it before or after me? Causality
is its own can of worms for distributed systems, hence this is a bit of a
hand-wavey argument that is ignoring some really critical (and nasty) aspects of
time, but having most things stored as a single time zone and stashing client
time zone preferences or figuring out what their device location is and using
the time zone from that can save a <em>lot</em> of grief.</p>
<p>As a recap here's the basics:</p>
<ol>
<li><strong>Pick one format and stick to it</strong></li>
<li><strong>Always store your timestamps in a single, canonical time zone</strong></li>
<li><strong>Store time zone preferences for clients as a string rather than the
offset</strong></li>
</ol>
<p>And that's a short primer. Time can get a lot more nuanced, such as focusing on
monotonically increasing time for servers and thinking through concerns like
storage space, but most applications won't need to care too much about these
things. A little upfront focus on consistency will save you a lot of shed tears.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Production Environment's New Clothes</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-production-environments-new-clothes.html</link>
      <guid>https://justanotherdot.com/posts/the-production-environments-new-clothes.html</guid>
      <pubDate>Fri, 10 Jan 2020 20:08:56 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Staging environments are a distraction. Massive hours have been poured into
making them coherent with production all to little effect. When staging
environments become unbearable developers start resorting to alternative
environments that can be spun up at will or are equally long-lived as staging
environments are. These production clones feel safe to developers and product
managers because they they aren't shown to customers. One uncomfortable fact
that developers and product managers struggle with is that <strong>production doesn't
mean seen.</strong></p>
<p><a href="https://www.justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html">I've written about this
before</a>
but received some confused responses and I think I've realised why people feel
uncomfortable about this concept. Feature flag aren't just for testing shades of
blue. Feature flag services at as curtains over new functionality until you are
ready for the big reveal. There are third party services out there but you can
write your own feature flagging system to hide away details although there are
some limitations with doing it yourself. I have long been a proponent of small
pull requests; small changes give large boosts of energy, helping progress and
leading to the eventual discovery that you've built a mountain when it has felt
like a short jog. Developers who feel safe pushing changes start pushing a lot
of changes, hence, I think having a great feature flagging system is pivotal to
making the small pull requests approach feasible in a team.</p>
<p>It's understandable why there is a reluctance to have one environment. There is
a natural pain associated with pushing bad code and frantically trying to fix
it. Tucking things into the shadows means you are growing and building while no
one else really notices. Then, one day they come around and notice the
flourishing garden and sculptures you've built that they couldn't see before.</p>
<p>A basic feature flagging system is a key-value store for named tags, the key,
and booleans, the value. Non-existent tags are always false to avoid strange
behaviour. The steady state of the system is when all flags are off. Flags
should persist across the whole of the architecture to reduce mismatch and
bloat, which means a flag should be visible to all parts of the larger system or
product. Flags should be persisted to long term to be robust in the case of
failures.</p>
<p>Fancier feature flagging systems support things like traffic routing and mutual
exclusion. As noted before, a user may be randomly assigned to a split in an
<a href="https://en.wikipedia.org/wiki/A/B_testing">A/B test</a> and that particular flag
they were assigned may be incompatible with other flags. This isn't needed out
of the box unless your platform is already messy or you are suffering from too
much load.</p>
<p>Buggy code or migrations can poison a production database. If you are not
already taking regular snapshots of your database, fix that! Trying to prescribe
solutions for various use cases could easily fill other articles, but I will say
that despite it seeming scary that you are mixing feature-flagged code and
steady-state code that both touch the same shared state, with some forethought
it is far easier to curate one pool of data. If you can get back to a good known
state, you can work towards a granularity of restoration that suits your
products needs. A fantastic book on operations around databases that goes in
much greater depth is <a href="https://www.goodreads.com/en/book/show/36523657-database-reliability-engineering">Database Reliability
Engineering</a>.</p>
<p>Tying this all together, you should treat features as immutable migrations. An
immutable migration is one that doesn't happen in place, such that if I have
state A and want to be in state B, I first create state B and transition over. A
mutable migration is the one most people are familiar with, changing a piece of
pre-existing code, testing it locally and in a staging environment or similar,
perhaps even prod, and hoping for the best. Another way to put it, there is a
time where state A and B exist at the same time with immutable migrations, but
with mutable migrations the migration happens as a single commit.</p>
<p>With immutable migrations and feature flags, you can push the code progressively
to prod and test at-will. I'm a big fan of pushing code to production that
doesn't isn't being used <em>yet</em>. Doing this a lot and using namespacing on a
lexical level, e.g. with function or module names. When I wrote about this
before I mentioned the idea of breathing room and immutable migrations give you
just that. In fact, they ought to be your default form of migration unless you
are certain doing an in-place change is going to make things quicker and be
relatively pain-free.</p>
<p>Less is more; have one environment you put all your effort and love into. Hide
things from your customers until you are confident of your release. Make
immutable migrations the default instead of risky (albeit faster) in-place
migrations. Put some thought in what you need to do to protect your shared
state. With this your deployments will get more fearless and frequent as well as
your changes smaller and easier to reason about. <strong>Production doesn't mean
seen.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Soft Skill Hygiene</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/soft-skill-hygiene.html</link>
      <guid>https://justanotherdot.com/posts/soft-skill-hygiene.html</guid>
      <pubDate>Tue, 07 Jan 2020 19:57:10 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Eyes glaze over at the words &quot;soft skills&quot; for developers of both the &quot;ship it&quot;
and technical purists camp. Unfortunately, humans aren't emotionally empty
robots and team work is generally required to build anything of sufficient
complexity. Trust helps build innovative and productive teams. <a href="https://www.goodreads.com/book/show/33517721-the-culture-code">Trust is built
up from cultivating safety and vulnerability</a>
and we can achieve these characteristics through effective communication.</p>
<p>Listening comes first. People trust when they feel heard. Listening takes
patience and being patience is a skill that takes effort to improve. When we
listen, we can bubble with questions, agreements, and disagreements.
Disagreements may cause us to stop listening. Especially disruptive are
disagreements about expressed emotions. This is when respect comes into the
picture.</p>
<p><strong>Respectful listening means emotional validation.</strong> Although you may not think
that the other party's emotions are real that does not change the fact that the
other party feels them. It is disrespectful to hold the stance that you know a
person's feelings better than themselves. Validating someone else's emotions
means you trust the other party. A lack of trust tends to form from the worry
that other people are manipulating us. To properly validate other people's
emotions we need to take the leap of faith that the other party is being honest
with us. People look at others who share as having great depths of courage to
draw from, but it is the act of sharing that creates courage. It's the same with
trust; in order to trust others we also need to foster trust. I won't deny that
there are situations where someone may be manipulative and if you find you are
surrounded by this type of behaviour, get out! Signs of manipulation tend to be
oversharing of information (the subtle approach) or out-shouting peers (the less
subtle approach).</p>
<p>A conversation has two directions. We can weaken our discourse by being
aggressive or not even participating at all. When we don't participate that's
known as submissive or passive communication. Both passive and aggressive
communication can erode relationships. Refraining from sharing information does
not encourage others to share. Aggressively attacking shared information creates
cold speech. People fall into the aggressive mode of communication by likening
it to boldness and assuming that effective communication is <em>bold</em>
communication. Those with aggressive communication then cause others to refrain
from sharing.</p>
<p>Assertive communication is middle ground. Instead of chipping away at a
collective sense of safety and willingness to share, it helps strengthen
relationships. From a time management perspective it may make sense to say &quot;no&quot;
to as many things as possible; your time is a precious resource and wasting it
with needless tasks is a waste. But we don't always have to frame a &quot;no&quot; with
the word &quot;no&quot;. One variation of this technique I've found incredibly helpful is
the &quot;yes and ...&quot; approach. When you say &quot;yes but ...&quot; you inadvertently invite
debate, but when you say &quot;yes and ...&quot; you <em>validate</em> the other person's option
and provide your thoughts in addition.</p>
<p>Assertive communication is also respectful. As such, avoid using accusatory
language. Often when we feel something there is a tendency to state an
accusation rather than a reflection of how we feel, such as &quot;I feel nervous
about this option&quot; as opposed to &quot;you are always trying picking lousy options&quot;.
Note, also, the use of the word &quot;always&quot;. Using exaggerated language tends to
intensify debate. How do you tell someone they need to switch gears after a lot
of wasted effort on irrelevant details? Reminding someone of the facts will have
a far greater impact that the ensuing isolation created from making a judgement
on the person's ability to act.</p>
<p>Also, speaking with intent tends to encourage healthy discussion without
stopping healthy activity. This is one of the most empowering things I can
recommend. Stating &quot;I intend to ...&quot; allows others to raise any concerns but
also makes it clear that this is something you truly are going to do unless
there are serious complications with it. Paradoxically, asking for permission
tends to foster a lack of action <em>and</em> discussion.</p>
<p>Lastly, things that may seem useless but have big impact are making eye contact
(you can even possibly get away with <a href="https://www.sciencedaily.com/releases/2019/02/190205102532.htm">looking in the general
area</a>),
<a href="https://www.goodreads.com/book/show/25066556-presence?from_search=true&amp;qid=wpWrhGt3hv&amp;rank=6">posture</a>,
and
<a href="https://www.tandfonline.com/doi/abs/10.1080/00224545.1982.9713408">smiling</a>.
And that's it, really. Validate, share, be intentional, don't be an asshole even
if you're technically right, and you will go far in developing trusting and
cohesive teams than if you plugged your ears and pretend everything comes down
to luck.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The One Dimensional Programmer</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-one-dimensional-programmer.html</link>
      <guid>https://justanotherdot.com/posts/the-one-dimensional-programmer.html</guid>
      <pubDate>Sun, 29 Dec 2019 21:00:26 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Technical skill alone is not going to advance your career as a programmer. In
the last two years I've received questions asking what I felt would be the
golden ticket to career advancement. The question is usually peppered with
specific examples, things such as &quot;do I need to learn language X?&quot; or &quot;perhaps I
should focus more on data structures, algorithms, or even infrastructural
knowledge?&quot;.</p>
<p>Yes, we, as programmers, need to keep afloat of many differing technical skills.
Yes, it is daunting and unclear what will best advance a career, especially if
what you are currently doing as a professional is already murky. &quot;Full Stack&quot; is
a rather bogus term that seems to sugar coat the ideas of &quot;roundedness&quot; and
&quot;balance&quot; in a programmer's skill set. The ability to parachute into unknown
territory and still manage to become a local is a remarkable skill, but it is
not found entirely by refining computer related expertise for if you do you
become a one-dimensional programmer.</p>
<p><em>The seed of this line of thought was inspired by Patrick McKenzie's article
<a href="https://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/">Don't Call Yourself A Programmer, And Other Career
Advice</a>
which is well worth a read, but I am going to expand on the topic with a few
areas I don't think Patrick explores that I've felt have aided me.</em></p>
<p>Do you understand the ins-and-outs of business? I promise you it's not as
intense as learning the ins-and-outs of error-correcting codes, the minutiae of
details regarding LSM- or B-trees, or the specifics of the
fast-fourier-transform algorithm. Knowing the mechanics of business helps you
better weigh options when working directly on whatever it is you are programming
to be sold. A great book for this is <a href="https://www.goodreads.com/book/show/9512985-the-personal-mba">The Personal
MBA</a>.</p>
<p>Do you understand other people? You may not care to manage people but reading
about leadership and management will endow you with newfound abilities to better
work with peers and managers alike. Learning more about management and
leadership also allows you to see what kinds of management styles are out there
and whether or not you should be content with how you are being managed. My
favorite leadership book of last year was <a href="https://www.goodreads.com/book/show/16158601-turn-the-ship-around">Turn The Ship
Around!</a>
albeit not the only fantastic book on leadership and management I've read in the
last few years. Bonus points if you pick up a few top-tier parenting books. I
find they often teach me a lot about how to better interact with other people in
addition to my own children. The first I ever read that stuck with me was <a href="https://www.goodreads.com/book/show/769016.How_to_Talk_So_Kids_Will_Listen_Listen_So_Kids_Will_Talk?from_search=true&amp;qid=A20YKgAad8&amp;rank=1">How
To Talk So Kids Will Listen, And How To Listen So Kids Will Talk</a>.</p>
<p>Do you get how to write prose in your native tongue? Writing is ingrained in
what we do as programmers, but we need to be honest that we are still employees
that have to write emails, send messages on chat platforms, prop up
documentation, and so forth, all in a language not fit for a computer. Learning
how to write well drastically improves your ability to communicate effectively
with a time tested asynchronous format. Probably one of the best &quot;practical&quot;
books with concise tips is <a href="https://www.goodreads.com/book/show/41769546-100-ways-to-improve-your-writing-updated?from_search=true&amp;qid=0bszRxf7vW&amp;rank=3">100 Ways to
Improve Your Writing</a>.</p>
<p>If tasked with giving a talk could you whip up slides and enthrall an audience
in the allotted time without fuss? We practice public speaking each time we
open our mouths to another human being yet many of us don't consider that
&quot;public speaking&quot;. Enhancing your ability to communicate with spoken word and
body language drastically improves your capacity to convince others of a course
of action, improve relations with your peers, and rally management to give you a
promotion. One of my favorite and no-nonsense books on the subject is
<a href="https://www.goodreads.com/book/show/32784222-demystifying-public-speaking">Demystifying Public Speaking</a>.</p>
<p>Could you, left to your own devices, dream up a number of non-computer related
topics you'd want to explore? If the things you are reading only fit into the
categories noted above, it might be worth purposefully pursuing ideas that are
external to your field. Everything you explore need not be non-fiction, either!
Here's a sample of topics from my own current explorations:</p>
<ul>
<li><a href="https://www.goodreads.com/book/show/35297608-the-second-kind-of-impossible?from_search=true&amp;qid=nXScUtfJXZ&amp;rank=1">A new form of matter</a></li>
<li><a href="https://www.goodreads.com/book/show/39667068-acid-for-the-children?ac=1&amp;from_search=true&amp;qid=iYB5JCa0eO&amp;rank=1">A biography on the bassist Flea</a></li>
<li><a href="https://www.goodreads.com/book/show/36739320-because-internet?from_search=true&amp;qid=c5tPDh71fD&amp;rank=1">How English has evolved in the age of the Internet</a></li>
<li><a href="https://www.goodreads.com/book/show/28256439-the-hidden-life-of-trees?from_search=true&amp;qid=QvC55VC7MR&amp;rank=1">How trees communicate with one-another</a></li>
</ul>
<p>The more you gaze into the abyss, the less you are actually <em>in</em> an abyss; if
you are in &quot;box-mode&quot; all the time with your chosen field, the light on your
candle from within the cave will eventually grow darker and darker as it starves
of oxygen, but use the same candle to look in different places for light
switches and you'll start forming pools of light to help you better see the
world around you. <strong>Exploring different topics is how we discover disparate
ideas and connecting disparate ideas in unique and useful ways is is how we
innovate.</strong> Stop being a one-dimensional programmer and get out of your box from
time to time.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Running Build Bots On Premise</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/running-build-bots-on-premise.html</link>
      <guid>https://justanotherdot.com/posts/running-build-bots-on-premise.html</guid>
      <pubDate>Sun, 22 Dec 2019 15:13:05 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Late November I did a <a href="https://www.youtube.com/watch?v=DL_hODqnUy0&amp;list=PLG8S6YrJRoYI3CIUqvGX4NBSaMWZxe9in">video
series</a>
discussing continuous integration and automation strategies for projects. I used
<a href="https://github.com/features/actions">GitHub Actions</a> as they aided me in
demonstrating configuration of pipelines without setting up supporting
infrastructure. If you are a developer making things having a fast response time
for feedback is crucial and continuous integration helps drastically.</p>
<p>When you use a SaaS offering for CI you are stuck using whatever tiers and
upgrades are on offer. For the last six months, however, I've not used a SaaS
offering for my infrastructure and instead have chosen to run computers in my
home. I use buildkite to pick my own infrastructure. I did the numbers for
renting my ideal EC2 instances on AWS and figured I could pay the same amount of
money to buy a machine or two to do my bidding that would still be relevant four
to five years later. Buildkite has an offering for an elastic stack build agent
that can scale to zero when idle but I the stack configuration was too bloated
to my liking. Nonetheless, having the ability to opt into whatever
infrastructure you please has some cool consequences and I doubt this will be my
last post on the subject!</p>
<p>Regardless of running compute locally or in the cloud, one can choose if they
want to pay the overhead of virtualization or let things build on bare metal,
which makes sense for slower machines. In the case of local compute this is a
&quot;true&quot; bare metal unless you are paying a cloud provider the money to not rent
in a co-tenant server. It does help to <em>build</em> things in a virtualized
environment with the excuse that it is a &quot;clean room&quot; but if you want to do a
compile check or run some tests you probably don't care about dirt.</p>
<p>Initially I bought three raspberry pis; two B+'s and one Zero. The intent was to
run docker on them but I had forgot the host needs to be the same architecture
as the image you intend to build on and I often use x86_64 images. There was
nothing stopping me from converting these little boxes to running the languages
directly for tests and basic checks. I have yet to see any architecture specific
failures with the languages I tend to build for. These agents don't produce
artifacts as I don't need ARM releases.</p>
<p>Later I took a box I use for streaming video, a <a href="https://everymac.com/systems/apple/mac_mini/specs/mac-mini-core-i7-2.7-mid-2011-specs.html">2011 Mac
mini</a>
I upgraded to 16GB of memory, and equipped it with a few buildkite agents and
docker. I have since reduced the box to running a sole agent. This build bot has
been the most frustrating because the box isn't only doing CI related things.
It's easy for docker to randomly die or get OOMed as the browsers can take up a
fair portion of CPU and memory when streaming video, often ~20-30% CPU load, and
it makes no sense for me to constrain the docker daemon unnecessarily.
Eventually I split configurations into two types: those that use docker or those
that don't.</p>
<p>I also own a rather beefy Intel ATX tower that sometimes participates as a build
bot. I recently dissected an older tower to contribute whatever parts I could
collect to build another ATX box for full-time builds to ease pressure off the
mac mini on release builds. No matter what the machine is, I try to use it do
any kind of automation, CI related or not. I was curious if anyone else was
crazed enough like me to do this. I poised a question on twitter a bit
indirectly:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Those paid
professionally to code, how often do you dump money into buying
computers?</p>&mdash; Ryan James Spencer (@_justanotherdot) <a
href="https://twitter.com/_justanotherdot/status/1208218000626634753?ref_src=twsrc%5Etfw">December
21, 2019</a></blockquote> <script async
src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>My intent was to find out how often people buy various machines and actually put
them to use for purposes beyond what they use directly. I got several awesome
responses, primarily that often people buy new machines every 4-7yrs and, no,
this isn't really a common thing, at least for the people following me and keen
enough to have respond.</p>
<p>That said, a few cool machines were mentioned:</p>
<ul>
<li><a href="https://www.intel.com.au/content/www/au/en/products/boards-kits/nuc.html">NUC</a>
is a small form-factor device sold by Intel. There are equivalent ones
such as from <a href="https://system76.com/desktops/meerkat">System76</a> and if you care
about open firmware it's well worth supporting them!</li>
<li><a href="https://fit-iot.com/web/product/mintbox3-pro/">Mintbox3</a> -pro and -basic
both seem really cool and are
competitively priced in comparison to building your own machine. They are also
fanless if noise is a concern!</li>
<li>You wouldn't run this as a node in the cluster itself, but I had never heard
of the <a href="https://www.amazon.com.au/GPD-Portable-Ultrabook-Notebook-m3-8100Y/dp/B07W8MW2ZR">GDP portable
ultrabooks</a>
that seem like they'd be useful for quickly SSH'ing into a box without having
to carry around a full-sized laptop.</li>
</ul>
<p>Inevitably there are other computers I'd love to own &quot;just cus&quot;:</p>
<ul>
<li><a href="https://www.pine64.org/rockpro64/">Rockpro64</a> which Daniel Lemire
occasionally throw into his benchmarks</li>
<li><a href="https://www.sifive.com/boards/hifive-unleashed">Hi-Five unleashed</a></li>
<li><a href="https://system76.com/desktops/thelio-massive-b1/configure">System76 Thelio Massive</a></li>
</ul>
<p>You don't need a super computer to build and test software. Unless you are
looking for a laptop or need really bespoke hardware you can <em>generally</em> build a
desktop machine that'll run laps around most of its earlier variants. Building
your own also gives you a degree of customisation although, to be fair, it is
partly on par with cloud compute options: if you want to upgrade your file
system storage to a higher IOPS device, for example, you can abstractly do that
with a cloud provider, although you get a much finer degree of resolution with
your own computers.</p>
<p>Now that renting compute from cloud providers is commonplace, I suspect most
people would favor the ease of cattle-based systems administration and simply
slay any misbehaving servers. I find doing local systems administrations to be a
bit educational and cathartic in the sense of being a master of what you have
and working within limitations.</p>
<p>In terms of availability it's ok! There is always the risk that my children flip
a power switch or something goes wrong with my internet connection or power.
This sort of thing is already solved if you have, say, an AWS EC2 spot instance
in an autoscaling group where the terminating box will swiftly be replaced.
Because of this risk I occasionally supplement with cloud compute temporarily.</p>
<p>There are a few lingering things such as:</p>
<ol>
<li>
<p>External access into the specific network that has the bots. Something like
<a href="https://smith.st/">smith.st</a> and <a href="https://www.wireguard.com/">wireguard</a>
could supplant this if configured correctly. I remember seeing Brad
Fitzpatrick <a href="https://twitter.com/bradfitz/status/1206058552357355520?s=20">asking about ways to do a TLS based
termination</a>
into his home network awhile back.</p>
</li>
<li>
<p>Doing smarter things with scaling agents in and out as ewll as scaling to
zero, regardless of location. I had one crazy thought which was to have
agents scale out if local bots have pending work but haven't picked
anything up in X amount of time, say an hour tops. The newly spun up agent
could sit around for an hour attempting to pick up work and then kill
itself when idle, too.</p>
</li>
</ol>
<p>If you or someone you know runs local compute at home I'd love to get in touch
and see what usages are in practice out there. I'm always curious to see how
people are using on premise computing rather than switching entirely over to
cloud compute.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Patterns Of Knowledge Acquisition</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/patterns-for-knowledge-acquisition.html</link>
      <guid>https://justanotherdot.com/posts/patterns-for-knowledge-acquisition.html</guid>
      <pubDate>Thu, 12 Dec 2019 15:30:40 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Having written another <a href="https://www.justanotherdot.com/posts/reading-review-2019.html">yearly review of
reading</a> I have
been collecting thoughts around how I think about learning in general. Although
this list is horribly incomplete, I've identified a few core patterns I
tend to fall into when learning.</p>
<h2>Exposure</h2>
<p>In order to study something you need to know it exists. This stage of learning
consists of putting yourself into positions where you will best pick up new and
interesting ideas, words, and concepts without worrying about their finer
details.</p>
<p>Exposure is partly social and partly personal. People attend schools, go to
meetups, and join social media and forums. Other times they read blogs,
magazines, and book, watch videos, and listen to podcasts. The world is teeming
with sources of fresh content for us to discover and that is exactly the
problem; you can and will collect more information than you could imaginably
consume in your lifetime. This is why I won't spend too long discussing exposure
and will straight onto the next idea.</p>
<h2>Filtration</h2>
<p>Some quick <a href="https://www.justanotherdot.com/posts/fools-gold-time-estimates.html">back of the envelope
calculations</a>;
The &quot;average reader&quot; reads at a rate of 250 words per minute. The &quot;average page&quot;
is about 250 words which means the &quot;average reader&quot; ought to be able to consume
about a page per minute, assuming constant rate of reading. If you were a
perpetual motion machine and didn't need to eat or sleep, you could read 1440
pages in a day, which means 525,600 pages read in a year. The &quot;average book&quot; is
roughly 300 pages, which means approximately 1,752 books is the theoretical
upper limit for reading in a year at this non-stop, machine-like pace.</p>
<p>In reality we need to account for sleeping, eating, social activity, and general
motivation and energy. If you are lucky enough to dedicate 1-2 hours a day to
reading that means you could theoretically have an upper bound of 60 books
(calculating conservatively) per year.</p>
<p>This doesn't even take into account other media. Most adults, even without kids,
are lucky to have 1-2 hours in a work day to dedicate towards reading, watching
videos, or working on side-projects. Admittedly some people are night-owls and
may have higher-than-average time to dedicate to these endeavours, but skimping
on sleep is probably not the best idea.</p>
<p>If your definition of &quot;done&quot; for books, videos, and audio is only &quot;done&quot; if
every word is read, minute watched, and syllable heard then you are not reaching
for your potential for what you can learn. All information is composed of a
ratio of signal to noise. A one-hour video on fast fourier transforms with about
thirty minutes of anectdotes about the speakers' cats has a half ratio of signal
to noise. A book that has one message and spends the entirety repeating it to
draw out 300 pages is predominantly fluff. A riveting retelling of someone's
epic story of perseverance peppered with deep philosophical gems could largely
be signal.</p>
<p>You filter by applying a predicate to the material in question. People tend to
focus on what the predicate is but having processes for quickly trying things is
its own form of filter. Consuming a lot of material has helped me better
determine what I want to read and what I'd rather not spend my time on. If
you've read several books on leadership from a variety of popular authors, you
probably don't need to keep exploring every new title that comes out. If you are
passionately devoted to the collatz conjecture you might read every imaginable
piece of information you can find. Despite what passion you have, there is
always some level of diminishing returns.</p>
<p>It also helps to consider priorities. If you weren't going to wake up tomorrow,
what would you prefer to do? For me, studying, practice, and side projects go
down the ladder dramatically. I focus a lot more on being a better husband and
father. That said, it's just not possible for me to be in that mode all of the
time; my kids go to bed and my partner has things she wants to focus on so I
find myself with time to spend on the things that came lower in priority after
this practice.</p>
<h2>Redundancy</h2>
<p>Our brains are accustomed to freeing up space whenever possible so this is why
we must test ourselves on new knowledge until it finds it's way into
the more long-term storage we possess. You've opened up a lot of opportunities
and narrowed that list right back down to the things you want to spend time
with or know will truly help for you to work through. For myself, the best way
to help reinforce knowledge is through having multiple formats or even variants
on a concept or idea available for my perusal. If I've an interest I will
naturally keep picking things up, re-exploring particular articles, reading
certain things.</p>
<p>Going over material in multiple passes is a way to combine exposure, filtration,
<em>and</em> redundancy all in one go. The first few passes will store away the
top-level details, provide coarse definitions, and also help eliminate whole
sections that don't need examination. I will also own various formats for a
single title or groups of titles. Having an ebook is great when you're on the
go. Having a physical book is great for pacing and absorption when you're
relaxing at home. Having videos or audio can be played while working, or even
when reading is a bit too taxing (say after a very long day or week of work).</p>
<p>In my experience this last year my own framework consists of usually picking
books up as audiobooks first. If the top level view of the book is worth it,
I'll pick up the kindle and physical copies. With those I'll peruse various
specific chunks. When I noted that reading word-for-word is a barrier to your
learning potential, linearity is another such barrier, and redundancy helps
fight that all while providing retention. One of the reasons why testing
ourselves on knowledge works so well for retention is because it helps identify
the areas you are still unclear on.</p>
<h2>Applying This Framework</h2>
<p>Hopefully this has helped provide some insight into some principles that might
help with your own learning adventures or given some insight into how I look at
the problem of cramming knowledge into my head. I find I get a lot of value out
of making the exposure and filtration stages as fast as possible that by the
time I get to the redundancy stage it's much more high-signal-to-noise. It's far
more invigorating to work through material that is high signal <em>for you</em>.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Perils of Test Taxonomy</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-perils-of-test-taxonomy.html</link>
      <guid>https://justanotherdot.com/posts/the-perils-of-test-taxonomy.html</guid>
      <pubDate>Sun, 08 Dec 2019 15:53:57 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You are wasting your time by classifying tests. Instead of discerning what
defines a test we'll hone in on tests to avoid. If a test is:</p>
<ul>
<li>slow</li>
<li>flaky</li>
<li>or subject to churn as new features are added</li>
</ul>
<p>then delete the offending test right now.</p>
<p>For testing to work your test suites can't be grounds for noise pollution. Nor
can they be a museum for specimens fit for dissection. Decide on what you want
to guarantee and work to achieve that guarantee <em>within contraint</em>. <strong>Tests
themselves are un-tested chunks of code.</strong> Tests that exhibit any of the
characteristics listed above lose local reasoning and are, therefore, hard for a
human to verify.</p>
<p>Slow and flaky tests mean you can't form a feedback loop with them. It means
people will stop running the test suites to drive development. I often will
chalk up work in CI for build bots to test and also test things locally at the
same time, racing the two to get feedback as soon as possible. Tags and simple
test names provide a handle to hone in on specific areas of functionality that
can be verified as new features are added. Fast tests also mean people will add
more tests and while a test <em>suite</em> might continue to increase in time needed to
finish, it is arguably a point to break test suites up into new test suites and,
possibly, separate libraries and programs that have their own test suites.
Decomposition shows its beautiful face once again.</p>
<p>A non-deterministic (i.e. flaky) test may seem to <em>sometimes</em> provide a
guarantee but the reality is much bleaker: a non-deterministic test tests
nothing. I am not talking about tests that fail because of the occasional
third-party service going down or network issue. I know you will be accordingly
<a href="https://xkcd.com/303/">play-fighting with swords</a> if that happens. What I am
referring to is the situation where tests are <em>known</em> to occasionally but the
reason is unclear. Is it configuration with a database? A third party library?
Some state setup or internals of the subject of the test? Flaky tests are white
noise. Devs start to ignore them and must waste time determining what is at
fault if they are to ascertain if the test failure is because of something they
should truly be concerned about or &quot;just because&quot;.</p>
<p>It is also a waste of time when a new feature is birthed into the system only to
lead a dev on a surgery process of fixing an array of tests that now fail. This
is distinct from intentional changes: a test might need fixing because you are
intentionally migrating away from some older behaviour into a new one and doing
so in-place. But tests should have isolation: bringing in new functionality
shouldn't <em>necessarily</em> mean overlap on older functionality and, therefore,
older tests.</p>
<p>It's helpful to delete tests and see if you would passionately defend against
their deletion in the process. If there is no passionate defense you will not
likely miss them when they are gone. A giant wall of tests is also a giant wall
of maintenance burden and there is only so much energy a group of persons can
apply to maintaining something they don't care about whatsoever.</p>
<p>Tests and types provide a degree of confidence, one that allows us to assuredly
tell others something is <em>more likely</em> to be correct, such that is to say it is
aligned with some specification or set of requirements. <strong>Lacing your codebase
with questions that can be quickly answered with a clear yes or no helps aid
confidence.</strong> Debating if something is <em>truly</em> a unit test or integration test
or whatever test is the equivalent of the art communities cliché  of <em>&quot;but is it
art?&quot;</em>; humorous but not useful. Along with foundations such as quality release
and deployment engineering, operations, visibility into running systems, and so
forth, pushing things out to production becomes trivial with time. I obviously
and hand-waving away from the concern of scale here. Scale drastically impacts
trust and confidence, but many organisations are still paving a path forward and
charting new territory in this space to still make shipping code something sane.
Whatever you take from the above, the most important aspect about any kind of
testing is to make sure you are asking yourself one primary question when
writing tests: <a href="https://www.justanotherdot.com/posts/the-lowly-assert.html"><strong>What will you
assert?</strong></a></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Review 2019</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/reading-review-2019.html</link>
      <guid>https://justanotherdot.com/posts/reading-review-2019.html</guid>
      <pubDate>Tue, 03 Dec 2019 20:53:17 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This year came up at roughly sixty books &quot;consumed&quot;. I've pared the list here
down to about forty. I say &quot;consumed&quot; because I decided to give audio books a
go so while I would normally read about thirty books I've managed to double
that amount by listening to things while I work.</p>
<p>Things of note:</p>
<ul>
<li>
<p>It may seem obvious (or not?) that there is a tremendous amount of noise out
there but I'd like to reiterate that there is ridiculous amount of fluff out
there in what we choose and are asked to read.</p>
</li>
<li>
<p>The best books were too complex and deep to truly grok in a single pass,
but still lucid enough to catch on core points in that first pass. I call
this quality &quot;replayability&quot;.</p>
</li>
<li>
<p>Because of the number of titles to put thoughts against here I've opted to
describe from what I remember as quickly as possible and either be direct
about truly loving the book or not.</p>
</li>
<li>
<p>The worst books were shallow and forgettable.</p>
</li>
<li>
<p>Overcoming the discomfort of skimming and skipping through books has had a
remarkable impact on helping me tackle material I'm requested to ingest.</p>
</li>
<li>
<p>I found a lot of value in taking notes during or after reading. I heard
once that spending a brief period (specifically thirty seconds, but that
feels <em>too</em> short) of time to summarize recently received information, no
matter how complex, drastically improves retention.</p>
</li>
<li>
<p>When I found a book truly worth its weight, I would buy a physical copy to
review sections.</p>
</li>
<li>
<p>Of notable mention are Naval Ravikant's tweet storm and podcasts on <a href="https://twitter.com/naval/status/1002103360646823936?lang=en">How to Get
Rich (without getting lucky)</a>.</p>
</li>
<li>
<p>Two particularly inspiring non-technical blog posts were <a href="https://jsomers.net/blog/speed-matters">Speed
Matters</a> and <a href="https://jsomers.net/blog/more-people-should-write">More People Should
Write</a>. I also felt a
kinship for Alexis King's process described in <a href="https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/">Parse, Don't
Validate</a>.</p>
</li>
<li>
<p>I found it useful to read contrarian takes on subjects so that I'm not stuck
in my own bubble of thought.</p>
</li>
</ul>
<p>Onto some short blurbs on what I've read. These are no replacements for properly
reading the texts, but they may give you an idea of some core concepts and
provide some interest for diving into some (or avoiding others). As noted, some
descriptions may be shallow because the text in question is either too deep or
too shallow. I may do a top pick article to help clarify in the future.</p>
<hr />
<h5><a href="https://www.goodreads.com/book/show/38502098-a-people-s-history-of-computing-in-the-united-states">A People's History of Computing In The United States</a></h5>
<p>A fabulous recount about the history of time sharing computing systems in the
united states. I found the exploration of the social and economic contexts of
how these large-scale systems rose up fascinating. Easily goes in the
collection of great narrative historical stories of computing along with Soul
of a New Machine.</p>
<h5><a href="https://www.goodreads.com/book/show/48518642-unix">Unix: A History and a Memoir</a></h5>
<p>For the *nix fans out there this is a wonderful telling of the people involved
in the makings of Unix at Bell Labs. Brian Kernighan has written several books
that are always fun to get through: The Awk Programming Language, The C
Programming Language, The Go Programming Language, and so on. Learning that
Ken Thompson wrote the prototype of what would become Unix in three weeks
absolutely blew my mind.</p>
<h5><a href="https://www.goodreads.com/book/show/7776209-the-rational-optimist">The Rational Optimist</a></h5>
<p>The thesis of this book is that although things seem bleak in our current
state of human affairs, especially in regards to the knock-on effects of
capitalism, the degradation of the environment, and rampant political
fracturing, we are actually in a far better place than we were before now.
Ridley proposes that bartering is what makes us distinct from other animals
and that our ability to form markets and labour has allowed us to accelerate
advancement of our day-to-day lives.</p>
<h5><a href="https://www.goodreads.com/book/show/28815.Influence">Influence: The Psychology of Persuasion</a></h5>
<p>There's about six-ish things that Caildini focuses on as pillars of influence
on humans. By taking various roles and exploring each concept in relation to
the presumed job, Caildini explores things like scarcity, consistency,
attractiveness, providing reasons to requests, etc. Caldaini's original
framing of the book was to help provide tips so people could better understand
how to combat the effects of influence but it seems like it has become a
reference as a means of understanding a subset of persuasion on others.</p>
<h5><a href="https://www.goodreads.com/book/show/50221.God_s_Debris?from_search=true&amp;qid=4zOFwRZO5W&amp;rank=1">God's Debris: A Thought Experiment</a></h5>
<p>Thought experiment where a parcel carrier has a series of discussions with
God. Deep (and quick) enough for a read.</p>
<h5><a href="https://www.goodreads.com/book/show/40983156-platform?from_search=true&amp;qid=LAkkm07sMX&amp;rank=1">Platform: The Art and Science of Personal Branding </a></h5>
<p>Johnson's argument is that you are only truly an expert in a field if you hold
a reputation for it. A reputation is only built by putting work into gaining
an audience (platform). She outlines several ideas and approaches for doing
this digitally.</p>
<h5><a href="https://www.goodreads.com/book/show/11346463-beyond-religion?from_search=true&amp;qid=jheoqrhh92&amp;rank=1">Beyond Religion: Ethics for a Whole World</a></h5>
<p>The Dalai Lama examines various topics of life from a consciously secular
standpoint.</p>
<h5><a href="https://www.goodreads.com/book/show/35239798-the-courage-to-be-disliked?from_search=true&amp;qid=jmj2ZmFqaG&amp;rank=2">The Courage To Be Disliked</a></h5>
<p>A discourse about Adlerian psychology expressed as a young-man/old-man
dialogue which is trite. I found the various concepts (especially that of
people inventing their dramas) to be particularly intriguing. In the story
they explore the concept that other people have desires that they expect of us
but we ought to ignore them and instead focus on our freedom to react to
events however we truly prefer.</p>
<h5><a href="https://www.goodreads.com/book/show/641604.Purple_Cow?from_search=true&amp;qid=o7w51MStqc&amp;rank=1">Purple Cow: Transform Your Business by Being Remarkable</a></h5>
<p>Remarkable things sell. If you don't have a remarkable thing, you ought to
find one to sell. If you have a remarkable thing, don't pretend the effect
will last forever.</p>
<h5><a href="https://www.goodreads.com/book/show/13593553-to-sell-is-human?from_search=true&amp;qid=lr2xkw3ANr&amp;rank=1">To Sell Is Human: The Surprising Truth About Moving Others</a></h5>
<p>No matter your career selling is a part of it. You must convince people with
resources to part with them, usually in exchange for something else. This
thesis is not dissimilar from The Rational Optimist.</p>
<h5><a href="https://www.goodreads.com/book/show/34623128-the-autobiography-of-gucci-mane?from_search=true&amp;qid=urDgzb1Wai&amp;rank=1">The Autobiography of Gucci Mane</a></h5>
<p>This was highly ranked in recommendations for marketing but I'm not sure why.
Gucci Mane explains his upbringing, his entry into rap, the continual
inclusion of criminal activity and drugs (especially 'lean'), all while
achieving stellar success. The book finishes with him being fully incarcerated
and going through a period of withdrawal where he realizes he wants to
properly abandon drugs and turn his life around.</p>
<h5><a href="https://www.goodreads.com/book/show/773858.Born_Standing_Up?from_search=true&amp;qid=llM4Y5MV2a&amp;rank=1">Born Standing Up: A Comic's Life</a></h5>
<p>Incredible autobiography about Steve Martin's adventure through standup and
eventual explosion of fame (and his disgust of it thanks to his
dissatisfaction of the isolation it caused). It's interesting to note his
exploration of his standup style and career as a serious business endeavour
rather than coming off as part of his 'art'.</p>
<h5><a href="https://www.goodreads.com/book/show/31625067-hacking-growth?from_search=true&amp;qid=UGo93qIfEu&amp;rank=1">Hacking Growth: How Today's Fastest-Growing Companies Drive Breakout Success</a></h5>
<p>A cut-and-dry prescription approach to &quot;growth hacking&quot; and the book that
claims to have been authored by its inventors. Business intelligence is
definitely far older. and I wound up finding the book a chore to get through.</p>
<h5><a href="https://www.goodreads.com/book/show/47883410-the-mom-test?from_search=true&amp;qid=qeUkkrkDUz&amp;rank=1">The Mom Test: How to talk to customers &amp; learn if your business is a good idea when everyone is lying to you</a></h5>
<p>People will lie to you to feel good about themselves when asked questions
about your product. Your Mom does the same when you ask her, too. Your goal is
to get past the fanciful future uses that people love to labour on about and
focus on the actual pain they've experienced when dealing with a particular
problem or competitors product. You want to know how you can help ail these
pain points.</p>
<h5><a href="https://www.goodreads.com/book/show/40549476-this-is-marketing?from_search=true&amp;qid=P2VSbnc6yx&amp;rank=1">This is Marketing: You Can't Be Seen Until You Learn To See</a></h5>
<p>Godin argues that marketing is actually good for us; marketing directs us to
products that change our lives for the (arguably) better position and helps
normalize this behaviour.</p>
<h5><a href="https://www.goodreads.com/book/show/23848190-extreme-ownership?from_search=true&amp;qid=9p68jMJuxQ&amp;rank=1">Extreme Ownership: How U.S. Navy SEALs Lead and Win</a></h5>
<p>If you have direct reports and they fail, it's always back on you to teach
them (unless they truly are inept). It's a long book with many drawn-out
combat stories filling in the bulk.</p>
<h5><a href="https://www.goodreads.com/book/show/26156469-never-split-the-difference?from_search=true&amp;qid=gCfmxLBGid&amp;rank=1">Never Split the Difference: Negotiating As If Your Life Depended On It</a></h5>
<p>Many fantastic tidbits of negotiation techniques. Some of these are drawn from
other sources but it's how Voss connects the concepts to practical usage I
found particularly helpful.</p>
<h5><a href="https://www.goodreads.com/book/show/18077875-essentialism?from_search=true&amp;qid=kGC6L4RhhR&amp;rank=3">Essentialism: The Disciplined Pursuit of Less</a></h5>
<p>Lots of great little points on the &quot;less is more&quot; rhetoric.</p>
<h5><a href="https://www.goodreads.com/book/show/11878168-anything-you-want?from_search=true&amp;qid=IsDZruDbD6&amp;rank=1">Anything You Want</a></h5>
<p>Tells the story of Darek Sivers, the fella who unintentionally built CDBaby
and eventually sold the company at $12m. This was quick and had lots of great
point, like the higher leverage obtained by keeping teams small.</p>
<h5><a href="https://www.goodreads.com/book/show/34536488-principles?from_search=true&amp;qid=Sa02e6suZb&amp;rank=1">Principles: Life and Work</a></h5>
<p>Divided into two sections, principles discusses life and work principles, the
latter being far larger but the former being the most applicable for the
general public in my opinion. You can watch and read most of his material
without buying his book online as his emphasis isn't making money off the book
but sharing his knowledge (or so he claims). I've found it to be a fantastic
trove of knowledge that seems to come up in several other books I've read
without being Yet Another Business or Self-Help book and without being Get
Rich Quick themed.</p>
<h5><a href="https://www.goodreads.com/book/show/242472.The_Black_Swan?ac=1&amp;from_search=true&amp;qid=eH8HC2oJoi&amp;rank=1">The Black Swan: The Impact of the Highly Improbable </a></h5>
<p>I did not know about Nassim Taleb until this year when a mentor of mine
mentioned he was also reading him. Taleb, a former quantitative analyst,
explores the concept of the Black Swan, a highly improbable, but not
impossible, event, under the auspices that any attempt to claim that
predictions are rock-solid is a farce. The classic example he gives early in
the book is about a turkey who seems to be living the grand life, only to be
later killed (the black swan event). One connected idea I found particularly
intriguing is that of randomness being really about the observer of the random
variable: the turkey may not know about the black swan event but the farmer
who owns the turkey knows about this date for month.</p>
<h5><a href="https://www.goodreads.com/book/show/33517721-the-culture-code?from_search=true&amp;qid=jLO3Cob5xd&amp;rank=1">The Culture Code: The Secrets of Highly Successful Groups </a></h5>
<p>People are productive when they feel safe and, therefore, can be vulnerable.
They are also more creative and innovative. This is fundamentally Maslow's
hierarchy of needs but he also discusses the emphasis of sharing goals in
tandem with sharing vulnerability. One of the better books outlining why you
want to structure a business culture with these characteristics.</p>
<h5><a href="https://www.goodreads.com/book/show/6732019-rework?from_search=true&amp;qid=lERy4jotVP&amp;rank=1">Rework</a></h5>
<p>Your work isn't everything and ought not to control you. A well-rested and
clear-headed employee stays longer at a company and is wildly more productive
than the run-down equivalent. I liked the idea of &quot;JOMO&quot; (Joy of Missing Out)
being pushed in comparison to the usual sense of &quot;FOMO&quot; people experience. I
usually like Jason Fried and DHH books because they are full of lots
&quot;common-sense&quot; knowledge that are still helpful to be reminded of.</p>
<h5><a href="https://www.goodreads.com/book/show/25817524-alibaba?ac=1&amp;from_search=true&amp;qid=498oXbPB8f&amp;rank=1">Alibaba: The House That Jack Ma Built</a></h5>
<p>A rather bland telling of Jack Ma's growing of his various business interests
and, later, Alibaba. His translation work and early interest in the internet
helped eventually leading to his starting of Alibaba. There were many
&quot;everything store&quot; styled shops and Jack's early translation company did far
more than translation services, selling whatever they could.</p>
<h5><a href="https://www.goodreads.com/book/show/35167685-surely-you-re-joking-mr-feynman?ac=1&amp;from_search=true&amp;qid=F94Xrq62EN&amp;rank=1">&quot;Surely You're Joking, Mr. Feynman!&quot;: Adventures of a Curious Character</a></h5>
<p>Feynman is a mixed bag. He is quick witted and smart but is also a creep. His
autobiography paints a picture of a happy-go-lucky, curious-minded,
exploration-driven individual who loves to prank, be a smart alec, and isn't
shy to discuss his interest in women as well as visitations to strip clubs
where he would draw the strippers and work on physics problems. That said, it
is interesting to look at how Feynman is always deducing from other facts or
principles which you can see in the way the text is written (also notable in
his lectures).</p>
<h5><a href="https://www.goodreads.com/book/show/42734244-no-hard-feelings?from_search=true&amp;qid=qFp7eARyCT&amp;rank=10">No Hard Feelings: The Secret Power of Embracing Emotions at Work</a></h5>
<p>An exploration into how to accept that emotions are always apart of our lives
and decisions, including at work, and how to use that knowledge to best
effect. Also a good reminder that our work is not everything.</p>
<h5><a href="https://www.goodreads.com/book/show/687278.When_Things_Fall_Apart?ac=1&amp;from_search=true&amp;qid=w15Uw6ZCvZ&amp;rank=1">When Things Fall Apart: Heart Advice for Difficult Times</a></h5>
<p>A discussion from a Buddhist monk about how chaos is inevitable and how we
must, no matter how stable we think life is, continually invite &quot;chaos in with
a cup of tea&quot; if we are truly to handle what life has to throw at us.</p>
<h5><a href="https://www.goodreads.com/book/show/16158601-turn-the-ship-around?ac=1&amp;from_search=true&amp;qid=okBtnacd7S&amp;rank=1">Turn the Ship Around!: A True Story of Turning Followers into Leaders</a></h5>
<p>A naval captain tells how he managed to change from a top-down organisational
structure into something with more autonomy amongst the ranks, helping turn
one of the worst ranked teams into one of the best. The &quot;I intend to&quot; language
is gold, alone, but there are other techniques explained throughout the book
as it covers the discovery of these in relation to particular organisational
problems they were facing. For example, to deal with better handling
operational concerns on the submarine, they begin practicing deliberate action
where they vocalise what they are about to do, gesture at the subject that
will be acted upon, and, after a short pause, perform the task.</p>
<h5><a href="https://www.goodreads.com/book/show/44770129-ultralearning?from_search=true&amp;qid=6FlLLX6FpL&amp;rank=1">Ultralearning: Master Hard Skills, Outsmart the Competition, and Accelerate Your Career</a></h5>
<p>Discusses a form of learning where progress and speed are the focus;
compressing learning into short bursts at a time to positive effect. I have
mixed feelings about this book after having come out of therapy this last year
because I am into this processes but I felt the emphasis of an always-on
approach isn't healthy for long-term learning as well as general psychological
health.</p>
<h5><a href="https://www.goodreads.com/book/show/29513878-inner-engineering?ac=1&amp;from_search=true&amp;qid=7psLjNmm1N&amp;rank=1">Inner Engineering: A Yogi's Guide to Joy</a></h5>
<p>Particularly memorable parts of the book include reminding oneself of their
mortality to help prioritise what truly matters and that taking responsibility
is about taking charge of our ability to respond to life and events around us.</p>
<h5><a href="https://www.goodreads.com/book/show/40121378-atomic-habits?ac=1&amp;from_search=true&amp;qid=579XxYAfHC&amp;rank=1">Atomic Habits: An Easy &amp; Proven Way to Build Good Habits &amp; Break Bad Ones</a></h5>
<p>Although nothing <em>new</em> is prescribed here, I applaud Clear with compiling a
simple compendium of core concepts that fuel habit formation. All the
worksheets and activities I ignore but the four points are well worth the
extra bit of reading to cement the ideas.</p>
<h5><a href="https://www.goodreads.com/book/show/71730.Nonviolent_Communication?ac=1&amp;from_search=true&amp;qid=Ai25kkFMVC&amp;rank=1">Nonviolent Communication: A Language of Life </a></h5>
<p>This one is considered a classic in that it is commonly referenced as being a
big part of negotiation and interpersonal relationships discourse.
Rosenberg's foundations are that failure to recognise needs, of our own and
of others, is the crux of interpersonal conflict.</p>
<h5><a href="https://www.goodreads.com/book/show/187633.Art_and_Fear?from_search=true&amp;qid=SLSNWiUVy3&amp;rank=1">Art and Fear: Observations on the Perils (and Rewards) of Artmaking</a></h5>
<p>This one is great as it applies to areas of work outside of creating artwork.
I found this incredibly useful in terms of recognising that quantity is much
more valuable for the effort of learning (experimentation) than is focusing on
quality upfront. Other people's magic is not your own so don't go attempting
to copy in hopes that you'll achieve the same results. In the same train of
thought, only compare your development to your own history.</p>
<h5><a href="https://www.goodreads.com/book/show/34466963-why-we-sleep">Why We Sleep: Unlocking the Power of Sleep and Dreams</a></h5>
<p>Most of us know sleep deprivation is problematic but this book goes deeper in
how debilitating it can truly be, as well as how little sleep deprivation is
required before achieving ill effects.</p>
<h5><a href="https://www.goodreads.com/book/show/41795733-range?ac=1&amp;from_search=true&amp;qid=MgKQ7yawGG&amp;rank=3">Range: Why Generalists Triumph in a Specialized World</a></h5>
<p>Being an expert usually is less effective than being a jack of all trades.
Why? Because experts go down deep holes that start to lose innovation as they
make less and less cross-disciplinary connections.</p>
<h5><a href="https://www.goodreads.com/book/show/13588356-daring-greatly?from_search=true&amp;qid=DdogRhA3wb&amp;rank=3">Daring Greatly: How the Courage to Be Vulnerable Transforms the Way We Live, Love, Parent, and Lead</a></h5>
<p>Being courageous means sharing. We tend to think its the other way; that
sharing <em>takes</em> courage, but in reality if you share, you are being
courageous. Brown calls sharing so much information that your intent is to
manipulate others &quot;oversharing&quot;.</p>
<h5><a href="https://www.goodreads.com/book/show/17859574-how-to-fail-at-almost-everything-and-still-win-big?from_search=true&amp;qid=3TboDOwJdp&amp;rank=1">How to Fail at Almost Everything and Still Win Big: Kind of the Story of My Life</a></h5>
<p>Systems over goals. Energy drives success which drives passion, not the other
way around. A varied collection of ideas and memoirs of how Scott Adams built
various aspects of his career and had lots of little failures testing out
ideas and going through a life as a cubicle jockey.</p>
<h5><a href="https://www.goodreads.com/book/show/40796176-infinite-powers?from_search=true&amp;qid=lZHrEo7PEB&amp;rank=1">Infinite Powers: How Calculus Reveals the Secrets of the Universe</a></h5>
<p>Absolutely wonderful exploration of the history and internals of calculus.
Strogatz is the author of Sync, Non-linear Dynamics and Chaos, The Joy of X,
and others. He has a knack for breathing life into mathematical subjects. I
found his historical bits just as exciting as exciting as the clarity of his
explanations for the inner workings of calculus.</p>
<h5><a href="http://www.101thingsilearned.com/">101 things I learned from (urban planning|architecture|advertising|engineering) school</a></h5>
<p>I have several of these '101 things' books. They are a breeze to get through
(some of the points are just quotes, or usually short blurbs, less than two,
on average about one, paragraphs each) and often about 2-4% of those points
are absolute gold. One that is off the top of my head from recently finishing
the advertising version of the series is the notion that you should emphasis
getting your message across as many people as possible if it applies to a
broad reach (say, toilet paper) while a specific niche demographic is best hit
with higher frequency.</p>
<h5><a href="http://fabiensanglard.net/gebbdoom/">Game Engine Blackbook (Wolf 3D|DOOM)</a></h5>
<p>These are fabulous books and if you have any nostalgia for the games they
cover whatsoever, and you program, you should pick them up without a moments
hesitation. They cover a portion of history regarding product development,
hardware, and follow naturally into solid analysis of the internal tech of the
rendering and game engines.</p>
<h5><a href="https://www.goodreads.com/book/show/51291.How_to_Lie_with_Statistics?from_search=true&amp;qid=5RNxdWm9Ro&amp;rank=1">How To Lie With Statistics</a></h5>
<p>This is a truly dated book that has an odd style of speech that rambles at
great length so it can require some serious concentration to get at the core
of what he is driving at for most chapters. It is also exceptionally light so
not much is lost attempting to get through it, I suppose. The main theme of
the book is where people, organisations, teams, researchers, surveyors, and so
on, manipulate and confuse statistical data.</p>
<h5><a href="https://www.goodreads.com/book/show/28815322-the-danish-way-of-parenting?ac=1&amp;from_search=true&amp;qid=ZtM8X1oDqS&amp;rank=1">The Danish Way of Parenting: What the Happiest People in the World Know About Raising Confident, Capable Kids</a></h5>
<p>Nothing exceptional here that a quick glance through the headlines and a few
paragraphs wouldn't suffice with.</p>
<h5><a href="https://www.goodreads.com/book/show/23418.The_Architecture_of_Happiness?ac=1&amp;from_search=true&amp;qid=gcTS4eEKxc&amp;rank=1">The Architecture of Happiness</a></h5>
<p>This was one I read a bit more slowly than the rest. Alain De Boton is an
intriguing author in how he explores a myriad of different topics in truly
unique ways. I had previously read his book on Proust and how someone so bed
ridden could write so well about human pyschology. In this text he explores
how we tend to look at buildings aesthetically and jabs at modernist notions
of functionality and science being at the core of architecture. It inspired me
so much to write <a href="https://www.justanotherdot.com/posts/make-a-home.html">Make a
Home</a> which is still
one of my favourite articles.</p>
<h5><a href="https://www.goodreads.com/book/show/43726517-hello-world?ac=1&amp;from_search=true&amp;qid=0SNAVOVt36&amp;rank=1">Hello World: Being Human in the Age of Algorithms</a></h5>
<p>Fry explores how machine learning amplifies our biases. She argues that we can
actively expose and rectify these biases and use machine learning to
supplement human judgement rather than replacing it entirely.</p>
<h5><a href="https://www.goodreads.com/book/show/32919530-a-mind-at-play?ac=1&amp;from_search=true&amp;qid=k64DMsqzeg&amp;rank=1">A Mind at Play: How Claude Shannon Invented the Information Age</a></h5>
<p>I liked learning about Claude Shannon but this was tricky to get through as
the writing was a bit bland. Claude is the founding father of information
theory (the notion that different forms of data can be expressed with binary
data). Shannon worked at Bell Labs for a good portion of his life, liked to
juggle, ride a unicycle, and do mathematical puzzles. Apparently he had a
knack for statistics and probability, too, to the point which he obsessively
analysed gambling odds for roulette and had a knack for markets and building
precursory computing machines.</p>
<h5><a href="https://www.goodreads.com/book/show/44647144-database-internals?ac=1&amp;from_search=true&amp;qid=yB3M5IE3Og&amp;rank=1">Database Internals: A deep-dive into how distributed data systems work</a></h5>
<p>Technical books are hard to review because what may work for one person may
not work for another. Sometimes you need something as a heavy reference that
you'll be rifling through often and without any clear ordering, and other
times you may be looking for exactly a tutorial that starts and ends with a
clear path (like a university course). &quot;Database Internals&quot; has a <em>lot</em> of
knowledge <em>and</em> concise explanations of database concepts so it straddles the
line between reference book and tutorial, but in a way I think is appealing to
more voracious students on the subject. If you're looking for a simpler
explanation of internals of databases with less meat or details there is the
<a href="https://www.youtube.com/channel/UCHnBsf2rH-K7pn09rb3qvkA/playlists">CMU Database
course</a></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Release Does Not Make a Deploy</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-release-does-not-make-a-deploy.html</link>
      <guid>https://justanotherdot.com/posts/a-release-does-not-make-a-deploy.html</guid>
      <pubDate>Sun, 24 Nov 2019 20:21:53 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Is the vision in your head of your pipelines that of lean, graceful atheletes?
Do branch builds simply test your changes swiftly and anything that hits master
builds artifacts finished with the flourish of a ephemeral &quot;deployment&quot;?</p>
<p>Your pipelines are overweight slobs, unwilling to truly do real work.</p>
<p>Conventional wisdom dictates that deployments occur at the ends of pipelines by
running a simple task, say <code>kubectl apply</code> or similar, with the produced
artifact mentioned. This act is transient and for many pipelines means rolling
back is an act of rerunning the whole pipeline, an individual step in the
pipeline, or even reverse-engineering the action in the deployment step and
performing it manually, given the level of desperation.</p>
<p><strong>Build artifacts aren't deployments.</strong> By turning deployments from &quot;transient
action&quot; into their own artifact you can scrobble across deployments with little
fuss. A deployment artifact can be anything that describes the act of deploying.
This might be a script, a set of versions packaged together, or even a
specification like a kubernetes manifest. <strong>Once you have release artifacts <em>and</em>
deployment artifacts start the exercise regime for your pipelines by building
and publishing all the things</strong>.</p>
<p>Won't all this extra work cost more money and time? The reality is that
amortizing the cost of storing your artifacts and building whenever you get a
chance helps provide options so you don't have to do extra work when it is the
most untimely to do so. What costs more? Having a terrible
mean-time-to-resolution (MTRR) and frequent outages or paying for more build
bots and storage space? If you haven't learned the cost of burning the trust of
your end users, then you have an important lesson to learn.</p>
<p>Scrobbling deployments not only helps reduce the blast radius of botched code
hitting the pool of production by increasing your MTRR but it also gives you the
opportunity for functionality such as preview deployments. Some approaches may
provide previews in different deployment environments entirely whereas others
allow service or resource &quot;naming&quot; (e.g. unique URLs or distinct IP addresses)
to route traffic accordingly. Some blend the two together. This last approach is
often how services such as <a href="https://zeit.co/">zeit</a> and
<a href="https://linc.sh/">linc.sh</a> do their previews for branch builds. It depends on
how much reproducibility you care about to get a sanity check before deploying
to production.</p>
<p>The one wrench in all of this is the matter of shared state; sometimes the
complication of going backwards or forwards from a certain deployment involves
running or reversing migrations, reinstating or removing coupled infrastructural
changes, or even having third party services paid and available. There are
islands of deployments that may become totally inaccessible due to the above and
the best advice I can provide in the briefest period of time is that all of
these can be (somewhat) circumvented by ensuring nothing exists that isn't code.
To address the noted issues (which is incomplete, mind you):</p>
<ul>
<li>Having a process that ensures all migratory actions on a database are verified
to revert properly and that snapshots are regularly taken at frequent
intervals</li>
<li>All infrastructure is code so rolling back infrastructural changes isn't a
matter of someone GUI-poking or frantically performing manual changes</li>
<li>Providing configuration and testing that ensures a system behaves as it needs
to behave without the reliance on third-party software and services</li>
</ul>
<p><strong><a href="https://charity.wtf/2019/10/28/deploys-its-not-actually-about-fridays/">Fear not the Friday
deploy</a>
when you have options at hand; fear the duct-tape and popsicle-stick
infrastructure that makes Friday deploys a nightmare.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Simplest Programming Language I Know</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-simplest-programming-language-i-know.html</link>
      <guid>https://justanotherdot.com/posts/the-simplest-programming-language-i-know.html</guid>
      <pubDate>Wed, 20 Nov 2019 21:35:16 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I'm going to teach you the simplest programming language I know.</p>
<p>Everything starts with functions:</p>
<pre><code>(\x -&gt; x)
</code></pre>
<p><code>x</code> is the input argument. There may be more than one by adding comma separated
values, e.g. <code>x, y, z</code>, and so on. The body comes after the arrow (<code>-&gt;</code>).</p>
<p><code>x</code> is a variable. We can call functions like this:</p>
<pre><code>(\x -&gt; x) 3
</code></pre>
<p>This says &quot;substitute the value of <code>3</code> everywhere you see <code>x</code> in the body of the
function&quot;, like this:</p>
<pre><code>1. (\x -&gt; x) 3
2. (\x = 3 -&gt; x)
3. (3)
4. 3
</code></pre>
<p>Here's another example with more than one argument:</p>
<pre><code>1. (\x, y -&gt; x + y) 3 4
2. (\x = 3, y = 4 -&gt; x + y)
3. (3 + 4)
4. 7
</code></pre>
<p>Calling a function is called &quot;function application&quot; and when all arguments are
substituted with actual value we are left with the result. When we assign values
to variable names we call it &quot;binding&quot;:</p>
<pre><code>1. f = (\x -&gt; x)
2. f 3
3. 3
</code></pre>
<p>Sometimes when we talk about function application. It can be bit-by-bit:</p>
<pre><code>1. (\x, y -&gt; x + y) 3 4
2. (\x = 3, y -&gt; x + y) 4
3. (\y -&gt; 3 + y) 4
4. (\y = 4 -&gt; 3 + y) 4
5. (3 + 4)
6. 7
</code></pre>
<p>Functions are values. We call the above &quot;partial application&quot; because we get
functions back when we apply one argument at a time. This format for function
application is called &quot;currying&quot; where a function takes one argument at a time.</p>
<p>This programming language has many types but has no way to check this before
running the program. Hence we can wind up with weird expressions such as adding
the number <code>3</code> and <code>true</code> together. This programming language is called the
<a href="https://en.wikipedia.org/wiki/Lambda_calculus">&quot;lambda calculus&quot;</a> and when you
add a basic form of types you get the <a href="https://en.wikipedia.org/wiki/Simply_typed_lambda_calculus">&quot;simply typed lambda
calculus&quot;</a>.</p>
<p>And now you know one basis of functional programming and a model of computation.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Habit: A Tale of Two Water Bottles</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/habit-a-tale-of-two-water-bottles.html</link>
      <guid>https://justanotherdot.com/posts/habit-a-tale-of-two-water-bottles.html</guid>
      <pubDate>Mon, 18 Nov 2019 20:48:12 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I've the same one-litre water bottle at work as I do at home. I always drink
water at work, and I usually drink about 2L a day. When I work from home,
however, I barely hydrate.</p>
<p><a href="https://www.goodreads.com/book/show/40121378-atomic-habits">Atomic Habit</a> has
four basic rules stipulated for habit formation.</p>
<ol>
<li>Make it obvious</li>
<li>Make it attractive</li>
<li>Make it easy</li>
<li>Make it satisfying</li>
</ol>
<p>If you want to kick a habit, you reverse each of these four rules.</p>
<p>Access can affect ease, but easiness isn't just access. Before I owned a
camelback I had twist-lid water bottle. This isn't as easy as the standard
camelback because you always have to unscrew the lid if you want to take a swig.</p>
<p>At home, my water bottle is endlessly lifted by the resident little people. When
I do find my water bottle, it tends to be a greasy, dirty mess. Camelback came
out with these new plastic valves and my children have happened to chew them to
plastic mush. Whenever I get a chance to drink water, it is room temperature or
warmer.</p>
<p>Contrast this to my water bottle at work: it is always at my desk or near my
person, it is appealing because I don't have to use any cups and interact with
the dishwasher. It's also nice I don't have to contribute to the washing. It is
satisfying because I can always fill the bottle with refreshing, cold water from
the zip tap.</p>
<p>I see attractiveness about prospective satisfaction versus enjoyment during the
process, but you could also argue fuzziness about the terms &quot;obvious&quot; and &quot;easy&quot;.
Despite what you may think about the book, these principles are helpful at
forming habits (and breaking bad ones) and I've tried to apply them to other
things in my life.</p>
<p>I read a lot. I've <a href="https://www.justanotherdot.com/posts/pushing-the-boulder.html">talked
before</a> about
some formats I use for reading. I've learned to overcome the discomfort of not
reading all books from cover-to-cover, but I still enjoy reading front-to-back
with certain titles.</p>
<p>A clear, plastic chair is positioned next the shoe cupboard near the shower
room. I sit there when my kids shower before they are put to bed. The shoe
cupboard has a stack of Communications of the ACM magazines on its top. It was a
straightforward habit to form: attractive (reading always is for me), satisfying
(they are written on subjects I like reading), easy (it's a part of my daily
routine), and obvious (the top of the cupboard is only a little below shoulder
height. That ticks all four principles of habit formation.</p>
<p>Within time I found other places to put books. A fresh collection of paperbacks
arrived last month along with a title I haven't finished. &quot;DOOM&quot; by Fabien
Sanglaard went to the nook where my keys, wallet, and similar live. &quot;Unix: A
History and a Memoire&quot; by Brian Kernighan went next to mantle in the living
room. &quot;How to Lie with Statistics&quot; sits near the clear chair next to the shoe
cupboard. I am similar to <a href="https://www.youtube.com/watch?v=3qHkcs3kG44">Naval
Ravikant</a> in that I enjoy exploring
several books simultaneously. Progress is now far easier to make on each of
these books in tandem. Having progress on several different titles let's me
connect disparate ideas and sometimes certain titles discuss different sides of
an argument.</p>
<p>Habit formation has high practicality because its range of application is high.
A modern opinion is that we don't even build products for people to rationally
consume; heavy habit forming interfaces are more likely to be the products that
&quot;delight users&quot;.</p>
<p>Part of this leads back into code. People sometimes refer to the &quot;pit of
despair&quot; as the state where an ecosystem or user interface leads people to do
the wrong thing. The &quot;pit of the success&quot; is the facetiously named inverse of
the effect; doing the right thing is trivial. Not only are user interfaces and
ecosystems prime examples of habit formation for their end users, but the
processes used in how they are built are subject to habit formation, too. <strong>Want
you or your team to code or operate in a particular manner? Make it addictive.</strong>
And please, don't try to manipulate others into every imaginable whim. The
autonomy of people is their most valuable asset to a technical team.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Teleporting At The Speed Of Thought</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/teleporting-at-the-speed-of-thought.html</link>
      <guid>https://justanotherdot.com/posts/teleporting-at-the-speed-of-thought.html</guid>
      <pubDate>Fri, 15 Nov 2019 20:25:52 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Adept text editor users fly around and manipulate text as if by <em>teleportation</em>.
For me, this is a principle I hold dear when considering my editing experience.
<strong>Teleportation is chiefly driven by thought and is effortless by
construction</strong>. This is actually not something inherent to teleportaiton.
Instead of hopping in a car and driving around the neighbourhood to find where
you want to go, you tend to make a decision about your destination ahead of
time. However, driving expects a certain amount of effort to reach a destination
whereas teleportation requires little effort if at all. Teleportation doesn't
just mean <em>jumping</em> someplace but also transporting text somewhere, whether it
be someplace else in a buffer or into textual purgatory.</p>
<p>Some call this &quot;code golf&quot; but using that term implies falling into the trap of
constantly optimizing when the aim is to <a href="https://www.justanotherdot.com/posts/how-fast-can-you-take-your-time-kid.html">carve up the text in front of
you</a>.
As such, optimizing for teleportation comes from finding ways to facilitate your
thinking, rather than endless reduction for the sake of reduction.</p>
<p>As teleportation is driven by thought occasionally some
<a href="https://twitter.com/gregmcintyre/status/1194811646234873856">&quot;precognition&quot;</a> is
required. I <a href="https://twitter.com/_justanotherdot/status/1194732136948875264">recently gave an
example</a> where I
and former colleagues would abuse vim's &quot;paragraphs&quot; to jump up and down between
chunks of text by leaving gaps of newlines between them and hitting <code>Shift-{</code> or
<code>Shift-}</code> respectively. I also use syntactical constructs to <a href="https://www.justanotherdot.com/posts/dumping-grounds-for-good-and-bad.html">form
barriers</a>
where chunks of text might go to die if they aren't ultimately used or I might
further abuse whitespace to do <a href="https://www.justanotherdot.com/posts/stdout-is-forever.html">temporary debug
statements</a>. As I
write this article my editor is cutting newlines at eighty characters to make
sculpting up sentences and paragraphs easier.</p>
<p>This brings us to an important point; teleportation is editor agnostic. All
editor users alike, by experience and refinement, have been taught and taught
their editors how to zip around as if they are lightning incarnate.</p>
<p>As such, we have, as a larger community, cultivated a melting pot of ideas that
continually enhance teleportation as a practice. Sublime, VSCode, and others
have popularized the idea of the fuzzy-find palette for discovering files, text
matches in a buffer, git commits for a project, and so on. Things like <code>fzf</code> and
plugins for it now make this accessible to editors that don't have built-in
support. I particularly love fuzzy-find because it favours an aspect of
teleportation I call <em>course correction</em>, so long as the &quot;palette&quot; in question
provides a collection of results. From the results we can change our mind about
the direction we want to head. We can even simply go to some other option
without having to delete and type different results so long as the option is
present (you can do this in <code>fzf</code> with <code>Ctrl-P</code> and <code>Ctrl-N</code>).</p>
<p>Next time you trudge your way across your editor by keyboard or mouse, think
about how you could be teleporting, instead. Spend all those lost minutes on
stuff you want to spend them on. This principle is flexible enough to support
all sorts of optimisations and hopefully I've piqued your interest to explore
building your own.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>An Infinite Barrage of Mountains to Climb</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/an-infinite-barrage-of-mountains-to-climb.html</link>
      <guid>https://justanotherdot.com/posts/an-infinite-barrage-of-mountains-to-climb.html</guid>
      <pubDate>Wed, 13 Nov 2019 20:14:36 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This Tuesday I went to my last therapy session for the year. In that session I
finalized a relapse prevention program for my obsessive compulsive disorder and
recapped strategies I learned to handle various stressors in my life.</p>
<p>I’m one of those exuberant but actually shy people. I love reading and
programming because I’d rather be in an internet-equipped mountain cabin away
from the rest of civilization than on a boat in the Caribbean partying. I spend
a great deal of time honing skills through study and experimentation. The
studying has, and sometimes still, makes me anxious and exhausted.</p>
<p>A <em>lot</em> of people experience this; they learn new thing after new thing and when
the mountain seems climbed and truly conquered they look up again only to reveal
a new mountain waiting for them to ascend. If only they climb that next one will
they truly be done. It is a lie. There is never an end to the mountains.</p>
<p>This hurdle of seemingly endless mountains is partly why people feel constant
imposter syndrome. I do. I also feel imposter syndrome when I am full of
self-criticism by attaching my self worth to my productivity. <a href="https://www.google.com/search?q=thought+challenging">Thought
challenging</a> is a
convenient way to question the validity and usefulness of the thoughts we might
encounter that attempt to undermine us. If the thoughts still plague us despite
challenging them, it can also help to try <a href="https://www.google.com/search?q=thought+defusion">thought
defusion</a> where one displaces
the importance of a thought, or thoughts, to something less dominant by various
means. For example I sometimes imagine someone whom I would not trust their
advice as saying the plaguing thoughts to me. This makes it easy to ignore the
thought then. I know it's there, but like a hand on my lap instead of a hand in
front of my face, I barely notice it's presence.</p>
<p>Education matters and in a technical field it is unavoidable. If you don’t want
to stagnate you need to keep pushing to improve your skills and knowledge.
Pushing towards discomfort is good! It is the essence of growth to push until we
are uncomfortable. But <em>continually</em> pushing towards things until the bucket of
energy is dry withers away at who we are. We can become not only paranoid from
the rampant thoughts of imposter syndrome but also burned out from our desire to
improve. Rest is a crucial part of the process. Get uncomfortable for a bit,
take a breather, repeat.</p>
<p>Full disclaimer: I am not a medical professional of any kind. All I can do is
try to share some things I've learned that were prescribed and work for me.
Writing this article is more for reminder than it is for sharing. I give no
warranty as to the use of anything said in the entirety of this article. If you
are able to attend therapy and think it might benefit you in any way, shape, or
form, by all means you should try to attend. Getting personalised support is
best. With that said, How do I learn and stay healthy these days? Here's some
things I've picked up in my time wandering around:</p>
<ol>
<li>
<p><strong>You are not your work.</strong> Your work is its own thing. If you are your work you
can not have a critical, and therefore healthy, stance towards it.</p>
</li>
<li>
<p><strong>If you don't feel dumb you aren't learning and everyone who is learning
feels dumb.</strong> You need to pursue this feeling all the while reminding
yourself that everyone feels this and it does not signify that they, or you,
are a dumb person. As they say, it's not about being right but knowing the
truth. People who are always right don't go through this process and don't
actually obtain any deep understanding or learning.</p>
</li>
<li>
<p><strong>Compare yourself to yourself.</strong> Your history is yours alone and it isn't
logical to try to cookie-cutter other people's tales onto your own. Track
your improvements not by comparing yourself to others but by comparing
yourself to your past.</p>
</li>
<li>
<p><strong>Life itself is not work.</strong> Some people might argue that it's OK to dump all
your time into studying or hacking on side projects or whatever but the
reality is we need different entertainment for our brains and we need to give
it rest. Simply &quot;taking a holiday&quot; from your studying or work where you do
literally nothing is a fantastic way to replenish your excitement for the
things you are passionate about.</p>
</li>
<li>
<p><strong>Excitement is healthy and an absence of it is a warning flag.</strong> It helps
clarify what you want to concentrate your efforts on. Yes, sometimes things
will be boring when you learn about a topic but that doesn't mean you have to
read the entirety of Donald Knuth’s <em>The Art of Computer Programming</em> to make
a program (although it certainly might help). If you can make the task at
hand fun you will probably replenish your store of energy all the while
<a href="https://www.psychologytoday.com/files/attachments/4141/the-neuroscience-joyful-education-judy-willis-md.pdf">retaining information more
effectively</a>.
Doesn't this contradict my argument that we need to get to the state of
discomfort to grow? Not quite.</p>
</li>
<li>
<p><strong>Questions can be fun.</strong> Having a healthy sense to question everything is
what makes people smart. We want to tirelessly get at the truth but the
process of getting there need not be a slog through a muggy swamp on a hot
summer afternoon. You may have had that giddy moment exclaiming &quot;Aha! It
works! But <em>why</em>?!&quot; and whiled away another stream of hours on the problem,
and resulting problems, at hand.</p>
</li>
<li>
<p><strong>Flexibility is a key to happiness.</strong> Rigidity towards expectations on
ourselves and others can lead to a lot of misery. Accepting that things don't
always go to plan and that life is messy goes a long way.</p>
</li>
</ol>
<p>There's always another mountain and it <em>is</em> daunting but hopefully something
I've said here will help make overcoming it, and the chatter of self doubt that
tends to comes with it, easier.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Nothing of Value Will be Lost</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/nothing-of-value-will-be-lost.html</link>
      <guid>https://justanotherdot.com/posts/nothing-of-value-will-be-lost.html</guid>
      <pubDate>Sun, 10 Nov 2019 19:49:53 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>Drop your backlog. Burn all of your tickets. Eject your issues into the sun.
Nothing of value will be lost.</strong> Teams and maintainers alike cling to reminders
of work as if they are the same as the result of the work itself. Backlog
grooming sessions pass and only thin slabs of the gelatinous mass disappear into
the abyss all the while delaying developers from focusing on actual work.</p>
<p>Design and debate need to occur for a project to progress and when those things
happen it's good to record the results. As these records accumulate they age
because parts take priority over others. Ok, maybe you don't want to drop
<em>everything</em> but you definitely want to drop items older than a certain age. I'm
fond of choosing a natural period of time where you, the human, can easily
enumerate key points that have happened. Longer periods of time produce smaller,
less detailed lists. Periods of time that are too small might experience churn
on the issue tracker as items disappear and return repeatedly.</p>
<p>This process might sound crazed. How dare we close valid issues tied to real
people on an open source project or abandon fixes and feature work that could
drive up revenue and delight users purely because of age? Finding what to work
on is not the hard part, despite what you may think. Prioritising, hashing out
ideas, and setting goals has value but <a href="https://en.wikipedia.org/wiki/Sturgeon%27s_law">ninety percent of everything is
crap</a> and issues sitting in the
dark, ignored and unloved, are alike.</p>
<p>Those using todo lists will know the value of scrapping them at the end of a day
or week. Copying a few things over from prior days or periods of time can be
beneficial but usually the gain is marginal. Adopt a process that reflects the
fact that things change rapidly. <strong>Work that needs doing is from problems and
pain points that are being frequently encountered.</strong> It's work that's at the tip
of the tongue. This is the reason we care about 99th percentiles and avoid
one-off optimisations and bug fixes. What if the bug fix is data being
spuriously deleted? I can guarantee that issue won't stagnate and if it does I
think there are deeper issues that need handling. In the same way <a href="https://stackoverflow.com/a/153565/2748415">Kent Beck
told people on Stack Overflow</a> that
he gets &quot;paid for code that works, not for tests&quot;, one also won't (shouldn't?)
get paid writing or pruning issues.</p>
<p>Neglect the fool's gold of issue trackers. <a href="https://www.goodreads.com/book/show/1633.Getting_Things_Done">Your brain isn't a storage
device</a>; enable
your mind to process what it ought to be processing by using these glorified
todo lists to offload information.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Composition</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-composition.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-composition.html</guid>
      <pubDate>Thu, 07 Nov 2019 07:07:08 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Using composition gives you superpowers. It is by far the most practical
experimentation tool I know.</p>
<p>The <a href="http://hackage.haskell.org/package/base-4.12.0.0/docs/Data-Function.html#v:.">dot (.)
operator</a>
is my favorite infix operator in Haskell. Statically typed languages help ensure
that <a href="https://en.wikipedia.org/wiki/Function_composition">function composition</a>
is structurally sound before anything is run. Composition of two functions means
the type of the output of the first function must equal the type of the input of
the next function. Many languages now have a pipe operator which is the
composition operator in reverse. Some even use pipe or dot to write flow of
execution top-to-bottom or bottom-to-top, given how you can stack the calls.</p>
<p>This isn't just an article about the usefulness and specifics around function
composition itself. Composition as a concept forms a basis of for problem
solving and systems of proof. By decomposing a system or problem into parts we
can scrutinize and, thus, verify them for use in constructing the same or
potentially different solutions, proofs, and so on. Having solid building blocks
means we can play around with different arrangements. Playing around with these
building blocks and assumptions is how
<a href="https://www.goodreads.com/book/show/192221.How_to_Solve_It">mathematics</a> and
<a href="https://www.justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html">experimentation</a>
works at its core.</p>
<p>Composition also forms part of the basis of a fascinating branch of mathematics
known as <a href="https://github.com/hmemcpy/milewski-ctfp-pdf">category theory</a>.
Envision a type of mathematics that encodes any arbitrary concept as a
graph-like diagram to explore general structures and relationships. Having a
<a href="https://rs.io/why-category-theory-matters/">mechanism for encoding general
topics</a> empowers you with the
ability to play with structure and assumptions and study the structure and
implications of those arrangements. Caveat emptor; I am not saying composition
<em>requires</em> category theory to be useful! In fact, having too complicated a
system defeats the purpose of having a
<a href="https://www.justanotherdot.com/posts/lightweight-is-beautiful.html">lightweight</a>
guide.</p>
<p>Architecturally, the common phrase that &quot;systems are the sum of their parts&quot; is
a farce. If systems were some linear combination then removing individual
elements would merely reduce the size of the system, but removal can mean total
system failure, no change whatsoever, and possibly improvement in the system as
a whole!</p>
<p>It is rare to find a mental tool so broadly applicable and yet so uncomplicated
in nature. I'll reiterate strongly here; you don't necessarily need to be
pedantic about the shape of things to reap these benefits. Nor do you need to
understand category theory to its <a href="http://eugeniacheng.com/wp-content/uploads/2017/02/cheng-lauda-guidebook.pdf">highest levels of
complexity</a>
to piece together solutions. In my mind the <em>broad</em> steps are always the same:</p>
<ol>
<li><strong>Take, or produce, components</strong></li>
<li><strong>Scrutinize the components</strong> as you may be able to
i. break things down further (1)
ii. see how things connect
iii. or verify the parts are sound</li>
<li><strong>Experiment with arrangements of components</strong></li>
</ol>
<p>I see composition as a framework for experimentation with no added consequence
of increased complexity from the use of the framework itself. Experimentation
allows us to explore new connections. Exploring new connections means finding
solutions to problems in any domain. Discoveries are the bedrock of learning.
Rapid experimentation increases rate of knowledge acquisitions as well as
improved retention of knowledge. This is why composition is a superpower.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Roundtrips</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-roundtrips.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-roundtrips.html</guid>
      <pubDate>Sat, 02 Nov 2019 21:37:16 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Data &quot;roundtrips&quot; when it goes from one value, to another, and back to the same
value without any data loss, gain, or corruption. If you write code, you have
probably roundtripped JSON, YAML, TOML, or some other serialization format in
your time. You have also probably written versions of functions that do a
similar 'cycle' of some data. Any time you care about data being the same after
it's gone through the ringer, you want to write a roundtrip test.</p>
<p>Pretend we have a system where data comes in as JSON. We slurp up that JSON into
a type using <code>serde</code> (rust's idiomatic, type-driven serialization +
deserialization library). That data might later go onto being a type unrelated
to JSON, so we might write some <code>From</code> instances. This will be our adaptive
layer so we can keep the shape of the JSON and our core types distinct. I
mention this approach briefly in my post <a href="https://www.justanotherdot.com/posts/safely-shape-code-with-curtains.html">&quot;Safely Shape Code with
Curtains&quot;</a>.
The <code>From</code> instance would normally be trivial, but we don't want the JSON layer
and the core types to look the same, do we? That would make the point of the
JSON types moot:</p>
<pre><code>struct JsonType {
  names: Option&lt;Vec&lt;String&gt;&gt;,
  ids: Vec&lt;i64&gt;,
}

struct CoreType {
  names: Vec&lt;String&gt;,
  ids: Vec&lt;i64&gt;,
}

impl From&lt;JsonType&gt; for CoreType {
  fn from(x: JsonType) -&gt; Self {
    Self {
      names: x.names.unwrap_or(vec![]),
      ids: x.ids,
    }
  }
}

impl From&lt;CoreType&gt; for JsonType {
  fn from(x: CoreType) -&gt; Self {
    Self {
      names: Some(x.unwrap())
      ids: x.ids,
    }
  }
}
</code></pre>
<p>We could test each direction in isolation, but that would mask the actual
mistake here. Can you spot it? The roundtrip test in a property based testing
context would find the failure quite quickly. I'll do it by hand here to
demonstrate the mistake:</p>
<pre><code>let beg = JsonType {
  names: None,
  ids: vec![1,2,3],
};
let roundtrip_fwd: CoreType = expected.into();
let end: JsonType = roundtrip_fwd.into();
assert_eq!(beg, end);
</code></pre>
<p>When the data comes back to the JSON layer, unless we tell <code>serde</code> that empty
vectors are always <code>None</code>s for this field, we've now lost information. Clients
might care a lot that their POST of some JSON for creating an entity in this
make-believe system is non-symmetric. Developers might be going between the core
and the JSON types regularly, and they may even be using the JSON types to write
to disk, too, which would mean what was passed up from the client is now not the
same as what is stored.</p>
<p>We can extrapolate this sort of information loss or corruption to other
conversions. If you author an automatic code formatter, say <code>prettier</code>, <code>gofmt</code>,
<code>mix fmt</code>, <code>rustfmt</code>, and so on, you'd want to make sure that any time you save
a file and the formatter runs that your code is still the same code,
semantically, as it was before saving the file. Although things might possibly
look the same by eye, it could be another program entirely when run.</p>
<h3>Food for thought</h3>
<p>A quick refresher on functions.</p>
<ul>
<li>Functions can be seen as <strong>mappings</strong> from one type of value to another</li>
<li>All possible values that can go into our mapping are known as the <strong>domain</strong> of
a function</li>
<li>The set of all possible values our mapping can produce is called the <strong>codomain</strong></li>
<li>The set of all values the mapping realistically produces is called the
<strong>range</strong> or <strong>image</strong></li>
</ul>
<p>Ok, onto the concepts with fancy names:</p>
<ul>
<li>
<p>An <strong>injective</strong> mapping is when a mapping goes from values in the domain to
<em>unique</em> values in the codomain.</p>
</li>
<li>
<p>A <strong>surjective</strong> mapping is when a mapping goes from values in the domain to
<em>every</em> value in the codomain, even if some mappings overlap.</p>
</li>
<li>
<p>A <strong>bijective</strong> mapping is <strong>simultaneously injective and surjective</strong> which
means every value in the domain maps to every value in the codomain exactly
once.</p>
</li>
</ul>
<p>Why does this matter?</p>
<p>Bijective mappings give you an inverse function for free. If you are a value in
the codomain and you know the mapping is bijective, then you can be sure that
there must be one, and only one, value where you came from in the domain.
One could <a href="https://math.stackexchange.com/a/165440/156419">prove bijections</a>
using classical means but we don't need to for production usage. Instead, it
suffices to simply show the action going forwards and backwards.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Idempotence</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-idempotence.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-idempotence.html</guid>
      <pubDate>Wed, 30 Oct 2019 21:28:07 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Charging someone twice is bad for business; it burns trust with customers and it
involves a lot of unnecessary churn. Payment providers go to <a href="https://stripe.com/au/blog/idempotency">great
efforts</a> to support <em>idempotent</em>
endpoints. When you do something more than a given number of times, and every
time after that, things don't change. In the case of a payment it would be once
and only once, no matter how many times the request was submitted after that.</p>
<p>An
<a href="https://www.justanotherdot.com/posts/the-lowly-assert-involution.html">involutive</a>
function is idempotent modulo a <em>certain</em> number of applications. Involutive:
Driving a car around a square block means after four turns you're back on the
same corner you began on. Idempotent: A volume knob that reaches maximum volume
but still keeps turning. The assertion of idempotence looks suspiciously like
involution, but the concepts aren't quite the same:</p>
<pre><code>-- Involutive

f(x)       != x
f(f(x))    == x
f(f(f(x))) != x

-- Idempotent

g(x)       == x
g(g(x))    == x
g(g(g(x))) == x
</code></pre>
<p>If the function <code>f</code> was a one-hundred and eighty degree turn around a point then
the next part of the series would be another equality and would alternate back
and forth for every other function application. In the case of <code>g</code>, we do
something once, twice, or n-many times and nothing seems to change. Per the
volume example, there might be <em>some</em> changes initially, but <code>g</code> becomes
idempotent at or after a particular value.</p>
<p><a href="https://en.wikipedia.org/wiki/Idempotence">Idempotence</a> can relate to values,
but it can also relate to side effects, such as the payment example we've given
above. A &quot;thunk&quot; is a function that performs a calculation once and then stores
(&quot;memoizes&quot;) that result to return on all future calls: in this case a thunk is
idempotent in its computation: it's lazy <em>and</em> cached.</p>
<p>Things don't always <em>need</em> to be idempotent but can be chosen to be idempotent
for stylistic reasons. One API may force users to use explicit <code>insert</code> and
<code>update</code> calls, managing the housekeeping of keys itself, whereas a different,
but equally effective, API could allow a single endpoint that &quot;saves&quot; the
provided data, inserting at first, overwriting when the data is different, and
idempotent when the data is the same, forcing the tracking of keys on the
client. Both of these are valid options but have different trade offs for
particular applications!</p>
<p>When you think of idempotence, think about the mental model of things &quot;clamping&quot;
into place for a particular subset, or all, of our domain (inputs). And while
you're at it, make sure no one ever gets charged again for smashing the refresh
button for a slowly loading payment submission page!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert: Involution</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert-involution.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert-involution.html</guid>
      <pubDate>Tue, 29 Oct 2019 06:43:10 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>As part of <a href="https://www.justanotherdot.com/posts/the-lowly-assert.html">The Lowly
Assert</a> series I
wanted to go over some mathematical patterns. Filling your arsenal of known
properties helps with recognizing common ways functions, systems, etc. are, and
should continue, to behave.</p>
<p>Occasionally you'll write functions that flip-flop: when you call the function
multiple times in a row, chunks seem to cancel out. Mathematics calls these
functions <a href="https://en.wikipedia.org/wiki/Involution_(mathematics)">&quot;involutive&quot;</a>. Negating a number or a boolean twice gets you back
to the original value. Involution is handy to recognize because it's a simple
assertion:</p>
<pre><code>x == f(f(x))
</code></pre>
<p>The classic property based testing example of this is the
<code>reverse(reverse(some_list))</code> you'll see in endless tutorials and getting
started guides on the subject. When you reverse a list you expect to simply flip
the contents one end to the other, but this may not be immediately applicable to
day-to-day affairs. Here's one: a function that opens a dialogue box has two
states, open and closed, and is commonly tested for involutivity; if you didn't
have the toggling action you'd see no feedback after clicking!</p>
<p>But involution doesn't have to be about functions applied exactly twice.
Rotating around a point back to an original direction can be done with various
divisions: two rotations of pi, four rotations of pi/2, and so on. Precisely,
involution requires that the application be a single function, but we can
sometimes be a bit less rigorous and claim that two or more actions that
eventually cancel out are involutive: removing and adding an item in a
collection or returning someone's money in an exchange.</p>
<p>With this we can hopefully see the application isn't on any end of the scale:
from small to big, XORing bits to whole product flows that might 'restart' the
user in a funnel. Properties like these are worth their weight in gold because
they are useful in almost any type of testing and are definitely a shining
example of The Lowly Assert.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Making Plants Thrive</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/making-plants-thrive.html</link>
      <guid>https://justanotherdot.com/posts/making-plants-thrive.html</guid>
      <pubDate>Wed, 23 Oct 2019 20:38:18 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It's often lamented that software projects become dead plants in an unloved
garden: we excitedly keep buying new plants but we don't put in the time to see
them thrive.</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Anyone else&#39;s GitHub account literally just a graveyard of good intentions? 🙎‍♀️🙋‍♀️</p>&mdash; CaroOooOoOolyn 👻 (@carolstran) <a href="https://twitter.com/carolstran/status/1184938790533681152?ref_src=twsrc%5Etfw">October 17, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The appeal of building something new, playing with some fancy dependency or
tool, trying out some new process; if only we could resist the temptation. But
we shouldn't resist the temptation because this is the sign of healthy
experimentation! <strong>It's far better to experiment in your spare time than to use
your career as an excuse to try out the next shiny thing.</strong></p>
<p>I'm a huge fan of &quot;laboratories&quot; where questions you have regarding code are
answered by creating code and committing them to a central repository. Making
them multi-language helps by reducing friction for testing things out. A
graveyard of good intentions becomes a collection of prior discoveries.</p>
<p>This doesn't change the fact that we feel guilty that we can't keep the plant
alive. It takes a little discipline, and maybe for some, a bit of prior
knowledge, but it's not too hard to get things into place. In the same way we
reduced friction by making a project multi-language, introducing automation to
reduce toil is the best way for us to combat bitrot; if we can come back to
projects knowing full-well they build, we are much more willing to continue to
&quot;water the plants&quot;. Making a project thrive comes in a few major parts:</p>
<ol>
<li>Testing and building the code before it reaches trunk/master</li>
<li>Artifacts (library, binary, etc.) are created and published</li>
<li>Said artifacts may be deployed to a server to run</li>
</ol>
<p>Many other automations can be done too: linting, dependency updates, scheduled
builds, et. al. Scheduled builds are cool (and underrated) because they
continuously show projects are building and tests are passing. You now have
extra capacity from all the built artifacts to handle services going down or
security updates having been released. <strong>If you automate away the toil, you can
treat a project less as a chore (by focusing less on the accidental complexity)
and more as a labor of love (by focusing on the inherent complexity of the
problem you are trying to solve).</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Dumping Grounds for Good and Bad</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/dumping-grounds-for-good-and-bad.html</link>
      <guid>https://justanotherdot.com/posts/dumping-grounds-for-good-and-bad.html</guid>
      <pubDate>Mon, 21 Oct 2019 20:04:52 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>A former colleague and friend once referred to modules that grow without any
clear organisation as &quot;The Dumping Grounds&quot;. You probably know this module: it
often is named &quot;utils&quot; and acts as a kitchen sink for anything you are unsure
where to put. It might come in a different name, and there might be several of
them when the last dumping grounds were abandoned. When that happens, it's only
a matter of time until the majority of modules all become dumping grounds.</p>
<p>I do this thing when I'm coding or writing articles sometimes that I <em>also</em> call
&quot;The Dumping Grounds&quot;. It's similar because it's a pit of random junk I think
might be useful but may also just be crap. I have a rule about this space:
whatever is left in there by the time I'm done with the main chunk of work gets
thrown out. No disputes.</p>
<p>I might do this with things like whitespace or certain patterns of characters,
when writing these articles. For example, I'll tend to put a triple-hyphen or
triple-underscore to mark where the dumping ground begins and it (generally)
ends at the end of the file.</p>
<p>With the clear discrepancy of the dumping grounds, you can pick and choose what
you want from it, knowing full well it will get deleted. Other times, like in
code, it can be a bit more subtle.</p>
<p>When people work, there tends to be a bit of mess accumulated in particular
areas of the final piece. If the whole thing is a mess it can be hard to think,
but if the mess is distinct it can be a guiding force. Parts of a sculpture
might clearly be finished and other parts are in the rough. When I code, there
is usually a combination of tactically using whitespace, comments, and sometimes
syntactic/type constructs to keep chunks of mess easily identifiable, and, most
importantly, deletable.</p>
<p>If you <strong>treat dumping grounds as the landfills they are and try to keep them
out of your results</strong> then you'll get closer to the target you want. By using
this form of recognizable mess attached with a rule you can do just that, but
remember that mess is inevitable and that's ok! This approach isn't saying &quot;it
can <em>all</em> be a bit messy <em>just for this one piece</em>&quot;. What it is saying is &quot;this
is what I want and this is the mess I'm using to get there which <em>I will not
keep</em>&quot;.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Safely Shape Code With Curtains</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/safely-shape-code-with-curtains.html</link>
      <guid>https://justanotherdot.com/posts/safely-shape-code-with-curtains.html</guid>
      <pubDate>Sat, 19 Oct 2019 22:29:55 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Once upon a time I studied photography at an art school. It was there that I
learned the importance of separation between tones in an image. If the
separation, tone or color, between objects in my images wasn't quite right I'd
have to redo all my work in order to get a grade. <a href="https://en.wikipedia.org/wiki/Gestalt_psychology">Separation is how we often
define our mental maps of the
world.</a> For this article, I'll
call anything that is distinct from other things an &quot;entity&quot;.</p>
<p>Entities have edges. They may have eddies of communication or arrows of
connection through these edges. Edges may be incidental, e.g. defined by people
you don't know or from natural consequences, or they may be intentional, i.e.
the result of deliberate planning and execution. Entities have non-zero surface
area, otherwise they wouldn't exist, but that doesn't mean they cannot be
relatively invisible.</p>
<p>Clarifying (edges) means simpler mental maps. Simpler mental maps means easier
to reason about systems and programs. Simpler systems and programs means
increased velocity for progress and experimentation. Each of these examples
could be their own posts, but for now it suffices to say that examples of this
type of organization (clarification) are,</p>
<ol>
<li>
<p>Serialization to the wire (network), disk, and internal datatype definitions
<em>individually</em> go into their respective modules</p>
</li>
<li>
<p>Core logic that performs calculations versus reading from disk, e.g.
application level versus storage engine logic, are separated</p>
</li>
<li>
<p>Munging layers, or what some call an adapter, that transform data to the
shape you so desire are not tied into (1). This is bidirectional; it's
equally fair to have the adapting layer work on outbound and inbound
interfaces.</p>
</li>
</ol>
<p>Most of this might feel a bit obvious: things have edges and that's how
we tell they are distinct, but how does this relate to coding?</p>
<p>It's common to think that programming <em>has</em> to be a balancing act between
progress (by accepting breakage) and stability (by leaving things alone). I've
talked a bit before about <a href="https://www.justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html">how vital constant experimentation
is</a>,
but this balancing act is not the <em>only</em> way to go about things. Yes, things
break when they have production data running through their digital veins and
having instrumentation to gain visibility into your running code in production
is crucial to combat this statistic of failure, but let's consider another
approach to the development side of correctness.</p>
<p>Although it may seem strange for me to use the term <em>artificial</em> when all the
boundaries discussed here seem planned by ourselves or by others, I use the term
&quot;curtain&quot; here to denote <em>artificial</em> delineations we establish to avoid working
in <em>slices</em>. A sliced approach to development means we attempt to get all
working functionality, from front to back, one slice at a time. In the following
diagram the red boxes are slices of features whereas non-sliced functionality is
stable:</p>
<figure>
  <img
    src="/assets/images/sliced-development-example.png"
    alt="a diagram depicting 'sliced' development"
    title="An example of 'sliced' development">
  </img>
</figure>
<p>We can define &quot;curtains&quot; (again: artificial edges for the purposes of
development) to retain stability in all areas &quot;exterior&quot; to the curtain.
&quot;Exterior&quot; may very well be &quot;interior&quot; code! Setting up curtains can be done
with <a href="https://www.justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html">feature-flags, parallel
implementations</a>
or even creating new surfaces where interaction will be performed and migrating
after the fact when everything seems settled. As long as the &quot;exterior&quot; to the
curtain go on its life as if nothing is wrong, a curtain serves its purpose. Per
the example above it might look like this:</p>
<figure>
  <img
    src="/assets/images/curtained-development-example.png"
    alt="a diagram depicting 'curtained' development"
    title="An example of 'curtained' development"
  </img>
</figure>
<p>In this diagram, you could be setting up the curtain to keep the core of the
application stable or the client and interfaces the client talks to stable. A
curtain based approach by no means requires having a layered architecture or
thinking in that manner. The fact that a curtain is malleable and artificial
means we can define its boundaries, but a curtain becomes a slice when it
overlaps too many real edges in a system. Why is this any better than a sliced
approach?</p>
<p><strong>Curtains buy you breathing room.</strong></p>
<p>There is always an implicit countdown when you keep things stable but don't make
progress. &quot;Where is the business value they are adding?&quot; squawks the manager. If
you are making a lot of progress but breaking things the countdown timer is time
to completion but in the face of churn, top-down pressure, peer-pressure, and so
on. <strong>Breathing room gives you space to think. Space to think means you can buy
yourself breathing room. This helps build healthy systems with reduced
complexity and healthy systems means higher rates of progress and
experimentation.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold: Time Estimates</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/fools-gold-time-estimates.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold-time-estimates.html</guid>
      <pubDate>Tue, 15 Oct 2019 20:02:35 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>It would be fantastic if we knew the future. With that knowledge we could plan
with the utmost precision. But we are not clairvoyant. We actively exercise a
process of guesses we dress up in the fancier name of &quot;estimates&quot;. Enrico Fermi
would feel these guesses are fine so long as you are <a href="https://en.wikipedia.org/wiki/Back-of-the-envelope_calculation">within an order of
magnitude</a>. This
form of educated guess is also known as a &quot;back of the envelope&quot; calculation and
most random guesses are not doing anything near this <a href="https://www.wired.com/story/how-to-get-better-at-back-of-the-envelope-calculations/">level of
rigor</a>.
Back of the envelope calculations take rough approximations, simplified
assumptions, and various tidbits of top-of-the-mind knowledge to calculate a
<a href="https://en.wiktionary.org/wiki/ballpark_figure">ballpark figure</a>.</p>
<p>Put yourself in the shoes of a manager of a team and imagine asking each person
in this team &quot;how long do you think this task will take to finish?&quot; This could
well be <em>any</em> type of task. Estimations may sometimes vary wildly and sometimes
group around a certain value. Any kind of grouping is a coincidence. Asking the
same person about an estimate even the next day may yield different results. How
could you help improve the accuracy of these estimations with such fluctuating
results?</p>
<p>One option is to decide on all the work you expect upfront. If you know how work
is subdivided <em>exactly</em> you'll know, transitively, how long the total task will
take, right? Wrong.</p>
<p>Instead you'll starve people of autonomy around how they can tackle a goal.
Starving autonomy means people stop thinking and now you'll have to work even
harder to keep everyone productive. It also makes people unhappy, whether they
realize it or not, and you'll probably wind up with a great deal of turnover
because of it.</p>
<p>What about using time taken from similarly sized tasks? If it took someone a
certain number of time to complete a task with a rough &quot;t-shirt size&quot; then it
ought to take another, or the same, person roughly the same, right? Wrong.</p>
<p>Even if you had the same person doing the same task there is the possibility
that some spontaneous act can change timings drastically. People get sick. Their
dependents and partners get sick. Trains get delayed and vehicles break down.
The human brain decides to watch a video for an hour instead of fifteen minutes.
A meeting that was scheduled for an two hours only takes one.</p>
<p>We tend to attach degrees of confidence to our guesses but never seem to discuss
those confidence levels openly. We also tend to wrongly consider <a href="https://blog.codinghorror.com/how-good-an-estimator-are-you-part-ii/">tapered
ranges</a> as
the more accurate guess. We feel pressured to pick the most likely range and
make it a promise. <strong>Resist this temptation and try making this range of
possibilities explicit.</strong></p>
<p>Many people focus on what kind of confidence they back on a single range, but
our confidence may differ between slices of respective intervals of time. <strong>Try
to consider that your confidence in your guesses is not a normal distribution
centered around a single range; Confidence may be distributed in any number of
ways.</strong> There may be a high probability that the job gets finished if we focus
on it at the beginning and the end of the year, but a low probability otherwise.
If we have the time to work on this task on the fortnight's the probability goes
up, but the likelihood of completion dips on the weeks in-between.</p>
<p>Does a low probability mean impossibility? No. Nor does a high probability imply
absolute certainty. A range of probabilities discusses the full spectrum of what
might be feasible. It is better to know you think a project might take only a
months work of time all the way up to six months than it is to merely work under
the assumption that the single month would do.</p>
<p><strong>Make back of the envelope calculations by decomposing problems into
constituent parts.</strong>. The refinement of the accuracy of these parts refines the
overall estimation. This isn't to say breaking down a random guess into several
random guesses will improve accuracy. In fact, with the random-guessing approach
you will probably be reluctant to go below a certain lower-bound, which means
that a decomposed guess might far exceed the original guess. Back of the
envelope calculations try to tie you to real facts, although possibly
simplified, without believing the hype that you can estimate down to the minute
based on prior similar scenarios (see above). Machine learning won't save you.
The great thing about back of the envelope calculations is that they work
equally well for both high- and
<a href="http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html">low-level</a>
concerns.</p>
<p><strong>Pretending estimates given on work are guarantees is fool's gold.</strong> Push back
on demands for promises when you know you are only making a guess. We might get
better at making guesses with time by practice and research but a guess is still
a guess, educated or not.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How Fast Can You Take Your Time, Kid?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/how-fast-can-you-take-your-time-kid.html</link>
      <guid>https://justanotherdot.com/posts/how-fast-can-you-take-your-time-kid.html</guid>
      <pubDate>Wed, 09 Oct 2019 21:38:58 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is nothing wrong with chopping wood and carrying water. Hard work
certainly pays off. Hackers are notorious for <a href="http://threevirtues.com/">revelling in
laziness</a> but laziness has a stigma of the lazy party
not <em>being enough</em>. <a href="https://en.wikipedia.org/wiki/Laziness">Wikipedia</a> states,</p>
<blockquote>
<p>[laziness] may reflect a lack of self-esteem, a lack of positive recognition by
others, a lack of discipline stemming from low self-confidence, or a lack of
interest in the activity or belief in its efficacy.</p>
</blockquote>
<p>I'm not sure I think laziness is the right virtue for what hackers claim it
stands for. I'm not sure I think laziness is right for me.</p>
<p>Taoism has this idea called Wu Wei <a href="https://en.wikipedia.org/wiki/Wu_wei">&quot;a concept literally meaning &quot;inexertion&quot; or
&quot;inaction&quot;&quot;</a>. Wu Wei implies that going
against one's nature is an act of exertion. A tree has Wu Wei as it grows; there
is no thinking or straining towards the new shape but simply an act of <em>being</em>,
of following it's instincts towards the light and water.</p>
<p>William S. Burroughs wrote an essay called 'Do Easy' and Gus van Sant <a href="https://www.youtube.com/watch?v=eoOUBETTyMI">recorded
a video about it under the same
name</a>. The Discipline of Do Easy
embodies much, in my eyes, that is Wu Wei.</p>
<p>There is sometimes an illusion of progress through lots of pull-requests,
updated dependencies, rushing to complete features, and so on. Discomfort and
pain are nature's way of telling us that we are growing. It is the same when
your brain hurts when you study as it is when you exercise at the gym; growth is
a constant cycle of compression and decompression. How do we pair this cycle
with the idea of Wu Wei and Do Easy?</p>
<p><a href="http://www.bopsecrets.org/gateway/passages/chuang-tzu.htm">The Parable of Cook
Ting</a> is my favorite
Taoist fable. In it, an emperor eats a meal that touches him deeply; someone who
cooks a meal like <em>that</em> must know a thing or two about life! And off he goes to
beg the chef to share his secrets. The cook replies:</p>
<blockquote>
<p>What I care about is the Way, which goes beyond skill. When I first began
cutting up oxen, all I could see was the ox itself. After three years I no
longer saw the whole ox. And now — now I go at it by spirit and don’t look
with my eyes. Perception and understanding have come to a stop and spirit
moves where it wants. I go along with the natural makeup, strike in the big
hollows, guide the knife through the big openings, and following things as
they are. So I never touch the smallest ligament or tendon, much less a main
joint.</p>
</blockquote>
<p>he goes on to talk about his tool of the trade,</p>
<blockquote>
<p>A good cook changes his knife once a year — because he cuts. A mediocre cook
changes his knife once a month — because he hacks. I’ve had this knife of mine
for nineteen years and I’ve cut up thousands of oxen with it, and yet the
blade is as good as though it had just come from the grindstone. There are
spaces between the joints, and the blade of the knife has really no thickness.
If you insert what has no thickness into such spaces, then there’s plenty of
room — more than enough for the blade to play about it. That’s why after
nineteen years the blade of my knife is still as good as when it first came
from the grindstone. However, whenever I come to a complicated place, I size
up the difficulties, tell myself to watch out and be careful, keep my eyes on
what I’m doing, work very slowly, and move the knife with the greatest
subtlety, until — flop! the whole thing comes apart like a clod of earth
crumbling to the ground. I stand there holding the knife and look all around
me, completely satisfied and reluctant to move on, and then I wipe off the
knife and put it away.”</p>
</blockquote>
<p><strong>It isn't to say that discomfort or pain won't come. Overexerting yourself when
a task needs far less effort is wasted effort.</strong> It damages you and your tools.
You and your mind are the knife. You are the chef. Many times we jump to hacking
away tirelessly when a no-code solution is right there in front of us. When code
is required finding the toil and automating it away alleviates wasted effort
from repetition you don't need to make.</p>
<p><strong>Work in a way so you never need a grindstone.</strong> Growth is no different. Exercising
without any rest means there is no time for your body to recover and build new
muscles. Sleeping on a subject you've been studying allows your subconscious to
forge new connections which is the essence of learning. <strong>There are healthy ways
to grow as there are healthy ways to work. Don't be the machine.</strong></p>
<p><strong>Keep your eyes on what you're doing, be patient, and move with the greatest
subtlety until the whole thing crumbles before you.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>The Lowly Assert</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/the-lowly-assert.html</link>
      <guid>https://justanotherdot.com/posts/the-lowly-assert.html</guid>
      <pubDate>Mon, 07 Oct 2019 22:43:52 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is one thing that ties all forms of testing together; <strong>assertions</strong>. The
lowly assert humbly serves whether it's as types, panics, automated tests, or
any other glorious form. Regardless of how it manifests itself, it allows us to
declare things about our systems or program and automatically check them.</p>
<p>But when people test they don't tend to think about what they are asserting.
I've met a great number of people who are taught testing as a mechanical
practice, one that is simply followed because of the social expectation that a
tested system is a 'correct' system. But what is correctness?</p>
<p>Correctness is not merely the absence of bugs. <strong>Correctness is the assurance
that a system is doing as is intended.</strong> This can be business logic or even
sterile concerns like if a function returns the right value given the right
inputs (forms of unit tests). It can be about output or generated content
looking the way it's supposed to look (snapshot tests). It can be about multiple
systems behaving when coupled (integration tests) or about whole flows of usage
(end-to-end tests or possibly contract tests). The things we are testing <em>for</em>
and the ways to test for them is vast.</p>
<p>It helps to think about blocks of computation as black boxes: inputs go in and
outputs come out. Assertions that need to be upheld,</p>
<ul>
<li>while things are happening inside of the box are called <strong>invariants</strong></li>
<li>before the box starts work are called <strong>preconditions</strong></li>
<li>after the box has finished work are called <strong>postconditions</strong></li>
</ul>
<p>There are also a number of general properties the box can uphold:
<a href="https://en.wikipedia.org/wiki/Involution_(mathematics)">involutivity</a>,
<a href="https://en.wikipedia.org/wiki/Idempotence">idempotence</a>,
<a href="https://en.wikipedia.org/wiki/Partial_function#Total_function">totality</a>, etc.
The specifics of each of these isn't important but the idea is that there are
reusable patterns for guarantees we can wish from our systems and programs.</p>
<p>This article is the start of many to describe how the varying forms of
assertions lines up with their respective forms of testing. There are even
meta-principles at play about asserting facts about systems that we should make
elicit in the hopes they better our testing in general. These explorations
aren't going to be exhaustive but I am hoping they help expand your mind in the
things you can ask your code enforce.</p>
<p>A quick journey and recap, if you will.</p>
<p>When you write a program, you might use a typed programming language. In this
case you can use types to encode facts about your problem domain and structure
of data. <a href="https://blog.janestreet.com/effective-ml-revisited/">With types we can help make illegal states
unrepresentable</a>.</p>
<p>Later, you are writing a program and you want to know it acts the way you are
expecting it to act. Compilation non-withstanding you start to run the program
and check the results manually. <a href="https://landing.google.com/sre/sre-book/chapters/automation-at-google/">But this sort of tedium is easily
automated</a>.
<strong>Toil should infuriate you!</strong> With this sentiment in mind you start writing a
program to run your program in different circumstances, hence automated testing
is born. Now that you have this tool in place, you can run tests on small things
all the way up to big things. When the assertions in question fail, the tests
fail.</p>
<p>When a system misbehaves, you might want to know immediately while you are
coding and what faster way to know than to have your program halt when an
assertion is not met. Perhaps a failure is even one which requires a process to
abort while running in production (a fatal error). The difference between these
two is the subject of recoverable versus unrecoverable errors, which I won't
indulge in here, but it suffices to say catching mistakes and misunderstandings
sooner is always better than later by <a href="https://www.cs.tufts.edu/%7Enr/cs257/archive/jon-bentley/correct-programs.pdf">attaching these sorts of assertions to
forms of
panics</a></p>
<p>Now your test suite tests both small and large. As these tests get more
complicated, assertions can be about <em>models</em> of these systems; as state
machines or even where the inputs are generated randomly. <a href="https://www.youtube.com/watch?v=hXnS_Xjwk2Y">Property based
testing</a> starts joining your
repertoire for this reason. For verifying raw memory access you consider
<a href="https://en.wikipedia.org/wiki/Fuzzing">fuzzing</a>. Perhaps the end-to-end tests
are brittle and always breaking which might lead you into <a href="https://docs.pact.io/">contract
testing</a> two systems to ensure that the pre- and
post-conditions (read: the contract) are being met. Maybe there are extremely
complicated concerns such as concurrency and you write a
<a href="https://en.wikipedia.org/wiki/Formal_specification">specification</a> in something
like TLA+ which can verify the model it describes as part of the tooling.
Specify the system or program abstractly and test that, instead.</p>
<p>Like anything, there are diminishing returns. Finding assertions everywhere
doesn't mean proving your TODO-list single page application with a theorem
prover or dependant types is worth the time, although if those processes were
more lightweight it <em>would</em> probably be worth it! <strong>Think of assertions as bets
that pay off when code is introduced that violates them.</strong></p>
<p>Systems come in all sizes but despite their mixed formats they are all guided by
principles. <strong>Instead of thinking the path to correctness is forged by
mindlessly coding and churning out fixes, try to think about the properties you
want upheld, instead, and work to encode those in every possible assertion you
can leverage within reason.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>How I Git</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/how-i-git.html</link>
      <guid>https://justanotherdot.com/posts/how-i-git.html</guid>
      <pubDate>Mon, 07 Oct 2019 21:02:46 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I thought it might be worth having a look at two things <code>git</code> allows I've abused
to remove some warts and toil from my day-to-day flow.</p>
<p>One thing <code>git</code> does is alias support. Anything under the <code>[alias]</code> key in ones
<code>$HOME/.gitconfig</code> is treated as a valid subcommand. This is fine for quick
things, like <code>r</code> as <code>rebase</code> or <code>a</code> for <code>add</code>, but you can also alias one-line
scripts, for example, here's a snippet from my <code>.gitconfig</code>.</p>
<pre><code>  it = &quot;!f() { git fp &amp;&amp; git r origin/master; }; f&quot;
</code></pre>
<p>This demonstrates defining an ad hoc shell function named <code>f</code> and calling it
immediately. What's notable about this is that it is <em>also</em> calling a <code>git</code>
alias. <code>git fp</code> in this case is an alias for <code>git fetch --prune</code> and <code>r</code> we've
already mentioned is <code>rebase</code>, so this, verbosely, is,</p>
<pre><code>$ git fetch --prune &amp;&amp; git rebase origin/master
</code></pre>
<p>Another thing <code>git</code> let's you do is invoke arbitrary scripts that are named in
the format <code>git-name</code>. If the script is on the path, you can call <code>git name</code> and
the script, <code>git-name</code>, will run. My old process for pushing to a branch I had
authored was a bit verbose,</p>
<pre><code># on first push
$ git push -u origin/master current-branch

# afterwards ...
$ git fetch --prune

# and, after hacking, changes both behind + ahead on branch (rewritten history)
$ git push --force-with-lease

# or, if simply, without any `--force*` flag
$ git push
</code></pre>
<p>I wrote a script that does all of this, automatically, called
<a href="https://github.com/justanotherdot/gits/blob/master/scripts/git-p"><code>git-p</code></a>,
which lets me call <code>git p</code>. It's doesn't work for all corner cases, and could be
extended to, but this fits ninety-nine percent of my use case. This worked well
for awhile, but I needed to build on it. I eventually wrote an alias called <code>git up</code>,</p>
<pre><code>  up = &quot;!f() { git it &amp;&amp; git p; }; f&quot;
</code></pre>
<p>The point of <code>up</code> is to ensure my changes are always rebased on master before I
push. This is pretty handy but I've recently added yet another alias called
<code>raise</code> (also aliased as <code>pr</code>),</p>
<pre><code>  raise = &quot;!f() { git up 2&gt;&amp;1 | awk '/http/ { print $2 }' | xargs open; }; f&quot;
</code></pre>
<p>This scrapes out the remote output with the PR creation link that GitHub
provides after a branch is first pushed to the remote repository and funnels it
into <code>open</code>. MacOS X has <code>open</code> as the default way to open mime-type related
files to respective 'default' applications. On linux, where I use the gnome
windows manager, I have the shell alias,</p>
<pre><code>$ which open
open: aliased to xdg-open
</code></pre>
<p>to try to bridge the gap, which just goes to show aliases and scripts that use
this same format can be really handy for hiding away toil! I don't know if it's
ideal for all CLI tooling but I think this approach is certainly an interesting
approach to let people slip in their own functionality and 'rewire' an interface
to better suit their needs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Actually Using Git Worktrees</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/actually-using-worktrees.html</link>
      <guid>https://justanotherdot.com/posts/actually-using-worktrees.html</guid>
      <pubDate>Sat, 05 Oct 2019 13:48:54 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Let's say you are expected to do code review and you are also expected to code.
When you do either a certain set of changes is in place. Switching because you
are blocking someone means you <em>have</em> to do a dance with stashing changes,
checking out a branch, perhaps cleaning temporary files, restarting tooling,
etc. Bar changing your codebase, workflow, and job requirements, here's an
approach that uses <code>git</code> <a href="https://git-scm.com/docs/git-worktree"><code>worktrees</code></a> to
ease the cost of these context-switches.</p>
<p><code>git</code> uses <code>worktree</code>s to track changes in a repository. <code>git</code> gives us the
ability to make more than one <code>worktree</code> at a time that are checked out to
potentially different sets of changes. This means we can effectively split up
our codebase into review and development environments:</p>
<pre><code>$ git worktree add ../foo-review --checkout master # where `foo` is the name of your project
$ cd ../foo-review
$ git clean -fddx
$ git checkout branch-name
</code></pre>
<p>You can tuck the change-directory code into a script if there's a slew of other
steps needed to get into a good-known state. If you are using GitHub, here's an
added bonus for checking out pull requests by number rather than by branch name:</p>
<pre><code>[alias]
  &lt;snip&gt;
  copr = &quot;!f() { git fetch origin pull/$1/head &amp;&amp; git checkout pr/$1; }; f&quot;
  &lt;snip&gt;
</code></pre>
<p><em>N.B. There are alternative ways of <a href="https://gist.github.com/piscisaureus/3342247">fetching all remote pull requests from
GitHub</a> which might be preferable
to the above alias.</em></p>
<p>GitHub assigns this special remote tracking branch to your PR, but it's
read-only so if you want to contribute changes you will need to know the name of
the original branch.</p>
<p>With this setup the context-switch dance is reduced. The workflow could be like
this:</p>
<ol>
<li>Someone asks for a review or perhaps you're done and want to get back to work</li>
<li>Calling <code>work</code> might get you back into your development environment where you
left off</li>
<li><code>review branch-name</code> will go the other direction preparing the pull-request
for inspection</li>
</ol>
<p>Git aliases are a neat way to remap the surface area of <code>git</code>. I actually think
this is a utility for configuration I don't see more CLI tooling using that
probably could to great effect. In the context of <code>git</code> it allows me to get
around some particular ergonomic warts. Also, I don't do this workflow anymore
as I leverage pull-requests largely for communicating changes more than
gate-keeping these days, but I understand not all circumstances are the same.
Worktrees could be used to keep a reference implementation around for quickly
inspecting without having to switch branches, for example. Little things like
this that help reduce toil are worth their weight in platinum so it pays to keep
your eye open to automation opportunities!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold: Code Coverage</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/fools-gold-code-coverage.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold-code-coverage.html</guid>
      <pubDate>Wed, 02 Oct 2019 06:41:51 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>If you are unfamiliar with code coverage, the idea is simple: you write
accompanying tests to code and a code coverage tool produces reports of lines
covered by tests and the percentage of that coverage to all lines of code. The
hope is that a higher coverage with tests means you'll have a 'correct' system.
I have even heard of some establishments initiating quotas on required coverage
per lines of new code being introduced. &quot;If it doesn't have tests it doesn't
exist&quot; is the usual argument for this requirement; code without tests is
potentially problematic code, but tests are also <em>untested chunks of code</em> in
our codebase. For example, consider this bit of React code:</p>
<pre><code>test('Breadcrumb renders', () =&gt; {
  expect(() =&gt; {
    &lt;Breadcrumb/&gt;
  }).not.toThrow();
});
</code></pre>
<p>What is this testing exactly? Literally any other test, even one without the
<code>toThrow</code> expectation, would mark this as a failure on an exception being
thrown. This will light up code coverage though. People learn to cheat the
system, or please the percentage going up, and focus less on the guarantees that
tests are providing them. <strong>This does not help us deliver better products to end
users.</strong></p>
<p><a href="https://twitter.com/KentBeck/status/812703192437981184">Code coverage percentage is a useless
metric</a>. There is no way
to know what percentage of code written is code your end users actually care
about when that percentage is derived from synthetic traffic. You may write a
continent of code, but only a thousandth of that code may actually be hit by
users. If you are using code coverage to tell you that you have greater than
zero percent code coverage than you have a code organization issue or there is
the possibility that many or all of your tests are false positives.</p>
<p>Here's a different approach: instrument your application to track invocations of
code paths. You can do with this tracing, structured logging, profiling replayed
traffic, etc. The technique employed doesn't matter but what does is determining
what is valuable and what is dead. Regardless of collecting metrics, you will
always need to consider this <a href="https://kentcdodds.com/blog/how-to-know-what-to-test">from a human
perspective</a>.</p>
<p>Detractors may argue that code that doesn't immediately show usage should not be
hastily deleted. They are partly right. If things are early on at your company,
doing eyeball statistics may be fair but eyeball statistics is not real
statistics. Practicing some basic statistical understanding is always in order
for any kind of analysis. It may take time to reach a statistically significant
result and whether one is reached should, more often than not, drive your
decisions. As noted we need to exercise judgement despite what the numbers may
tell us. Perhaps the piece of code in question is a critical piece of error
handling that is rarely executed, for example, or maybe the code is serving a
particularly infrequent, but high-paying, user base.</p>
<p>I'd like to stress that I am <strong>not</strong> saying that testing is a pointless errand.
What I am saying is that <strong>code coverage is fool's gold</strong>. Tests take more
effort to create because developers must check that a test is actually testing
what you think it's testing. <a href="https://stackoverflow.com/questions/153234/how-deep-are-your-unit-tests/153565#153565">Testing is a part of one's confidence
level</a>
in what they ship, and when we assert important properties of a system are
upheld you boost that confidence level. <strong>Don't buy into the idea that coverage
is going to lead to a correct system by default.</strong> Be vigilant with you tests
and instrument your application.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Errors Across a Boundary</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/errors-across-a-boundary.html</link>
      <guid>https://justanotherdot.com/posts/errors-across-a-boundary.html</guid>
      <pubDate>Fri, 27 Sep 2019 21:10:01 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>There is a line across our systems we shall call the boundary. On one end of the
boundary are the consumers and on the other side are the providers. This
boundary is what we are accustomed to calling an interface. Interfaces are the
embodiment of the dance needed to cross the boundary. The interface may have
adapters on either side whose purpose is to munge details of the internals into
this known language of communication. This way internals can continue working
without the fuss of the protocol driving their decisions.</p>
<p>Things go wrong. But when they do developers tend to clump everything up as a
single form of error. Errors are about reporting mistakes or complications. A
better name compiler writers have been using for years is 'diagnostics'; they
should help diagnose a particular problem by being part of the symptoms an
ill-behaving service might demonstrate. <strong>As such, when an infraction occurs you
want to know who is the offending party. <em>Are we holding it wrong or are you?</em></strong></p>
<p>Borders on your errors make clarify what the fix is by knowing who should be
performing the correction. This might mean the speech of diagnostics changes. A
person hacking on some code is much more accustomed to cryptic messages from a
compiler than an average person using a web interface to access their bank who
doesn't understand how any of this is rigged up.</p>
<p>Clearly delineate your errors and you'll know better if something is a mistake
or a matter of environment, if it is something a maintainer needs to worry about
or a blunder from usage. There are many styles to error handling but this
approach does not impact which style you end up using. You can use this whether
it is error codes, exceptions, or values. This is a matter of organisation.</p>
<p>Systems architecture itself can largely be seen as a form of organisation.
Conceptual partitions, domain concerns, and our pursuit for pieces that compose
drive this organisation. Independent the layer of granularity the focus on
organisatoin is always the same. <strong>When things go wrong, tell everyone which
side of the fence the mistake or complication came from.</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fool's Gold, An Introduction</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/fools-gold.html</link>
      <guid>https://justanotherdot.com/posts/fools-gold.html</guid>
      <pubDate>Sun, 22 Sep 2019 15:37:09 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Developer's are a big target for what I call &quot;fool's gold&quot;. It's the hope that a
piece of tech can solve all of our problems that keeps us going with the bait of
new tech. Solutions tempt despite us knowing better. An experienced software
developer realizes that <em>everything</em> has strengths and weaknesses which we call
&quot;tradeoffs&quot;, but plenty of developer's don't realise this yet or are in denial.
This article is an introduction to the concept that plenty software and services
are sold as panacea but anything sold as panacea should be considered with
caution.</p>
<p>Let us discuss two ends of an argument first. There is the camp of <a href="http://boringtechnology.club/">choosing
boring tech</a> and <a href="https://www.intercom.com/blog/run-less-software/">running less
software</a>. This camp says that
cognitive load and operational costs are distractions for teams whose primary
focus ought to be the product they are building that makes them profit. By
running less software you are curbing the desire to have a bijective mapping
between problems to solutions where the relations are each their own distinct
solutions. Think about it this way, if you have N many devs and M many distinct
solutions for your problems, you have three particular cases to consider</p>
<ol>
<li><code>N &gt; M</code>: devs can't be experts except for some subset of the total pool of
technology. Ditto (and more importantly) maintenance and operations. More
than one dev has to be allocated per tech to make this work (pigeon hole
principle).</li>
<li><code>N = M</code>: every dev can own a particular piece of tech and grow with. Devs may
get bored and want to congregate on other pieces of tech. If this happens you
wind up with <code>N &gt; M</code>, effectively.</li>
<li><code>N &lt; M</code>: Devs can congregate around pieces of tech without much fuss. They
have freedom to pick what they like most (within a certain degree depending
on the delta <code>M-N</code>). Maintenance and operations is bearable as the whole team
can participate and not have to spin plates.</li>
</ol>
<p>Then there is the camp of constant agitation. A company goes under if it's not
constantly pushing to optimise for end users and reducing costs. <a href="https://www.goodreads.com/book/show/28592994-simplify">Some even
argue you can only pick one of these two optimisations</a>. Enterprises claim to
avoid this complication because their golden goose is sitting pretty, but the
reality is that any revenue generating organisation has to constantly push
themselves into the future to compete. Technology is an enabler, it allows teams
to move faster by automating away toil, easing collaboration friction, and a
productive team means they can, hopefully, deliver user experiences that delight
or at a cost that is beats the competition.</p>
<p>All of this shuffling around comes at the cost of churn and bloat. Companies try
to sooth this issue by either by hiring more devs and/or performing lots of
migrations. <a href="https://en.wikipedia.org/wiki/Software_rot">Software rots</a>, which
can mean different things to different people, but I see it as the eventual
ineffectiveness of a piece of software as improvements are found in competing
solutions. That is to say, even though your software may not actually be getting
slower, it will definitely feel slower in the context of all neighboring
solutions getting faster. This is but one example yet other things like security
exploits, support for a particular version of a language or library, and so on
all work to disempower your application or system.</p>
<p>The reality is that we cannot simply pick one camp to be part of as professional
developers. We are paid to help companies continue to live and be better than
they were before we joined, all within the confines of the ethics and legalities
we are bound to. <strong>We cannot sit still but we can't move too much!</strong> Some have
suggested things like a <a href="https://www.shimweasel.com/2018/08/25/novelty-budgets">novelty
budget</a> to support
keeping the platform largely stable while pursuing new ways of handling
constantly arising issues.</p>
<p>This article isn't meant to attack particular companies or pieces of tech or
practices despite those practices that carelessly hold on to the past or
endlessly throw it out for the new. The core intent is to encourage critical
thinking and consideration of tradeoffs when problem solving. Weigh your
options! You cannot make decisions purely by reading and watching, nor can you
make a choice by trying everything as there simply isn't the time. Certain devs
learn to do cheap experiments at work or at home but this can have the pitfall
of comparing a toy project a success that may not handle the scale of an
industrial grade application. This sort of scrutiny is part of the weighing
process.</p>
<p>If anyone tells you they're going to make you rich, they're getting rich off of
you. <strong>Take marketing with a grain of salt and throw out the fool's gold!</strong></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Pushing the Boulder</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/pushing-the-boulder.html</link>
      <guid>https://justanotherdot.com/posts/pushing-the-boulder.html</guid>
      <pubDate>Thu, 19 Sep 2019 20:15:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Imagine you have a giant boulder in front of you. This is the task you want to
undertake. You know you need to push it to get it moving but you also recognise
that the inertia you'll need to overcome at first is substantial. You
unfortunately think that pushing the boulder will <em>always</em> take this much force.
The truth is, if you can get the boulder moving, keeping it in motion only takes
many small, infrequent pushes. The idea behind pushing a boulder is the same as
<a href="https://jsomers.net/blog/speed-matters">fast systems incurring usage</a>. Want to
improve your coding? Code. Want to read more? Read. <strong>Action begets action</strong>.</p>
<p>When you try to acquire knowledge there is always a pool of unknown-unknowns
whose size is, itself, infinite. We are constantly refining things from the
unknown-unknown pool into known-unknowns while we learn. One way to acquire
knowledge is to read. I recently joked on twitter about speed reading:</p>
<blockquote class="twitter-tweet">
  <p lang="en" dir="ltr">
    Here’s how you speed read: flip through a book and tell people you’ve read it.
  </p>
    &mdash; Ryan James Spencer (@_justanotherdot)
  <a href="https://twitter.com/_justanotherdot/status/1170118831219474433?ref_src=twsrc%5Etfw">
    September 6, 2019
  </a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>But in all fairness, this is a sensible approach. Some relate knowledge
acquisition as panning for gold. I think that's a misguided analogy because
panning for gold is <em>slow</em>. Yes, we want to find the gold and we're not quite
sure where it is, but no modern mining outfit would pan for gold these days.
They would use large machines that could tear, demolish, crush, rip, sift, sort,
detect, and on and on in orders of magnitude time more quickly than someone
holding a pan to a river.</p>
<p>There is a technique involving multiple passes of reading. I learned this first
when I got into <a href="https://www.elsevier.com/connect/infographic-how-to-read-a-scientific-paper">research
papers</a>.
It's also the basic idea behind <a href="https://www.goodreads.com/book/show/567610.How_to_Read_a_Book">How to Read a
Book</a>. You read a
first pass involving the abstract/introduction, the conclusion, and then you
skim, noticing headlines, captions, diagrams, and all the other top-level items.
You follow suit with other passes increasing in detail if you haven't already
dropped interest in the material. Basically, actions get prioritised by cost,
the lower cost stuff coming first. <strong>Don't pan for gold, mine for it with
machines.</strong>.</p>
<p>Take a book you want to get through. Try to just flip through it. Once you're
done skimming, try to concentrate on the primary points you've just become aware
of; your known-unknowns. In your future passes, try to hone in on the things you
care about more. Reading books out of order is actually fine for a lot of
material if the first pass didn't tell you enough of what you need to know. What
this does is change your definition of done.</p>
<p>There is always signal-to-noise and it's largely why there are diminishing
returns which is both true for what we produce as well as what we consume.
Ruthlessly closing browser tabs to reduce distraction, keeping your output high
and frequent, reading books in multiple pass, and on and on. Do things to put
you in an uncomfortable state. Make tons of little changes and skim content
frantically going back for more when you need it instead of sitting around
dreaming or scheming. Be a giant machine. Tinker like a maniac. Push the
boulder.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Lightweight is Beautiful</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/lightweight-is-beautiful.html</link>
      <guid>https://justanotherdot.com/posts/lightweight-is-beautiful.html</guid>
      <pubDate>Sun, 08 Sep 2019 20:03:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>We are all guilty of having done the &quot;edit a little bit, go to another terminal,
hit the up-arrow a number of times, fire off the found command&quot; dance over and
over again at some point in our careers. It's such an easy automation to remove
these steps! IDEs give this to you because they know best about when a buffer or
a file has been saved or modified. Indeed, people go crazy for IDEs because they
provide information directly in the editor.</p>
<p>Even though things like VSCode and the Language Server Protocol have done a
tremendous amount of work in reducing complexity around both the setup and
maintenance of an IDE environment since days of yore, there are still times when
the array of plugins and external tooling 'go wrong'. Bugs or even the nefarious
'opinionated' feature can cripple a dev's workflow. Fixing these issues isn't
necessarily time poorly spent but it's hard to shrug off because the integration
is so tight-knit—now that you depend so heavily on the plugin, switching to
something different is slow. Here's an approach I think is a bit more
<a href="https://www.goodreads.com/book/show/13530973-antifragile">antifragile</a>, to use
a term coined by author Nassim Taleb. An antifragile approach is distinct from a
fragile approach because</p>
<ul>
<li>a fragile approach will break when encountering an unexpected event and</li>
<li>a robust approach does not change when encountering an unexpected event but</li>
<li>an antifragile approach gets better as it encounters unexpected events</li>
</ul>
<p>I'm a bit spartan when it comes to coding. I do this largely because I've had a
lot of tooling mistreat me and this has taught me that the weight of a tool or
process is a matter of its cost. <strong>Lightweight is beautiful</strong>. By lightweight we
mean cheap to replace not 'small' and 'simple'. Sometimes you do need beastly
machines because you can't bore a hole into the earth to make a tunnel with a
few workers armed with spoons. <strong>Lightweight functionality is preferable to
mindless adherence to a given tool or process.</strong> In other words, it's
antifragile to be prone to lightweight .</p>
<p>So here is the setup; two terminals or windows or whatever you like to use. In
one is your source code and in the other is your tests, linting, typechecking,
you name it. Either they are side-by-side or perhaps there is a dead-simple way
for you to swap between them. You can have several of these going at once and in
fact I recommend it. If they are resilient to files changing from version
control that's even better. <strong>It's important they stay <em>relevant</em> and by that I
mean obvious and up-to-date.</strong> When we talked about
<a href="https://www.justanotherdot.com/posts/stdout_is_forever.html">debugging</a>, this
is the very loop I was referring to. With this in place you can progressively
slap in debugging statements and changes while watching the results come seeping
out.</p>
<p>There are plenty of testing frameworks and tools that support automatically
running tests or commands on file save. <code>jest</code>, <code>PyTest</code>, <code>cargo watch</code>, <code>go watcher</code>, <code>mix watch</code>, you name it. This sets up an automatic link between the
file(s) being edited and the suite of tests to run. Just alleviating the step
where you need to context switch is the small win here and is not the point.</p>
<p>With this approach, if anything like a plugin or even a specific command in the
pipeline you setup goes awry, you can cheaply swap it out for an alternative.
<strong>This is the best kind of feedback loop as it favours tinkering and
experimentation.</strong> Lately because I mostly write Rust at work, I tend to use
<code>cargo watch</code> but one incredibly handy, language agnostic tool is
<a href="http://eradman.com/entrproject/"><code>entr</code></a> which is useful when I foray into the
unknown or uncommon. Let's say I find that I need to run a particular pipeline,
I can do that by running,</p>
<p><code>rg -l . | entr -cs 'cmd1; cmd2; cmd3'</code></p>
<p>Now if <code>cmd2</code> is being a pain, I can take it out of the pipeline quickly or even
choose to replace it. Perhaps it's a new project and you are furiously adding
files, you could set up a governing loop that watches all files and tears down
the loop if that changes some known set.</p>
<p><code>while true; do ls src/* | entr -d cmd; done</code></p>
<p>Most people never even think of doing a <code>git bisect</code> because of the pain of
steering the interaction with the bisect and running the tests to confirm the
first failure in the regression suite. This isn't just the cost of swapping
between terminals. Sometimes it can be tests that are flaky and come up as false
positives or maybe a test suite is slow to run but there is no way to neatly run
a subsection without commenting out code. With this approach, however, we can
focus on the steering and watch what happens in the other window. If flaky or
slow tests show up, we can comment them out and move on (<code>git clean -fdxx</code> is
handy for these sort of tempermental changes if you tack on it on the back of
the pipeline you construct).</p>
<p>If a great style guide favours <a href="https://www.justanotherdot.com/posts/a_plea_for_style_guides.html">deletability and ease of
modification</a>,
this approach is stressing for <strong>replaceability</strong> for producing tinker-friendly,
antifragile feedback loops. If you lower friction you'll always beget action,
and <a href="http://jsomers.net/blog/speed-matters">fast systems incur usage</a>.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Stdout is Forever</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/stdout-is-forever.html</link>
      <guid>https://justanotherdot.com/posts/stdout-is-forever.html</guid>
      <pubDate>Wed, 04 Sep 2019 15:49:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Debuggers are worth their weight in gold but stdout is the diamond in the rough.
All the tools we have to pinpoint problems such as REPLs, automatic tracing,
stacktraces, and even printing to stdout wind up being about two things:
<strong>poking</strong> and <strong>prodding</strong>.</p>
<h2>A useful macro or two</h2>
<p>Rust has the <code>dbg!</code> macro and I love it. It's short enough to type and it shows
you what file you are in, line you are on, and how the code looks plus its
value after evaluation. e.g. <code>dbg!(dbg!(12) == dbg!(1 + 11))</code> will print</p>
<pre><code>[src/main.rs:2] 12 = 12
[src/main.rs:2] 1 + 11 = 12
[src/main.rs:2] dbg!(12) == dbg!(1 + 11) = true
</code></pre>
<p>Two important quirks with this are,</p>
<ol>
<li>No arguments passed means you just get the file and line number</li>
<li>The code still behaves the way it used to except now you have tracing</li>
</ol>
<p>This gives us just enough information to be lethal. This is possible because
this expands at compile time and can be replicated in other languages that have
macro support. This is a source transformation and we can't easily use a
function because our line number will always be the line number of the function,
not the calling site. As such, one option is to write it as some repeated action
in your editor of choice. Imagine you have the following go code in front of
you,</p>
<pre><code>func AddOne(x Int) Int {
  return x + 1
}
</code></pre>
<p>and you want to lay down some tracing so you highlight the <code>x + 1</code> and hit a
keyboard shortcut which transforms the code into the following,</p>
<pre><code>func AddOne(x Int) Int {
  fmt.Printf(&quot;[src/main.go:8] x + 1: %#v&quot;, x + 1)
  return x + 1
}
</code></pre>
<p>We could have also used the
<a href="https://golang.org/pkg/runtime/#Caller"><code>runtime.Caller</code></a> function to get
filename and line number but we can get that spliced in via our editor to avoid
an import. If you are curious what the <code>runtime.Caller</code> code looks like here it
is (and, yes, I'm ignoring error handling here since this is intentionally
throwaway code):</p>
<pre><code>func AddOne(x Int) Int {
  _, file, line, _ := runtime.Caller(0)
  fmt.Printf(&quot;[%v:%v] x + 1: %#v\n&quot;, file, line, x+1)
  return x + 1
}
</code></pre>
<p>The advantage with the above is now we can take our print lines and move them
around at will and we won't have to tweak the filename/lineno combo.</p>
<h2>Poking</h2>
<p>Sometimes the fastest way to get at a problem is by writing test cases that flex
assertions about the functionality in question. Other times that's not as fast
because the logic might rely on other systems, e.g. integration tests. In those
cases, if you have stacktrace support you might find it useful to panic/throw if
particular assertions aren't met. When that fails you are probably interfacing
with code that is covering up exceptions or panics, say a piece of library code
that takes your code as a callback. You could try stubbing in your own forked
version of the code (scripting languages tend to make this easy) or you could
turn to building your own stacktrace. You iteratively apply print statements in
the following fashion,</p>
<pre><code>fn foo() {
  dbg!() # beginning
  &lt;snip&gt;
  dbg!() # middle
  &lt;snip&gt;
  dbg!() # end
}
</code></pre>
<p>With <code>dbg!</code> this is really easy because I don't have to think
about what to pass to the printing function since <code>dbg!()</code> simply
emits the filename and line number. In languages that may not have this I've
done <code>printf(X)</code> where X = &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, and so on.</p>
<p>With this format in place you can use binary search to figure out where you need
to apply more printing statements on each subsequent run. If, however, your
tests or program take a long while to run it can pay to do upfront work but
perhaps limiting yourself to an arbitrary depth to avoid spending too much time
on tracing that won't pay off.</p>
<h2>Prodding</h2>
<p>You can load your <a href="https://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/">core
dumps</a> into
<code>gdb</code> and explore the call stack after a segfault among all sorts of other cool
things that debuggers allow you to do, or you can rig up systems to
automatically provide tracing, such as in <a href="http://erlang.org/doc/man/dbg.html">erlang or
elixir</a> but hopefully this article has shown
that stdout gives you powerful debugging functionality since we already have
access to executing the program and manipulating its source. We can print
assertions to see if they hold up or mess around with alternative solutions that
may work if the problem is clear. Stdout isn't always the fastest but it's
<em>lightweight</em> which makes it invaluable as it can circumvent a lot of
preparatory work. You can pair this approach into a feedback loop, too, to
reduce duplicated work such as running the tests or program over and over again.
In a future article I'll discuss ways to do this in a range of languages and
environments but at least we've set the tone for some thinking about how to
improve what we spit out while you hack to give you a better understanding of
what's going on under the hood.</p>
<h4>Acknowledgements</h4>
<p><em>The name of this post is inspired from <a href="https://twitter.com/bodil/status/878563460233277440?s=20">Bodil
Stokke</a> when
responding to what &quot;What are everyone's fave debugging tools for languages you
write code in?&quot;</em></p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Principles</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-principles.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-principles.html</guid>
      <pubDate>Sun, 01 Sep 2019 18:43:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Programmers make assertions all the time with a wide range of utilities: tests,
types, written prose, and so on. Assertions are how we establish some ideal of
the system at any particular scope. Maybe there is a requirement that all
services in an ecosystem do not rely on each-other's internal implementation
details or that exceptions should not be thrown in library code. Perhaps a team
decides that despite correctness being of the utmost importance, it doesn't give
anyone the right to be an asshole.</p>
<p>Ray Dalio <a href="https://www.goodreads.com/book/show/34536488-principles">wrote a whole book on the matter of
principles</a> and feels
they are at the center for decision making and iterative improvement,</p>
<blockquote>
<p>Principles are fundamental truths that serve as the foundations for behavior
that gets you what you want out of life. They can be applied again and again
in similar situations to help you achieve your goals.</p>
</blockquote>
<p>My likening of principles to assertions is superficial. Assertions are, in my
mind, simply the application of principles. We <em>assert</em> that an invariant is
upheld during each iteration of a loop, that some postcondition is still met
after execution of a function, that a system is composable and modular from
applying principles sat around the locus of flexibility. In some cases these are
machine-checked, in others they rely on humans to verify. Regardless of checking
they are still present.</p>
<p>When you have autonomy in a team it is key that you take initiative to achieve
excellence. The problem with initiative is precisely an issue of what to work
on. Principles provide a 'north star' metric that enables you to say &quot;is the
error handling in this code dividing concerns between internal versus external
concerns?&quot; or possibly &quot;do people feel supported and empowered to make the
changes they need ot make across the broader company?&quot; <strong>With principles we can
scrutinize, but also work towards, some ideal. Hence our application of these
ideals is the work we undertake.</strong></p>
<p>One could easily take principles from elsewhere but that would leave us with
little to no understanding of how to form our own principles that best suit our
needs and desires. What is the trick? The process I'm about to describe is
probably not the only way to go about carving up principles but it's one that
has worked for me and others.</p>
<p><a href="https://www.justanotherdot.com/posts/a_love_letter_to_feedback_loops.html">Feedback
loops</a>
are how we take risks and reflect. Experiencing this cycle for long enough means
we start noticing
<a href="https://www.justanotherdot.com/posts/a_love_letter_to_patterns.html">patterns</a>
that can take the form of principles. A principle is really just a pattern that
can describe some ideal clearly. You make failures (by way of decisions), you
learn from failures (by way of reflection), you get principles.</p>
<p>I've been thoroughly enjoying <a href="https://www.goodreads.com/book/show/16158601-turn-the-ship-around">Turn that Ship
Around!</a> and
one of the mentioned mechanisms towards getting a &quot;leader-leader&quot; culture is
changing the focus from avoiding errors to achieving excellence. In this same
manner I think the best principles are formed. When I talked about <a href="https://www.justanotherdot.com/posts/make_a_home.html">treating
your codebase as a home you want to
build</a>, this 'idealised
life' that a home represents is the same as these principles I've talked about
here. In that sense, from a programmer's perspective, excellence is really just
the code and the culture we wish to embody.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Patterns</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-patterns.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-patterns.html</guid>
      <pubDate>Fri, 30 Aug 2019 21:47:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Patterns are how we tend to carve up the world around us. Like <a href="https://www.justanotherdot.com/posts/a_love_letter_to_feedback_loops.html">feedback
loops</a>,
they are everywhere once we start noticing them. Roger Antonsen <a href="https://www.ted.com/talks/roger_antonsen_math_is_the_hidden_secret_to_understanding_the_world">feels that
mathematics is all about discovering
patterns</a>.
When the pattern is discovered, we come up with a language to describe that
pattern and give us a handle on playing with it. When we have both of those we
can start tinkering and playing with assumptions.</p>
<p>Patterns are usually globbed together with abstractions. Abstractions are
usually misunderstood. What people tend to call abstractions are actually
indirections. Abstractions are actually the rough structure or form of an idea
or concept. The abstraction of a program is a specification for the program
itself. I don't think patterns and abstractions are the same.</p>
<p>So how are they useful in your day to day? You may or may not realise you do
what I'm about to describe but I'm fairly certain most of us do this regardless
of us being conscious of it. Becoming more conscious of this process makes it
more powerful, so let's shed some light.</p>
<p>When you are working, there is generally a discovery of patterns. You build a
trust framework around these patterns; if a pattern is useful, you keep using
it. Perhaps a pattern betrays you. Maybe you sit and dabble with it to make it
work again at the same capacity it used to serve you. If you're ruthless enough,
you ditch the pattern entirely and move onto new ones.</p>
<p>Consider a scenario where you are rigging up some code that needs to walk across
a directory and perform an action on every file. You see a third party library
is more ergonomic than the standard library so you use it, instead. If you're
conscious about this as a pattern, you'll trust it and reuse it tirelessly until
it stabs you in the back. It <em>doesn't</em> pay for you to go fiddle around with the
standard library functionality or some other third party library because you'll
be wasting time. This is also why trying to 'save' a forsaken pattern can come
with a high cost. It <em>does</em> pay to generally treat patterns as throwaway in the
spirit of speed. There's tons of these every day, in every language, and even
beyond coding, so you will not run out if you decide a pattern isn't delivering.</p>
<p>As patterns age and you spend some time thinking about them you'll find you can
increasingly discover ways to generalise them further. Patterns are amazing
because they let you turn all the boilerplate into habits which means you'll
have the mental room to solve other problems with deliberate attention.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Love Letter to Feedback Loops</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-love-letter-to-feedback-loops.html</link>
      <guid>https://justanotherdot.com/posts/a-love-letter-to-feedback-loops.html</guid>
      <pubDate>Thu, 29 Aug 2019 21:28:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Feedback loops are everywhere and they're <strong>awesome</strong>. In essence, whenever
there is some system with inputs, outputs, and some readjustment based on the
outputs of the system, that's a feedback loop. The analysis of the output 'feeds
back' to the input and the system is now different, and hopefully better,
because of it.</p>
<p>what about refinement of the feedback loops themselves? Truly productive people
not only recognise feedback loops but relentlessly modify them so they reach
their full potential. We tend to call this act of refinement as making the
feedback loop 'tighter', although that might not mean making the delay between
some analysis and the adjustment shorter, <em>per se</em>.</p>
<p>For example, you might code and run something like <code>cargo watch</code>, <code>jest</code>,
<a href="http://entrproject.org/"><code>entr</code></a>, et. al. to avoid having to switch to your
terminal, find the test command, and hit enter. Dropping those steps makes the
loop 'tighter' and means you get feedback (the state of the tests) far sooner
(and automatically).</p>
<p>Or perhaps you are a car manufacturer who gets reports from the dealerships
about sales to help you better gauge how many to make next. If you react too
quickly you might wind up producing a surplus right before a lull in the market.
<a href="https://www.goodreads.com/book/show/3828902-thinking-in-systems">Thinking in Systems: A
Primer</a>
discusses this same case.</p>
<p>Refining a feedback loop is really a matter of finding the sweet spot where it
delivers the most value.</p>
<p>If you're mathematically inclined or curious, there is <a href="https://en.wikipedia.org/wiki/Control_theory">control
theory</a> for designing feedback
loops in mechanical and dynamical systems. This <a href="https://www.youtube.com/watch?v=O8xLxNje30M">AWS
re:invent</a> talk gives a good
practical application of the control theory to software with respect to the
creation of S3.</p>
<p>When it comes to feedback loops for humans, we need to take risks to make
failures to learn. We learn more from our failures than our successes, but we
can't make huge failures all the time. <a href="https://www.goodreads.com/book/show/34536488-principles">Ray Dalio's
Principles</a> describes
the process as reaching for goals (taking risk), experiencing failure
(inevitably), learning from the failures (growth), and then upping the audacity
of said goals. This is pretty cool because it means it's not only OK to
experience failure but it's crucial to achieve what we truly want in life. The
sweet spot is experiencing enough failure so as to allow one to come back with
big dreams.</p>
<p><a href="https://increment.com/testing/i-test-in-production/">Charity Majors reminds
us,</a></p>
<blockquote>
<p>It’s better to practice risky things often and in small chunks, with a limited
blast radius, than to avoid risky things altogether.</p>
</blockquote>
<p>You can't think through everything. Sometimes you'll need to tinker; take bold
risks, make mistakes, refine, and repeat.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Plea For Style Guides</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/a-plea-for-style-guides.html</link>
      <guid>https://justanotherdot.com/posts/a-plea-for-style-guides.html</guid>
      <pubDate>Wed, 28 Aug 2019 20:16:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>You commonly hear two particular attributes that drive style guides, and,
subsequently, automatic formatting tools: 'consistency' and 'readability'. The
argument goes that a developer reads a codebase far more than any other
interaction.</p>
<p>Now, you could always take a codebase that has a style and use <a href="https://github.com/antlr/codebuff">machine learning
to generate a formatter</a> to keep things
'consistent' and 'readable'. This would get around the subjective definition of
readability because it's what the team picked through usage. Some feel that a
community driven style guide is ideal because then the codebase's 'readability'
is 'consistent' with the larger ecosystem, so formatting tools should simply be
blindly adopted.</p>
<p>Unfortunately, they are focusing on the wrong thing.</p>
<p>I have read a lot of code. I care about it as a practice and I like teaching
others how to do it, but I don't think it's the right metric for a style guide.
Bar things like the <a href="https://www.ioccc.org/">obfuscated C contest</a>, minified
markup and javascript, and many other mind-melting formats , most code I see is
actually quite 'readable'. Consistency is no better because you can have a style
that is consistently spaghetti.</p>
<p>In truth, <strong>developers change code far more often than they read and write new
code and they sure as hell should be <em>deleting</em> code with a frantically high
frequency, as well, if they aren't already.</strong></p>
<p>About two years back someone mentioned <a href="https://elm-lang.org/docs/style-guide">the elm style
guide</a> to me. The focus on
ease-of-modification <em>for a human</em> was eye-opening. With this mindset, alignment
was pointless. What good would it do a developer to re-align things after making
a change than to simply let them make the change by itself, communicate it
simply to their peers, and get it into master ASAP?</p>
<p>Then, later, another practice I adopted was adding newlines to assignments/let
bindings, taken from a team of brilliant software engineers. Every time I wrote
an <code>=</code> I would hit enter, allowing the name of the thing and its guts to be
distinct. The contents of the variable could be expanded, shrunk, removed
entirely, turned into an error, whatever. The name could be changed to better
suit constantly shifting needs and not highlight the guts in code review. It was
a handy pair.</p>
<p>I found sometimes having stuff as modifiable or deletable meant you would get
the other for free, similar to what people claim about consistency and
readability. In the end, the specific practices aren't important here. What is
important is that gaining momentum and keeping up against inertia is pivotal
when keeping a project relevant and rot-free. And, if you do the same thing
everywhere, you'll inevitably get a 'consistent' codebase, anyways!</p>
<p>Next time you write a style guide, try to think about the sea of changes that
will need to take place and the stuff that will get old and need to die before
you consider your codebase as another magnum opus.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>May You Be The Author of 2^N Programs</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html</link>
      <guid>https://justanotherdot.com/posts/may-you-be-the-author-of-two-to-the-n-programs.html</guid>
      <pubDate>Mon, 26 Aug 2019 20:00:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>The sheer propensity of articles detailing productivity tips for software
developers under the auspices of them becoming better employees is alarming.
What about our mental health? Why not more articles about our productivity from
the point of view of improving our ability to deal with discomfort, burn out,
imposter syndrome, and so on?</p>
<p><strong>Progress is key.</strong></p>
<p><a href="http://jsomers.net/blog/speed-matters">Speed matters</a> because it principally
enables us to make progress with our work and our life. Half-finished projects
weigh on us like mangled fruit on a dying tree. Project deadlines sneer at us
right around the corner and we put unfair stress on ourselves to both reach a
deadline <em>and</em> achieve excellence. <strong>We cannot be happy and, therefore, productive
if we are unable to be flexible. The only way is to accept mess.</strong></p>
<p>The book <a href="https://www.goodreads.com/book/show/187633.Art_and_Fear">Art and Fear</a>
is chiefly about two things:</p>
<ul>
<li>Proliferation and practice are the way to improvement</li>
<li>Successes can only be determined based on one's own history</li>
</ul>
<p>One analogy that is given in the book is a pottery instructor who divides his
class in two. Half of the class will be judged by quantity and the other half by
quality. In the end, it is the quantity group that has the best work through the
simple act of constantly learning from their failures. Reflection and planning
<em>are</em> crucial but the quantity-based group's ability to accept the mess that
comes with failures is what drives their progress. Fantastic! All we need to do
is just constantly crank things out and we'll be masters of our medium in no
time.</p>
<p>Except for some of us mess instills great discomfort.</p>
<p>At the beginning of the year I went to therapy. I was burnt out from work and
other stressors in life. Many sessions later I am diagnosed with Obsessive
Compulsive Disorder, which in some ways was a shock and in others not a
surprise. Eventually, exposure therapy comes up as a tool to help tackle the
anxiety and stress from parts of my affliction. There I was, sitting with my
discomfort, watching it, not trying to solve it nor run away from it, and sure
enough, the discomfort would melt away with time. I started convincing some part
of myself that the discomfort I was dredging up was <em>not</em> based on fact and I
found myself able to do things I had been so slow or entirely unable to do for
ages.</p>
<p><strong>It has dawned on me that sitting with the discomfort of what you produce is part
of learning from the failures.</strong></p>
<p>You <em>can</em> defer some decisions for later and put in rudimentary solutions in the
meantime. You <em>will</em> probably save time by making progress in this manner that
will allow you to revisit those same problems you deferred. Sure, this may not
always be the case for those trapped in <a href="https://cutle.fish/blog/12-signs-youre-working-in-a-feature-factory">feature
factories</a>,
but that's an organisational issue rather than a personal one. You owe it to
your mental health to grow more flexible by accepting mess.</p>
<p>May you the author of 2^N programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Move Fast and Tuck Code Into the Shadows</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html</link>
      <guid>https://justanotherdot.com/posts/move-fast-and-tuck-code-into-the-shadows.html</guid>
      <pubDate>Fri, 23 Aug 2019 21:51:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Migrations are a part of life as a dev. They help <a href="https://lethain.com/migrations/">cut down tech
debt</a> but they can be risky. It's always less
risky merging in <em>new</em> and <em>different</em> sets of changes rather than changing code
in-place. This buys you time. <em>You</em> gain the control over the switch granted
switching doesn't adversely affect some shared, mutable store of data.</p>
<p>The <a href="http://sevangelatos.com/john-carmack-on-parallel-implementations/">parallel implementation
approach</a> is
brilliance incarnate; you keep a functional reference implementation and you
copy it as your 'experimental' version whose sole aim is to eventually replace
(and hence become) the new reference. However, Carmack hits a good point,</p>
<blockquote>
<p>It is often tempting to shortcut this by passing in some kind of option flag
to existing code, rather than enabling a full parallel implementation. It is
a grey area, but I have been tending to find the extra path complexity with
the flag approach often leads to messing up both versions as you work, and
you usually compromise both implementations to some degree.</p>
</blockquote>
<p>I am keen to start experimenting more with the Carmack approach, though. Some
things I've already thought about:</p>
<ul>
<li>Having a duplicated directories messes up navigation for a lot of editors and
is unnecessary bloat</li>
<li><code>git flow</code> styled approaches and any vcs-based approach will never work
because it lends into the 'change in place' idea by merging the reference with
the experiment</li>
</ul>
<p>Otherwise, there are many ways to define clear boundaries between the reference
and experimental implementation. The most popular solution out of many is
feature flag services but I recommend switching between whole modules rather
than having a lot of logic caked into modules to check flags. Keeping flags
macro and mutually exclusive is important because it means changes are kept
cohesive and conflict free. One thing people tend to forget about is the
original feature-flag: versioning. In the end it doesn't matter which technique
you employ so long as you can 1. <strong>toggle between changes</strong> and 2. <strong>keep
differences clear</strong>.</p>
<p>You can start something similar to this approach by focusing on leaving written
code in a disconnected state but being aggressive about it finding its way to
master. This is healthy because it will change the incumbent attitude of
&quot;production means done&quot; to &quot;production means refinement&quot;. I like this approach
and do it as often as I can remember to because it means PRs are kept small
(great for code review) and when I finally do want to rig everything up I can
focus squarely on the plumbing, rather than juggling the correctness of the core
implementation <em>and</em> the coupling to the rest of the system.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Make a home</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/make-a-home.html</link>
      <guid>https://justanotherdot.com/posts/make-a-home.html</guid>
      <pubDate>Thu, 22 Aug 2019 20:49:00 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I'd like to preface this article that analogies are rough comparisons. That is
why, after all, they are analogies. We say that one thing is <em>like</em> another, but
it is not to say they are the same, point-for-point.</p>
<p>The analogies to construction and architecture in software are abundant. We say
that we are 'building' a codebase. We assign people the role of 'architect'. One
camp of people regale <a href="http://www.laputan.org/mud/">The Big Ball of Mud</a>,
esteeming the progress they make as their rationale for constructing a wobblying
shantytown. Another camp sit high in <a href="https://blog.codinghorror.com/ivory-tower-development/">The Ivory
Tower</a>, planning the
sanctuaries that may give their inhabitants stamina as they rest in its glory.</p>
<p>In Architecture of Happiness, Alain de Boton waxes,</p>
<blockquote>
<p>Beneath the pleasure generated by the juxtaposition of order and complexity,
we can identify the subsidiary architectural virtue of <em>balance</em>. Beauty is a
likely outcome whenever architects skillfully mediate between any number of
oppositions, including the old and the new, the natural and the man-made, the
luxurious and the modest, and the masculine and the feminine.</p>
</blockquote>
<p>What analogy sits between these two extremes? Where is the balance between
progress and stamina?</p>
<p>I feel like it's building a home.</p>
<p>Recently, I watched <a href="https://www.youtube.com/watch?v=AxM9FYSs8V4">a man building his own log
cabin</a>, slowly outfitting large
portions while also doing his chores from day to day. You can tell the focus
placed on making it a lovely, warm place people can relax but also the
willingness to accept the mess where it need to be accepted. As Jonathan Blow
eloquently puts it in a <a href="https://www.youtube.com/watch?v=6XAu4EPQRmY">portion of one of his live
streams</a>,</p>
<blockquote>
<p>We are ignoring [a pointer problem] for now and we are making a note that that
problem needs to be solved. You don't [sic] so here's the thing, in a big
project you just don't have to solve every problem at once in fact if you try
you will not get very far at all. You'll just get crushed under the load under
all the things you have to do and of never getting anything done ...</p>
</blockquote>
<p>Jonathan goes on to state how he subdivides problems into ones he wants to
seriously tackle now, and ones where he is putting in a rudimentary solution so
long as it gains him progress. All of this, to me, is the same sort of balance
that comes in the mental idea of having one's own house.</p>
<p>Most likely several things are demanding ones attention for fixing; you might
need to get a wall repaired, perhaps a door is more important since it's
front-facing, and you need to rake out front and put out the trash. Despite all
this effort, you spend many relaxing moments in your home, recharging and
growing with the environment built up around you. Alain de Boton feels homes
represent our ideal lifestyles, and experienced programmers do the very same on
their codebases; they make it home not only for themselves but for others.</p>
<p>Maybe we all ought to make homes so we can grow and feel happy in them, without
sacrificing progress and stamina.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Custom Search Functionality for Coding</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/custom-search-functionality-for-coding.html</link>
      <guid>https://justanotherdot.com/posts/custom-search-functionality-for-coding.html</guid>
      <pubDate>Mon, 19 Aug 2019 16:10:36 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>This may not be revelatory to some, but it's a cool trick I use daily and I
thought I'd write about since it's managed to surprise enough colleagues and
friends when I've used it. Credit where credit is due, I was taught this trick
two years ago by Charles O'Farrell.</p>
<p>Firefox and Chrome both support this functionality but are setup differently.
Let's say you have a github codebase with a particular org (which are also,
confusingly, demarcated as users in github search). You want to find a repo
quickly; you can quickly go to your search bar and hit <code>repos a_project</code> (or in
the case of Chrome, <code>repos&lt;tab&gt; a_project</code>), hammer the enter key and you wind
up at <code>https://github.com/search?q=user%3Aorg+a_project</code>. How?</p>
<p>In both Chrome and Firefox, you can add a custom search engine by right-clicking
on the search 'bar' (form) you'd like to add, except in Firefox the mechanism
works via bookmarks and Chrome has the functionality as it's own thing (seems
like a first class citizen). There <em>are</em> custom search engines for Firefox you
can add (I've noticed I can add them for things like crates.io, docs.rs, amazon,
et. al.) out of the box given a specific version (I'm not sure which) of Firefox
but you'll still need to bookmark approach for most cases. Once you have the
search URL you care about just replace the term you searched for with <code>%s</code> and
all is well, e.g. <code>https://github.com/search?q=user%3Aorg+%s</code>.</p>
<p>Some other examples of uses are:</p>
<ul>
<li><code>code term</code> - similar to the <code>repos</code> keyword above but searches across all
repositories of an org for <code>term</code></li>
<li><code>(docs.rs|crates.rs|younameit) &lt;module&gt;</code> - looks for module documentation,
listing in some large store of knowledge</li>
<li><code>rstd term</code> - search for <code>term</code> in the rust std lib (handy when paired with
something like rust-tags so you can jump to definition inside of std in your
editor)</li>
</ul>
<p>The above are rust centric because it's what I've been in the headspace of but
you could easily set this up for things like hoogle, amazon, shortening things
like youtube to <code>y</code>, <code>hex.pm</code>, and so on. Personally, it's been empowering to
gain a handle on parameterizing search functionality with your address bar.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Review for 2018</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/reading-review-2018.html</link>
      <guid>https://justanotherdot.com/posts/reading-review-2018.html</guid>
      <pubDate>Wed, 12 Dec 2018 21:09:20 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Being a voracious reader, one thing that helps bring the mountain of things I
want to read down to something manageable is being able to read things faster.
Reading <em>faster</em> itself probably hurts comprehension and digestion of core
concepts, but getting digested content from others who have already read the
main body of work can drastically reduce the amount of fluff you'll have to wade
through yourself. If you've ever read a book review that basically told you
everything you needed to know before you've read the book, you'll know what I'm
talking about.</p>
<p>I've personally felt this way a lot towards reviews and have found it
invaluable. As such, I feel it's necessary for me to pay this back. It's not
worthwhile giving rundowns for <em>everything</em> I've read so the purpose of this
article is to focus on the things I've found fascinating. This is by no means a
comprehensive list and you'll probably find the details per body of work a tad
thin as time has waned on since I've read them. However, I do wish to make this
more of a regular habit and, in the end, hopefully I can at least express the
delight I've had getting through some of these wonderful wading pools of words.</p>
<h2><a href="https://www.amazon.com/Coders-Work-Reflections-Craft-Programming/dp/1430219483/ref=sr_1_1?ie=UTF8&amp;qid=1544954623&amp;sr=8-1&amp;keywords=coders+at+work">Coders at Work</a></h2>
<p>This had so many fantastic insights into programming in the large as well as the
small for some of my heroes as well as some people I had never heard of before.
Of particular note was the recurring question Peter Seibel kept bringing up:
&quot;How do you read code?&quot;. In fact, Seibel even did a follow-up blog post on the
subject, but my particular take was the specifics that people actually replied
with. In fact, the subject matter drove me to write an article about reading
code I dubbed <a href="https://justanotherdot.com/posts/Reading_Code_is_Decoding.html">'Reading Code is
Decoding'</a></p>
<p>One other thing I thought fascinating was that one of the leaders of the Haskell
language, Simon Peyton Jones, had feelings towards types as proof systems that
were less intense as I had originally assumed. In fact, SPJ repeatedly mentions
that the idea is about <em>confidence</em> and not about mathematical fact.</p>
<p>Lastly, guys like Brad Fitzpatrick and Jamie Zawinski 'got things done', but
they also had a lot of sensibility towards quality instead of simply being
<a href="https://www.joelonsoftware.com/2009/09/23/the-duct-tape-programmer/">'duct tape
programmers'.</a>
In my mind, the book does a phenomenal job of showing how several people can
think of simplicity from different angles, whether that's avoiding the barbarism
of C++ in the face of Netscape's pre-existing C codebase, avoiding unnecessary
enterprise software in exchange for possibly less-than-ideal open-source
solutions to keep Live Journal up and running, or sussing out obtuse assembly
language for an unknown system by mathematically dissecting hunks of code.</p>
<h2><a href="https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth-ebook/dp/B06XP3GJ7F/ref=sr_1_2?ie=UTF8&amp;qid=1544954644&amp;sr=8-2&amp;keywords=the+manager%27s+path">The Manager's Path</a></h2>
<p>Soft skills are hard, and we'll see in a few other reviews that they are even
harder to write about and absolutely harder to put into practice properly! But
that still doesn't mean people can't try. Things of note:</p>
<ul>
<li>
<p>You will sometimes need to do hard things and tell people hard news, but it's
better you do that than try to pretend everything is amazing. In other words,
It's better to be 'kind' (honest but considerate) than simply 'nice' (always
accommodating)</p>
</li>
<li>
<p>As you may find yourself with more managerial duties, you're ability to bridge
the gap between technical and non-technical members of the business increases;
this can be with metrics and even how you handle explaining or using
alternative jargon</p>
</li>
<li>
<p>A healthy codebase is an active one, but the rate of change needs to be kept
in check with the rate of errors. There is a subtle hint here about developing
a devops culture so that people are aware of how they can best help ship
software and know if they are within a predefined error budget. If you impede
the error budget, it's time to start focusing more on fixing things, which
means having flexible scheduling percentages.</p>
</li>
<li>
<p>Learning is the most vital aspect of a business and orienting it's processes
around it is pivotal in it's success</p>
</li>
<li>
<p>Having technical chops before you wind up in any managerial capacity is
crucial but it's also important to know that things like leadership and
empathy takes just as much effort to refine and perfect</p>
</li>
</ul>
<p>I feel like a good follow up to this was reading John Allspaw's article <a href="https://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/"><em>On Being a Senior Engineer</em></a>.
In it, Allspaw hits on several soft skills that 'senior' software engineers
absolutely must uphold on a day to day basis, and I really don't see them being
that different from soft skills that others should be upholding as well as they
increase in seniority.</p>
<h2><a href="http://mcfunley.com/choose-boring-technology">Choose Boring Tech</a>, <a href="https://www.intercom.com/blog/run-less-software/">Run Less Software</a>, and [You Need a Novelty</h2>
<p>Budget](https://www.shimweasel.com/2018/08/25/novelty-budgets)</p>
<p>I mention these three articles in tandem because they were thoughts I had
faintly felt being in various organisations but could not pinpoint with words
quite as well as these three articles do. The core ideas are:</p>
<ul>
<li>
<p>You don't always need to map every problem to an ideal solution, in fact,
doing so is problematic because you will wind up with too much tech to
maintain, and maintenance cost must weight in considering adoption</p>
</li>
<li>
<p>Every new piece of tech you acquire into your stack means more things people
need to know about and the more people need to know means the less they can be
experts.</p>
</li>
<li>
<p>Choosing or building swanky libraries, technology, and services may be
exhilarating but it's also problematic for the above reasons. Having a
'novelty budget' can help prevent 'fancy tech creep' into the codebase and
infrastructure. In fact, it may help to run as many crazy projects 'on the
side' (i.e. outside of work) as possible to help reduce the urge of
introducing shiny-new-things</p>
</li>
</ul>
<p>I kept going back to these articles at least every month or two; they serve as a
basis of what I think is productive coding: write boring code that <a href="https://www.youtube.com/watch?v=4Y0tOi7QWqM">fits in your
head</a>, is heavily tested, focuses
on some clear specification (the proper abstraction), and satisfies &quot;small is
beautiful/less is more&quot;. I can't imagine that being the end of the proper
characteristics and that list keeps morphing as I keep coding, but it seems
sensible enough to mention those characteristics in relation to these posts.</p>
<h2><a href="https://www.amazon.com/Twelve-Steps-Compassionate-Karen-Armstrong-ebook/dp/B003WUYPBA/ref=sr_1_1?ie=UTF8&amp;qid=1544954743&amp;sr=8-1&amp;keywords=12+steps+to+a+compassionate+life">12 Steps to a Compassionate Life</a></h2>
<p>For trust to cultivate between members working together to build impossible
programs and systems you need respect and you can't build respect until you have
empathy for others. You don't need to feel the same thing they feel, per se, but
compassion itself is, in my mind and shaped from the sentiments in the book, the
difficult but extremely rewarding practice of cultivating concern for others,
their suffering and feelings.</p>
<p>In the book, Armstrong equips us with a particular process of better developing
empathy for others via religious, sociological, and psychological references.
Since this is a heavy process-oriented book, I can't simply give a quick tl;dr,
but I do want to recommend this book heavily. It and <a href="https://www.amazon.com/How-Talk-Kids-Will-Listen-ebook/dp/B005GG0MXI/ref=sr_1_4?ie=UTF8&amp;qid=1544953942&amp;sr=8-4&amp;keywords=how+to+talk">How to Talk so Kids Will
Listen and Listen so Kids Will
Talk</a>
are both fantastic resources on this subject, although I've read the latter a
number of years before this post.</p>
<h2><a href="https://blog.acolyer.org/2018/08/20/filter-before-you-parse-faster-analytics-on-raw-data-with-sparser/">Filter Before you Parse</a></h2>
<p>The Morning Paper is probably <em>the</em> best resource for mind blowing information
on the internet for software engineers, and I could ramble off many articles
that absolutely twisted my brain this year. If you don't have a subscription to
the The Morning Paper, you need to go subscribe this instance! Despite it not
being the most mind-bending of articles this year, I wanted to point out this
particular piece and associated paper because it's such a &quot;oh <strong>of course</strong>&quot;
kind of moment.</p>
<p>The gist? Do a fast search on the JSON payload <em>before</em> transforming it into
it's in-memory representation.</p>
<h2><a href="https://lemire.me/blog/2018/05/03/how-fast-can-you-parse-json/">How Fast Can You Parse JSON?</a></h2>
<p>Daniel Lemire is a performance and database nut (and so I'm easily a fan). I
wanted to include this since I had also included the JSON parsing related paper
from The Morning Paper. In this short article, Daniel explores a few industrial
grade parser implementations and tries to get at how many cycles it would take
to parse per byte, eventually coming to the closing line of:</p>
<blockquote>
<p>So you should expect to spend 2 or 3 seconds parsing one gigabyte of JSON data.</p>
</blockquote>
<p>I am personally a fan of &quot;Latency Numbers Every Programmer Should Know&quot; and I
think things like this are actually handy, back pocket facts that ease
estimation, s.t. if you find you're parsing a gigabyte of JSON and it's taking
orders of magnitude more than this, you can probably know there are gains to be
made, and also know that there might be a happy limit for optimisation.</p>
<h2><a href="https://charity.wtf/2018/08/19/shipping-software-should-not-be-scary/">Shipping Software Shouldn't be Scary</a></h2>
<p>I have a number female software engineer role models; Julia Evans, Jessie
Frazelle, and Charity Majors, to name a few. Charity's kick is on empowering
devs to be both coding and operations gurus by empowering them with the
superpower of observability. This article by Charity also goes into some core
things about operational concerns that are &quot;everyone's problems&quot;. The hot take
is that devs should be able to take code from cradle to grave; ideation into
production which entails all the messy debugging and reversals after Things Go
Wrong. The aim is to make everyone a software owner rather than a mere
occasional participant.</p>
<p>It may also get the reward for the best work memes of 2018.</p>
<h2><a href="https://gist.github.com/bcantrill/835837a66bcc21b899f501dd794a7d5f">Bryan Cantril's 'internal' doc on hiring</a></h2>
<p>Bryan sums this up nicely in the beginning, I'll add my takeaways for each bit.</p>
<ul>
<li>
<p>Aptitude: can the person actually code?</p>
</li>
<li>
<p>Education: have they persevered through boring lectures and stress?</p>
</li>
<li>
<p>Motivation: are you driven to write code everyday or is it Just Another Job?</p>
</li>
<li>
<p>Values, per the doc itself:</p>
</li>
</ul>
<blockquote>
<p>One observation is that one's values -- and the adherence or divergence from
those values -- will often be reflected in happiness and satisfaction with
work. When work strongly reflects one's values, one is much more likely to
find it satisfying; when values are compromised (even if for a good reason),
work is likely be unsatisfying.</p>
</blockquote>
<ul>
<li>Integrity: The usual verification that people are who they say they are and
not actually crazy, serial axe murders or simply bullshit artists.</li>
</ul>
<h2><a href="https://lethain.com/migrations/">Migrations: the sole scalable fix to tech debt</a></h2>
<p>You have legacy technology and you wish it would just go away, but how are you
going to euthanize it? You could write a new system and hide the other one in
the corner, perhaps? But you end up realising that your business relies quite
heavily to this chunk of code and that it's actually grown into such a mess you
can't fathom now moving away from it entirely and now need to run two things;
this is a fairly common problem and I think the gist of this article was spot
on: if you want to deprecate A, you need to be focused and clear about how B is
going to entirely replace it's functionality to the point where it can be
politely deleted out of existence, and you need to make this a common practice
as an engineering team as every piece of code you write is legacy the moment you
write it. Code rots, and, as such, that shiny new piece of tech you've just
built will one day need a successor.</p>
<p>In general the process is the same for any piece of software: do upfront
planning with documentation and good old fashioned thinking, get people on
board, don't be the machine and automate as much of the migration as possible,
help <em>track</em> the actual shift away from the older platform, and finally, don't
forget to celebrate your achievements! That last one is huge because if you
don't make migrations a big deal, people won't see the value in it!</p>
<h2><a href="https://www.amazon.com/Writing-Without-Bullshit-Career-Saying-ebook/dp/B01A5CEKQM/ref=sr_1_1?ie=UTF8&amp;qid=1544955550&amp;sr=8-1&amp;keywords=writing+without+bullshit">Writing Without Bullshit</a></h2>
<p>I'm actually quite surprised that this book devolved into a description of a
particular process, but I think it's a worthy ally in the fight to cut the fat
out of one's prose (the emphasis is in a workplace setting). Two major things of
note I took from this book were:</p>
<ul>
<li>
<p>Be short and to the point but scrutinize your use of data.</p>
</li>
<li>
<p>It's incredibly useful to get out 'fat outlines' of work before you start
obsessing on details and get lost in the subsequent worrying. I write like
this now in general: large bulk of content upfront and several passes of
refinement afterwards.</p>
</li>
<li>
<p>The 'iron imperative': &quot;Don't waste the readers time&quot;</p>
</li>
</ul>
<p>That last point kept having me think about the book <a href="https://www.amazon.com/Dont-Make-Think-Revisited-Usability/dp/0321965515/ref=sr_1_1?ie=UTF8&amp;qid=1544955670&amp;sr=8-1&amp;keywords=don%27t+make+me+think">Don't Make
Me</a>
Think where the crux of design should be about alleviating cognitive load on the
user of the interface. It's a powerful phrase to have handy when needing to
point out really lengthy bits of code, comments, or documentation, as well as
general communiques.</p>
<h2><a href="https://www.amazon.com/Messy-Power-Disorder-Transform-Lives-ebook/dp/B01BD1SU2E/ref=sr_1_1?ie=UTF8&amp;qid=1544955751&amp;sr=8-1&amp;keywords=messy+the+power">Messy: The Power of Disorder to Transform Our Lives</a></h2>
<p>This book was monumental in getting me comfortable with the idea of doing rough
groundwork upfront. In fact, some people describe this as the <a href="https://en.wikipedia.org/wiki/Pareto_principle">'paeto
principle'</a> where 'eighty
percent of the work comes from twenty percent of the effort'. In my mind I'm
sure Pareto never intended this to be about a specific ratio but rather about
how a small portion of input winds up relating to a large portion of the output.
I actually found no better succinct description of the book except at the start
(the rest is still worth the read):</p>
<blockquote>
<p>The argument of this book is that we often succumb to the temptation of a
tidy-minded approach when we would be better served by embracing a degree of
mess.</p>
</blockquote>
<h2><a href="https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&amp;qid=1544955863&amp;sr=8-1&amp;keywords=programming+rust">Programming Rust</a></h2>
<p>I've not really read a specific programming-language book in awhile, but I love
systems programming and Programming rust (along with several other rust books)
focus heavily on the subject; whether it's talking about error handling, rough
edges with Unicode, or even how different languages handle assignment in order
to explain the borrow checker and moves, I was constantly enthralled to pick
this book up despite it being close to six-hundred pages. It's gained the
classic 'dirty and crushed pages' aesthetic and it'll probably gain a bit more
as I keep trying to double down on rust.</p>
<h2><a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?ie=UTF8&amp;qid=1544955948&amp;sr=8-1&amp;keywords=designing+data-intensive+applications">Designing Data-Intensive Applications</a></h2>
<p>I can't recommend this book enough. I actually finished reading this near the
end of 2017, but I love it so much that I wanted to write about it here. I
personally don't know of any other book that handles the fundamentals of
database internals (along with many modern improvements), database design,
clusters, nodes, schema encodings, discussions around guarantees (CAP, ACID) and
what are valid and invalid points, consensus in distributed systems, and even
modern data processing patterns all while in the context of proposing a credo
that data-intensive applications should uphold, in such an elegant and
ultimately fun way. I still read through the book for knowledge I may have
passed up or forgotten so it's a fantastic resource to have on my shelf.</p>
<p>It's also dawned on me that data processing <em>is</em> programming, and knowing more
and more about data and how to handle it is enlightening and empowering.</p>
<h2><a href="https://www.amazon.com/Thinking-Forth-Leo-Brodie/dp/0976458705/ref=sr_1_1?ie=UTF8&amp;qid=1544956045&amp;sr=8-1&amp;keywords=thinking+forth">Thinking Forth</a></h2>
<p><a href="https://twitter.com/lorentzframe/status/997997523301117953">Brian Beckman wrote a tweet I read awhile
back</a>, stating:</p>
<blockquote>
<p>&quot;SICP&quot; by Abelson &amp; Sussman should be read continuously, ~2 pages a day,
returning to page 1 every year. Ditto &quot;Thinking Forth&quot; by Leo Brodie, tho'
only ~1 page a day. The former teaches how to think, the latter how to
engineer. Both are in unpopular languages, on purpose.</p>
</blockquote>
<p>and having read most of the SICP, even watching the entire lecture series, but
never having heard of <em>Thinking Forth</em> (but having heard of the language), I was
intrigued to hear of a book put on the same page as the SICP! I am actually,
also, cheating here as I've read only half of it, but so far it's definitely a
different way of looking at software construction in the same vein that the SICP
went to lengths to describe.</p>
<p>The heaviest emphasis made so far in the book that I love seems drawn between
Domain Driven Design and traditional functional programming: words are vital to
how we code, the meaning they carry as well as their surrounding semantics; we
group words into lexicons which we can call 'components', and with 'components'
we get all the general benefits we get from composition. There is some forth
specific information nestled between, but <a href="https://twitter.com/lorentzframe/status/999121431559487489">Beckman is wise in a followup
tweet</a>, saying:</p>
<blockquote>
<p>If SICP were in, say, (popular) Python instead of (unpopular) Scheme, people
might be distracted by the Python and miss the thinking. Ditto if &quot;Thinking
Forth&quot; were in (popular) Java instead of (unpopular) Forth, people might miss
the deeper points about software engineering.</p>
</blockquote>
<p>I'm excited to finish it in January, in tandem with <em>Thinking in Systems: A
Primer</em>, which I've started just last week and has so far been an amazing way of
perceiving the structure of various systems from a proper, abstracted point of
view.</p>
<h2>Conclusion</h2>
<p>I originally thought a format involving simple review styled blurbs might be
handy, but it's kind of nice having not thought about some of these various
texts for a bit to really see what would stick. I hope this has been somewhat
useful or fun!</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>What makes a good pull request?</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/what-makes-a-good-pr.html</link>
      <guid>https://justanotherdot.com/posts/what-makes-a-good-pr.html</guid>
      <pubDate>Fri, 20 Jul 2018 20:40:08 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Pull Requests (or PRs) are a tango between two parties; the code author and the
code reviewer which I will simply refer to as the 'author' and 'reviewer' in the
remainder of this article. In a pull request, the author has provided code to
solve a particular problem and the reviewer is there to provide a feedback
mechanism to the author.</p>
<h2>Code review is not a gate keeping task</h2>
<p>I once worked for an organisation whose code review process centered around a
total lack of faith in its developers ability to deliver quality product; tech
leads acted as the gate keepers to their respective stacks forcing the average
developer to resort to underhanded tactics in order to get their changes
merged, regardless of quality or prospective bugs. This, in turn, meant the
gate keepers felt justified under the guise of 'keeping things safe'. Thus,
code wasn't being checked properly and wrong or flimsy changes would trickle
into master, making the code review process utterly broken.</p>
<p>As a software engineer, every line of code you ship is code you, or someone
else, will need to maintain, and as such, you should be fighting to deliver the
best quality you can offer, regardless of deadline. <strong>Code reviewers are there
to help people bring their code into the light of day where asking questions is
the chief tool a reviewer employs</strong>. This can include, but is not limited to,
probing to see if:</p>
<ul>
<li>Is the thought fully fleshed out?</li>
<li>Is this implementation correct for the problem it aims to solve?</li>
<li>Are the changes sound and principled?</li>
<li>Are there any performance or semantic concerns?</li>
</ul>
<p>I'm purposefully leaving out stylistic choices here as the discussion often
leads to the argument around adoption of some automated code-formatting tool.</p>
<h2>Raise early and raise often</h2>
<p>In &quot;Debugging Team's&quot;, the authors kick off the start of the book with a simple
analogy between two competing inventors. The one inventor does not want to share
his ideas for fear of them being poached by others, while the rival inventor
gleefully goes to local places where experts might hang out to get more
information about how to build her inventions.</p>
<p>Software development is no different in that knowing things earlier is always
better than knowing things later. Raising PRs even before they are 'complete'
(and appropriately marking them as WIPs or 'works in progress') allows people to
possibly, time permitting, look into your changes and see if there are any major
red flags.</p>
<p>That said, if you are a reviewer and are asked (or not!) to look at a set of
changes that is marked as a WIP, try to hold off on a more in-depth review until
the author changes this status. And to PR raisers, don't keep things in WIP
stage for too long, which brings me to my next point.</p>
<h2>PRs are for small chunks of code to merge often</h2>
<p>A PR should represent up to a day's worth of work. This is beneficial to both
parties in that it facilitates a 'merge often' approach for devs (and devs get
the little adrenaline kick from clicking that green <code>merge</code> button) and
reviewers can much more easily review a smaller hunk of changes. A reviewer
reviewing five PRs in the course of a week has to spend less time grokking
those individual changes than to review five days worth of work in a single PR.</p>
<p>Massive projects poised around scaled tooling and reviews such as the Linux
kernel would flat out reject a patch with, say, 3k additions and 1k removals. If
a project of that size and calibre, and that many international hands involved,
is marking code review of those dimensions as 'unmanageable', what hope does a
startup have at making fast, rapid changes in the same light?</p>
<p>Small PRs are also <em>focused</em> on a clear intent. Asking the author to fix
neighboring code 'just because' or refactoring/formatting several adjacent files
that are not directly tied to the immediate effort of the PR wastes the both
parties time. Opening a PR to refactor changes and another PR to add new
functionality is a much better way to get appropriate attention from reviewers.
As the joke goes:</p>
<blockquote>
<p>10 lines = 10 possible bugs, 100 lines = lgtm</p>
</blockquote>
<p>It's important to remember that a PR is not to encompass a single ticket or
issue. Tickets can have several PRs attached to them and all it takes is
lobbing <code>[FOO-123]</code> on top of one's PR title for Jira or marking <code>#&lt;issue number&gt;</code> in your description in GitHub. I like to call this act 'linking' and
it's useful for stakeholders to track down all the changes that have fed into a
particular ticket.</p>
<h2>Context matters</h2>
<p>Reviewers need to discuss with the author about the purpose of a set of changes
and how close or far off they are from that goal, but if the reviewer is
unclear about this goal, it's difficult for them to strike up a discussion with
the author.</p>
<p>In the context of OSS, raising a patch directly to a project such as the Linux
kernel is poor practice. If you want to make a change in any capacity it's best
first to contact the people who own the code on public channels. This provides
auditing and clear context for others. <strong>Your first instinct should be to raise
a PR unless it's a feature. If you feel uncertain about whether or not your
change is warranted, it's best to raise an issue first, instead.</strong></p>
<p>That said, raising PRs should feel natural; PRs are cheap and can be closed and
their branches pruned as need be, but regardless of the cost of raising a PR,
it's critical to include appropriate information. Some important things to
mention may be:</p>
<ul>
<li>What does this set of changes solve?</li>
<li>Is there a specific task (issue/ticket) that this relates to?</li>
<li>Is this blocked or blocking any other PRs/issues/tickets?</li>
<li>Is there any additional information that will help the reviewer know about my
manual testing of this ticket (screenshots, output from tooling, et. al.)?</li>
<li>Have you updated tests and documentation accordingly? Have you added tests
that the reviewer can skip to first to immediately see how you're proposed
changes are supposed to work and in what cases?</li>
<li>Is there current behaviour to contrast the new behaviour to?</li>
<li>Are there breaking changes present?</li>
</ul>
<p>The traditional approach for this was to include commentary in your actual
commit messages and headers. I don't think times have really changed in this
regard and the more context you sprinkle about the better, so long as you are
clear about your intent and you don't waste the reader's time.</p>
<h2>Check out changes locally when it makes sense</h2>
<p>A really healthy habit for reasonably sized changes is to always check out a PR
and see if it works for you. Some projects have powerful processes for testing
full 'e2e/raw hardware' scenarios such as Intel's <a href="https://01.org/lkp/documentation/0-day-test-service">zero-day testing
bot</a> which actually boots
up machines to test out differing version of the Linux kernel. While continuous
integration can catch a lot, it's important to sometimes get a human eye for
regressions that may not, or cannot, be encoded in automated tests. We all make
mistakes, and reviewers are there to add a layer of sanity checks to our
changes.</p>
<p>Use common sense. If a change is pretty sensible (e.g. a single line change to
update a variable name), you probably don't need to spend the time pulling the
change down, compiling, running the tests, and so forth. As the developer's
mantra goes, &quot;Don't be the machine&quot;!</p>
<h2>Clean up your mess</h2>
<p>Your changes have been merged and you can go on with your life, but before you
reach for beverage of choice, you should prune your dead branches. I actually
have a git bash script for this that I place in my <code>PATH</code> so I can call it as
<code>git wash</code>. The script is <a href="https://gist.github.com/justanotherdot/3e3a16df805d09a37e1c26bbedd23fcc">here</a>. When
run without arguments, this will delete the current branch you are on locally
and remotely so long as the branch specified to <code>git wash</code> is not master and the
remote branch is not protected. If you need other git functionality like this,
any script in your path with the name <code>git-&lt;thing&gt;</code> can be run as <code>git &lt;thing&gt;</code>.</p>
<h2>Conclusion</h2>
<p>Reviewing many small changes is much more manageable for a reviewer than
reviewing large, tangled changes. Decomposition in programming gives us the
ability to stitch together many small, verified solutions that lead up to an
equally trustworthy result and the same is no different in the process of
supplying code changes to a project; breaking up the changes you need to make
into reasonable chunks that can fit in everyone's heads not only helps to
provide code changes faster but it also paves foundations for robust and
resilient code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Trampling Trampolines</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/trampling-trampolines.html</link>
      <guid>https://justanotherdot.com/posts/trampling-trampolines.html</guid>
      <pubDate>Thu, 24 May 2018 19:40:17 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <h2>Continuation Passing Style</h2>
<p>or CPS for short, is a way to ‘continue’ a function call by calling into another function. The simplest example would be:</p>
<pre><code>function cps(x, return) {
  return(x);
}
</code></pre>
<p>The important thing is the callback is passed and it is called at the ‘completion’ of the function, passing things along.</p>
<p>CPS is not a problem in languages where Last Call Optimisation is commonplace. What these languages do (normally of eager evaluation) is collapse the stack frame and call the function call so long as there is nothing else beyond the function. <a href="http://erlang.org/pipermail/erlang-questions/2016-October/090663.html">Here’s a fun rant by Joe Armstrong regarding the implementation and implication of Tail/Last Call Optimisation</a>. Of note is how he is careful not to say this is the same as Tail Call Optimisation, or TCO, as that typically implies a recursive call.</p>
<h2>DIY</h2>
<p>Languages that don’t collapse the stack or represent functions in a form that is conducive to fusion are subject to ‘blowing the stack’ (exceeding it’s maximum size) when writing a recursive function. The function may be correct, but for particular values it may grow too large and too many stack frames will be pushed onto the program’s stack.</p>
<p>There are some <a href="http://chrispenner.ca/posts/python-tail-recursion">interesting ways to mimic TCO</a> in languages that don’t have native support for it. JavaScript does, technically, but implementation seems spotty across engines.  A clever way to mimic TCO in JavaScript is to rewrite your function to return a continuation, of sorts, and then wrapping it in another, generic, function (the trampoline) that knows about this arrangement in order to collapse the stack itself, for example:</p>
<pre><code>let ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`ERROR: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>I’ve chosen the <a href="https://en.wikipedia.org/wiki/Ackermann_function">Ackermann function</a> here as it’s a golden standard for testing recursive functionality in programming languages since its value grows rapidly even for small digits. I’m also using the convention of naming things as <code>go</code> to specify recursive helper functions with arguments we don’t care about. A common counterpart to <code>go</code> you might see (particularly popular in LISPs) is <code>do</code> .</p>
<p>If you play around with this function you should quickly find it exceeding the allotted call stack size. Let’s fix this with a trampoline:</p>
<pre><code>let trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (typeof rv === 'function') {
    rv = rv();
  }
  return rv;
};

ackermannGo = (n, m) =&gt; {
  if (m === 0) {
    return n+1;
  } else if (m &gt; 0 &amp;&amp; n === 0) {
    return () =&gt; ackermannGo(m-1, 1);
  } else if (m &gt; 0 &amp;&amp; n &gt; 0) {
    return () =&gt; ackermannGo(m-1, ackermannGo(m, n-1));
  } else {
    throw new Error(`urk: unhandled case m: ${m} and n: ${n}`);
  }
};
</code></pre>
<p>Note the zero-arity functions we are returning in order to signal that the return value can be applied (specifically lines 13 and 15). What kind of functions will <code>trampoline</code> fail on given it’s current implementation? When you’ve given it some thought, consider this case:</p>
<pre><code>let higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return offset =&gt; acc+offset;
  }
  return () =&gt; higherOrderFunc(n-1, acc+n, offset);
}

let hof = trampoline(higherOrderFunc);
</code></pre>
<p>Although this will terminate, it won’t give us the correct result. We want a closure in the end, but here we will wind up with <code>NaN</code> since that is the result of adding any number in JavaScript to <code>undefined</code>, and since we are calling the result value, <code>rv</code>, with no arguments, we are technically passing <code>undefined</code> to this final value.</p>
<p>A <em>structural</em> type system is one which cares only about the form of given values, whereas a <em>nominal</em> type system cares about the <em>names</em> that values have in the system. In a language like Haskell, we’d use what’s known as a “data constructor” to declare a type. Consider:</p>
<pre><code>data Cont a = Stop a | Cont (() -&gt; Cont a)
</code></pre>
<p>This says “for any given value of <code>a</code>, I can either be a <code>Stop</code> value wrapping some given value of <code>a</code>, or I can be a <code>Cont</code> wrapping a function which takes <code>unit</code> (the <code>()</code>, or, in other words, no arguments that matters) to another <code>Cont</code> value”</p>
<p>We can, again, mimic something similar using a <code>tag</code> field on an object, a la:</p>
<pre><code>higherOrderFunc = (n, acc, offset) =&gt; {
  if (n &lt; 1) {
    return {
      tag: 'stop',
      val: offset =&gt; acc+offset
    };
  }
  return {
    tag: 'cont',
    val: () =&gt; higherOrderFunc(n-1, acc+n, offset)
  };
};
</code></pre>
<p>Which we can leverage in a new definition of <code>trampoline</code>:</p>
<pre><code>trampoline = fn =&gt; (...args) =&gt; {
  let rv = fn(...args);
  while (rv.tag === 'cont') {
    rv = rv.val();
  }
  return rv.val; // Of tag 'stop'.
};
</code></pre>
<p>We only have two cases for our ‘type’, so when we no longer have a <code>cont</code> tag, we must have a <code>stop</code>, and with this we get the correct result: a function!</p>
<h2>Conclusion</h2>
<p>In mathematics it is just as important to reason about the types of objects we are dealing with as it is to reason about their values. The same is no different in programming. Even though you may say “I hack in a dynamically typed language, I don’t need to think about types”, the inverse is actually the truth! Hacking in a dynamically typed environment means juggling these notions around in your head rather than allowing the type checker make sense of the form of things for you.</p>
<p><em>food for thought:</em> What we’ve done by tagging these values is upgrade a <code>union</code> type into a <code>discriminated union</code>. A <code>discriminated unions</code> is also sometimes known as a <code>sum</code> type. The power of passing around sum types is that we can reason about the cases in our code: recursion itself is similar to mathematical induction, and both are forms of breaking down data whereas their duals, co-recursion and co-induction, build up data. This is an important notion because it means that when we write things recursively in this form with inductive-like types (e.g. sums) we can ‘pattern match’ on their values and know something about each case. In the above examples we knew that <code>cont</code> always contained a function that took no arguments and returned another <code>cont</code> or <code>stop</code> tagged object. When we finally got a <code>stop</code> we knew we had our final result we could return, and since there were only two values we could be sure that those were the only two cases worth exploring (show exhaustivity checking as well as case analysis). As is common with FP, some languages give you the power of actually statically checking this; ensuring that you’ve considered all cases, whereas in others you’ll be left to discipline or libraries to replicate this, just like trampolines.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Proficiency is Tiered and other Lies We Tell Ourselves</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/proficiency-is-tiered-and-other-lies-we-tell-ourselves.html</link>
      <guid>https://justanotherdot.com/posts/proficiency-is-tiered-and-other-lies-we-tell-ourselves.html</guid>
      <pubDate>Thu, 01 Mar 2018 16:48:02 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Tiered categorisations of knowledge and proficiency are fundamentally flawed as
they rest on the notion that all knowledge can eventually be obtained,
retained, and divvied up amongst n-many categories. Such categorisations also
ignore the fact that most skills rely on overlapping knowledge from various
domains. In this article I propose a way to evaluate subject matter in the
context of best prioritising <em>what should I learn next?</em> I’ll also offer up an
approach to evaluating others that isn’t based on ‘skill level’ (admittedly
regurgitated from Amy Cuddy).</p>
<h2>A Tagging System</h2>
<p>Instead of suggesting that knowledge from a domain of expertise can be chunked
and tagged in toto, we consider an alternative tagging system where we look at
knowledge from three types of labels and, most importantly, accept that the
fringes are fuzzy:</p>
<ul>
<li>Fundamentals
<ul>
<li>There is usually a corpus of knowledge that everyone can agree upon is
pivotal to ‘being competent/dangerous’ in a particular subject matter.
These may overlap to other skills and it may be unclear <em>which</em> skills they
overlap in, but what matters is that these skills are relatively obvious in
the domain of note.</li>
</ul>
</li>
<li>Nice-to-Haves
<ul>
<li>This is the knowledge that might be good to spend a bit of time on as it
refines and builds on fundamentals to introduce more powerful techniques
and practices. This is where the only clarity is that they are definitely
not fundamentals and they are definitely not esoteric.</li>
</ul>
</li>
<li>Trivia
<ul>
<li>This is the stuff you probably don’t need to know, like that some AIX
machines have a weird bug in certain prompts where inputting uppercase
characters will cause the machine to reboot or that earlier, alternative
architectures supported 7-bit bytes. These tidbits of information
(sometimes not so miniscule!) are probably very costly to pick up and don’t
give you much in return.</li>
</ul>
</li>
</ul>
<p>These tags map very much to the progression of learning a subject: when you
start learning a subject, everything is rough and unclear; you should focus on
exposing yourself to as much as knowledge as possible even if you don’t quite
understand everything. This ’5yo’ view of the world helps build the framework
wherein we can fill in further details as we step towards the nice-to-haves,
but instead of becoming an ‘expert’ by picking up trivia, we try to avoid it,
and if it were important, then it would fall back into the nice-to-haves. This
is the important caveat to learning anything in general I’m trying to make
here; mastering a subject has nothing to do with knowing absolutely everything
there is to know.</p>
<p>The practice of using these tags is simple: whenever you’re faced with a
variety of options, pick fundamentals over nice-to-haves, nice-to-haves over
trivia, and (per that last regard) try to pick more fundamentals and
nice-to-haves in a variety of subject matter than trying to pick up a
collection of specific trivia for a single subject matter.</p>
<p>I liken this to the Pareto principle, which effectively states that input
effort is usually disproportionate to output gains, or, as the common quote
goes, “20% of the effort for 80% of the output” (although it’s perfectly
feasible for the opposite situation to occur). This roughly implies that most
initial upfront work is high leverage and that driving towards ‘expertise’ may
have little return on investment. What I like about this proposal is that it
accepts the fact that knowledge categorisation is messy and that there’s
probably no one in the world that knows everything.</p>
<h2>Evaluation of Others</h2>
<p>One problem with the above proposal is that it doesn’t consider the common
usage of such tiered categorisations: evaluation of others’ sets skills. Amy
Cuddy proposes that most people are judging you on your competency after
they’ve judged you on whether or not they can trust you. I propose we try to
drop the skill-evaluation-at-moment-of-evaluation tactic and focus on
evaluating others in two primary metrics: trust and ability to advance ones
skills over any given period of time. Some people call this “hiring for the
slope rather than the Y axis”.</p>
<p>That said, I’m predominantly a software engineer and in my experience I find
the former usage of this proposal to be the one I care about the most.
Determining what’s appropriate for ourselves rather than trying to divvy people
up into boxes is a far more valuable use of engineering time, and further,
evaluating people on the merits of their enthusiasm, ability and desire to
continually learn, and their capacity to both work away and in teams is more
worth it’s weight in gold than if someone is a self-proclaimed 10x engineer
capable of cranking a lot of (read: complicated, un-maintainable, mal-scoped)
code.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Reading Code is Decoding</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/reading-code-is-decoding.html</link>
      <guid>https://justanotherdot.com/posts/reading-code-is-decoding.html</guid>
      <pubDate>Sat, 27 Jan 2018 14:43:09 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>Roger Antonsen says in his Ted Talk <a href="https://www.ted.com/talks/roger_antonsen_math_is_the_hidden_secret_to_understanding_the_world"><em>Mathematics is
the Secret to Understanding the
World</em></a>,
mathematics, or rather the act of understanding, is largely about:</p>
<ul>
<li>Discovering patterns</li>
<li>Devising language(s) to express said patterns</li>
<li>Making assumptions</li>
<li>Playing around with all of the above</li>
</ul>
<p>Early this January I finished reading <em>Coders at Work</em> and in each interview
there is a recurring question of “how do you read code?” Here’s a rough summary
of some styles mentioned I found particularly useful:</p>
<ul>
<li>Get the code building early and often and make various changes to study
connections</li>
<li>Read it like literature whether printed out or jumping around</li>
<li>Rewrite the code into a version optimised for legibility</li>
<li>Puzzle through it the same way one would tackle a mathematical problem</li>
</ul>
<p>It turns out I had previously read <a href="http://www.gigamonkeys.com/code-reading/">a post from Peter
Seibel</a>, the book’s author, who had
tried on several occasions to start code reading groups at his places of work,
in which he states:</p>
<blockquote>
<p>It was sometime after that presentation that I finally realized the obvious:
code is not literature. We don’t read code, we decode it. We examine it. A
piece of code is not literature; it is a specimen.</p>
</blockquote>
<p>He goes on to quote a passage (my favourite in the book) of his interview with
Knuth (emphasis added by me):</p>
<blockquote>
<p>Knuth: But it’s really worth it for what it builds in your brain. So how do I
do it? There was a machine called the Bunker Ramo 300 and somebody told me
that the Fortran compiler for this machine was really amazingly fast, but
nobody had any idea why it worked. I got a copy of the source-code listing
for it. I didn’t have a manual for the machine, so I wasn’t even sure what
the machine language was.</p>
<p>But I took it as an interesting challenge. I could figure out <code>BEGIN</code> and
then I would start to decode. The operation codes had some two-letter
mnemonics and so I could start to figure out “This probably was a load
instruction, this probably was a branch.” And I knew it was a Fortran
compiler, so at some point it looked at column seven of a card, and that was
where it would tell if it was a comment or not.</p>
<p>After three hours I had figured out a little bit about the machine. Then I
found these big, branching tables. So it was a puzzle and I kept just making
little charts like I’m working at a security agency trying to decode a secret
code. But I knew it worked and I knew it was a Fortran compiler—it wasn’t
encrypted in the sense that it was intentionally obscure; it was only in code
because I hadn’t gotten the manual for the machine.</p>
<p>Eventually I was able to figure out why this compiler was so fast.
Unfortunately it wasn’t because the algorithms were brilliant; it was just
because they had used unstructured programming and hand optimized the code to
the hilt.</p>
<p>It was just basically the way you solve some kind of an unknown puzzle—make
tables and charts and get a little more information here and make a
hypothesis. In general when I’m reading a technical paper, it’s the same
challenge. I’m trying to get into the author’s mind, trying to figure out
what the concept is. <strong>The more you learn to read other people’s stuff, the
more able you are to invent your own in the future, it seems to me.</strong></p>
</blockquote>
<p>Alas, if we’re to treat literacy in a human language as the combined skills of
writing <em>and</em> reading, why do we place so much emphasis on the former when it
comes to teaching how to code? I now actively seek out code to read for the
same reason Knuth mentions early in his interview; dispelling magic is an
invaluable skill we crucially need to keep improving. Treating things as a
black box may sometimes help reasoning but it doesn’t mean we should keep the
covers on until the end of the universe.</p>
<p>Take my <a href="https://j2kun.svbtle.com/mathematicians-are-chronically-lost-and-confused">favourite mathematical
post</a>
by Jeremy Kun in which he discusses, with a wonderful supporting analogy from
Andrew Wiles about stumbling around a dark house looking for light switches,
that feeling lost is far more common and acceptable than the enlightened state
we assume intelligent role models seem to possess. These role models have
simply learned to live with and accept the discomfort of being lost because
that’s what it means to be in a process of learning and growing!</p>
<p>Simon Peyton Jones is well known for stating how important it is to simply
<em>do</em>, no matter how humble the project in question may be. This is fantastic
advice for coding literacy; Writing this blog post involved an initial sit down
of a roughly one-thousand word brain dump followed thereafter by approximately
two days of refinements, with lots of rereading, simply honing in on the main
theme. It’s important to get fingers moving and code executing, but it’s just
as important to advocate to new starters that reading is something they should
be pouring time and attention into.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>Fail Fast not Error Out</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/fail-fast-not-error-out.html</link>
      <guid>https://justanotherdot.com/posts/fail-fast-not-error-out.html</guid>
      <pubDate>Sat, 07 Oct 2017 13:50:02 +1100</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p><strong>tl;dr</strong> Static analysis is a form of 'failing fast' that does not consist of
leaving error based exit strategies (which should be reserved for situations
where the program simply cannot transition to a new state) in code that will
eventually be shipped to production.</p>
<p>The notion of 'failing fast' in programming details finding faults at the
earliest possible time; when the application developer is fitting out the code!
This seems to be sensible, but is often strangely antithetical to the notion of
'the only true test of code is production data'; how can we fail fast and catch
a ton of bugs when the truly icky bugs we want to smash are after we've done
some kind of deployment? Clearly the distinction here is to find bugs, in any
context, as soon as possible, production or otherwise, but that does mean the
concept can be carried over to production, where failing fast could mean major
problems (payments not being processed, account information being leaked, etc).</p>
<p>Ops people have devised all sorts of methods to roll out code in deployment to
handle situations like this; blue-green deployments, canary deployments, et.
al. all focus on testing code on a much smaller subset (on some segment of
traffic) accepting <em>some</em> failure as an acceptable loss to know if the code is
ok enough to push to 100% of the traffic. Percentage deployments put a lot of
focus on monitoring and logging. Essentially, people have to watch the metrics
after the roll out to make sure everything is ok.</p>
<p>A computation does not need to crash the program in order to fail fast:</p>
<ul>
<li>
<p>Errors are for irrecoverable states of program transition; the program
depends on writing to disk for some critical task, and the disk has been ripped
out of the server rack and can no longer be accessed via the kernel drivers.
The kernel tells us something very bad is up, and we die. This is fine, because
there's no sensible state to transition to in this scenario.</p>
</li>
<li>
<p>Exceptions are for situations where something bad happened, but it's not bad
enough to cause us to fail completely, i.e. we can do something to transition
to another sensible step. The general frame of mind is that exceptions can be
problematic when they are not caught, but can be a pain to constantly look out
for (this is the source of the 'checked exceptions' controversy in the Java
community). The primary problem with exceptions is that if an exception is not
'checked' or 'caught', then it will bubble up to the main function (entry
point) of the program and cause it to error out as above. Exceptions are said
to be sensible if they preserve <strong>progress</strong> and <strong>preservation</strong>, meaning that
they are able to move forward and they don't manipulate the types of
expressions where they are thrown. In most languages, however, we can't be sure
if something is going to throw an exception, so many programmers are told to be
defensive and paranoid; hardly the kinds of things you'd want out of people who
need to also be innovative.</p>
</li>
</ul>
<p>In most pure functional programming languages, we know less about lurking
exceptions, and this is of particular importance. When we have a type system,
which is effectively a lightweight proof system that gives us static guarantees
and checks at compile time (a form of 'fail fast' but without the problem of
leaving 'ticking time bombs' in our code base that may still present themselves
in production), then it makes no sense to fail fast in an error-prone way.
Abstractions such as monads and friends allow us to do this elegantly and
tersely.</p>
<p>It is far more ideal to let pure computations transition gracefully to new
states, failures to be found at <em>compile time</em>, and production code to be
robust and resiliant. If we extend this notion of static analysis to property
based testing, formal correctness practices, and even linters, among other
things, there are several smarter alternatives to failing quickly and
validating the correctness of our programs.</p>

        ]]></content:encoded>
    </item>
    
    <item>
      <title>A Start</title>
      <author>Ryan James Spencer</author>
      <link>https://justanotherdot.com/posts/hi.html</link>
      <guid>https://justanotherdot.com/posts/hi.html</guid>
      <pubDate>Sun, 17 Sep 2017 16:06:55 +1000</pubDate>
      <description></description>
      <content:encoded><![CDATA[
        <p>I've managed to hack together this blog using Chris Penner's alternative to
Hakyll, <a href="https://github.com/ChrisPenner/SitePipe">SitePipe</a>, as well as using a
CSS framework I've wanted to try out, <a href="http://bulma.io/">Bulma</a>, and host it on
github pages. My plan is to:</p>
<ol>
<li>Move over to some other hosting platform to do my own infra (either Digital
Ocean or Linode w/ some kind of SSL reverse proxy.)</li>
<li>Tidy up a little bit of the layout around the site.</li>
<li>Write some content!</li>
</ol>
<p>I had written a <a href="https://medium.com/@justanotherdot/sapir-whorf-and-you-f4b45ff2f216">post over at
Medium</a>
and while I liked the overall editing experience, I knew I'd eventually find
some things lacking e.g. Mathjax integration and the like, of which I was able
to sort of hack into my <a href="http://justanotherdot.tumblr.com/">previous tumblr</a>
thanks to having access to the HTML of my blog, and is another thing I'll need
to add here.</p>
<p>When I was originally looking to make my own blog my ideal layout consisted of
Markdown being dumped as HTML which would be sent over some kind of ajax
request to fill in an SPA. That still may happen, but at the moment this
workflow seems fine, and I'm hoping I can fulfill a personal quota of 250 words
per week, but we'll see how that goes.</p>
<p>Also, there's something to say about how generic (and sometimes even bland)
some publishing sites make the blog-to-blog experience. Yes, it is just text
and media, but even a little bit of a personal touch goes a long way, I think.
I worry that, especially in the realm of tech articles, people will feel the
'overal sleek' experience of something like Medium will help carry the weight
of their voices beyond the actual rigor of their articles.</p>

        ]]></content:encoded>
    </item>
    
  </channel>
</rss>