<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="author" content="Ryan James Spencer" />
    <meta name="description" content="the blog of Ryan James Spencer" />
    <meta http-equiv="content-language" content="en">
    <meta name="HandheldFriendly" content="True" />
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- OPEN GRAPH BEG -->

    <!--

    <meta property="og:url" content="https://justanotherdot.com">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Profiling with perf and DHAT on Rust code in Linux">
    <meta property="og:image" content="https://justanotherdot.com/assets/images/">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:description" content="">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@_justanotherdot">
    <meta name="twitter:creator" content="@_justanotherdot">
    <meta name="twitter:title" content="Profiling with perf and DHAT on Rust code in Linux">
    <meta name="twitter:image" content="https://justanotherdot.com/assets/images/">
    <meta name="twitter:description" content="">

    -->
    <!-- OPEN GRAPH END -->

    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" type="text/css" href="../assets/style.css" />
    <title>Profiling with perf and DHAT on Rust code in Linux &#8226; Ryan James Spencer</title>

    <script async defer data-domain="justanotherdot.com" src="https://plausible.io/js/plausible.js"></script>

  </head>
  <body>
    <div class="layout centered">
      <div class="container container-half-desktop container-full-mobile">
        <div class="header">
          <h1>Profiling with perf and DHAT on Rust code in Linux</h1>
          <a href="/"><h3 class="subtitle">Ryan James Spencer</h3></a>
        </div>
        <div class="content">
          &lt;p&gt;You&#x27;ve got a slow Rust program you&#x27;re sure you can improve, but you have no idea
where to start. If you begin changing things and rerunning the program to test
with something like &lt;code&gt;time&lt;/code&gt;, it will take you ages. Isn&#x27;t there anything better?&lt;/p&gt;
&lt;p&gt;Profilers and hardware architectures are complex beasts. It can take awhile to
understand how everything connects together and what metrics a profiling tool is
reporting are worth investigating. It doesn&#x27;t help that programmers tend to act
like profiling is a dark art, left only for the elite.&lt;/p&gt;
&lt;p&gt;At its core, profilers inform you about how your program utilizes hardware,
usually paired with some information about &lt;em&gt;where&lt;/em&gt; in the program this specific
utilization is occurring. Is the service slow when it talks to the database or
when talking over the network to the client? Is disk access a slog or are
needless cycles being wasted doing duplicated work? We &lt;em&gt;want&lt;/em&gt; our programs to
utilize hardware, but we want to do it in a way that is taking full advantage of
of the potential of the hardware in question.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;perf&lt;/code&gt; works by using Performance Monitoring Counters or PMCs (also known as
HMCs for Hardware Performance Counters). These counters track various metrics in
hardware rather than in software, which can carry its own performance penalty.
&lt;code&gt;perf&lt;/code&gt; is generally a CPU oriented profiler, but it can track some non-CPU
related metrics. Try giving &lt;code&gt;perf list&lt;/code&gt; a try in your terminal and have a look
at what&#x27;s available your target machine. All the events listed can be explicitly
tracked by passing &lt;code&gt;-e NAME&lt;/code&gt; to the bulk of the &lt;code&gt;perf&lt;/code&gt; subcommands. In this case
&lt;code&gt;NAME&lt;/code&gt; means any of the names listed in &lt;code&gt;perf list&lt;/code&gt;, but it can also be exact
counter names if you are working directly with a specific CPU architecture, but
don&#x27;t worry about this for now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here&#x27;s what we&#x27;ll cover in this post:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why we want instrumentation to gain clarity on where to spend our efforts&lt;/li&gt;
&lt;li&gt;Problem description ( you are here )&lt;/li&gt;
&lt;li&gt;Establishing benchmarks to drive our understanding and comparison of improvements&lt;/li&gt;
&lt;li&gt;A quick intro to &lt;code&gt;perf&lt;/code&gt;, and where it shines as a profiling tool&lt;/li&gt;
&lt;li&gt;Checking memory usage via &lt;code&gt;dhat&lt;/code&gt; without having to use &lt;code&gt;valgrind&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are the eager sort, &lt;a href&#x3D;&quot;https://github.com/justanotherdot/perf-and-dhat-profiling-example&quot;&gt;have a look at the code up
front&lt;/a&gt;. The
commits are broken up to allow easily following the below tutorial.&lt;/p&gt;
&lt;h3&gt;Problem Description&lt;/h3&gt;
&lt;p&gt;Let&#x27;s begin with a problem; we have a program that consumes a CSV, and processes
every row to aggregate some result. It&#x27;s noticeably a bit sluggish, and gets
slower the bigger the input.&lt;/p&gt;
&lt;p&gt;In this initial implementation, we deserialize every row into a &lt;code&gt;HashMap&lt;/code&gt; of the
headers (&lt;code&gt;String&lt;/code&gt;s) to a designated value as a series of bytes (&lt;code&gt;ByteBuf&lt;/code&gt;). We
want to later parse the bytes into something we care about. We want raw bytes to
avoid parsing them immediately into unicode, which they may not be!&lt;/p&gt;
&lt;p&gt;The headers are &lt;code&gt;String&lt;/code&gt;s rather than pre-encoding the struct names as the
headers because we want this code to run on any sort of CSV we hand it,
regardless of the number and name of the headers.&lt;/p&gt;
&lt;p&gt;The point of this example is to show that there may be multiple places where
bottlenecks live, and of different types, each of which we&#x27;ll use various forms
of profiling or assertions to check.&lt;/p&gt;
&lt;p&gt;Astute readers might catch the main inefficiency while checking the code, but
the point here is to size up problems with actual data backing the necessary
changes we are looking to take, rather than blindly stabbing in the dark and
seeing if things work or not. Often this blind approach can undo perfectly good
performance gains from other work!&lt;/p&gt;
&lt;h3&gt;Setup and prerequisites&lt;/h3&gt;
&lt;p&gt;Unfortunately, &lt;code&gt;perf&lt;/code&gt; is a linux-only tool. We will cover other profilers in
other articles, and if you are using something like &lt;code&gt;dtrace&lt;/code&gt; on a BSD, you can
similarly flume output into something like &lt;code&gt;flamegraph&lt;/code&gt;, but this article will
focus on &lt;code&gt;perf&lt;/code&gt; exclusively. &lt;code&gt;perf&lt;/code&gt; should come equipped with most linux
distros. If it&#x27;s not, you may have to do some googling on how to get it for your
current machine.&lt;/p&gt;
&lt;p&gt;When running &lt;code&gt;perf&lt;/code&gt; you&#x27;ll see some output about changing settings for the
purposes of security. If this is a local machine, you&#x27;re likely safe to simply
run &lt;code&gt;echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid&lt;/code&gt;. This setting is
to ensure that other unprivileged processes can&#x27;t see what they&#x27;re not supposed
to see.&lt;/p&gt;
&lt;p&gt;As with any optimizing adventure, we need to always ensure we are actually
running comparisons on release builds.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cargo build --release
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Artifacts are generated under &lt;code&gt;target/release/*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, we&#x27;ll tweak a few common things in our cargo manifest (&lt;code&gt;Cargo.toml&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[profile.release]
incremental &#x3D; true
debug &#x3D; true
lto &#x3D; &quot;fat&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes you may want to try &lt;code&gt;codegen-units &#x3D; 1&lt;/code&gt;, but it can aversely affect
compile times and may not yield similar gains in performance. Ditty for &lt;code&gt;lto &#x3D; &quot;fat&quot;&lt;/code&gt;, which you can also try &lt;code&gt;lto &#x3D; &quot;thin&quot;&lt;/code&gt; alternatively (or just leave it
off). As we&#x27;ll see with our future steps, it pays to verify! You&#x27;ll notice that
&lt;code&gt;debug &#x3D; true&lt;/code&gt; is also noted; this is going to be necessary when we get to using
&lt;code&gt;perf record&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, it helps to be running with &lt;code&gt;export RUSTFLAGS&#x3D;&quot;-C target-cpu&#x3D;native&quot;&lt;/code&gt; to
ensure that the compiler is choosing instructions that are more appropriate for
the actual CPU you are using. &lt;code&gt;native&lt;/code&gt; isn&#x27;t a guarantee you will get the exact
machine you are on, and if you do know the specific architecture you are on it&#x27;s
better to put that in there, instead. You can list supported names with &lt;code&gt;rustc --print target-cpus&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Checking wall times&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;time&lt;/code&gt; is a trusty tool, but &lt;code&gt;hyperfine&lt;/code&gt; takes the idea of the tool and augments
it with the statistical finesses and reporting of the &lt;code&gt;criterion&lt;/code&gt; benchmarking.
Let&#x27;s install it if you don&#x27;t have it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cargo install --force hyperfine
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It&#x27;s worth showing the difference in performance between &lt;code&gt;--release&lt;/code&gt; and debug
if you&#x27;ve never done this. Here&#x27;s a useful experiment, and a template for how to
use the tool &lt;code&gt;hyperfine&lt;/code&gt; to compare multiple targets of measurement at the same
time:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hyperfine &quot;target/debug/perf-and-dhat-profiling-example test.csv&quot; &quot;target/release/perf-and-dhat-profiling-example test.csv&quot;&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Benchmark #1: target/debug/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):     443.2 ms ±  10.8 ms    [User: 437.2 ms, System: 1.6 ms]
  Range (min … max):   431.4 ms … 467.4 ms    10 runs

Benchmark #2: target/release/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):      21.2 ms ±   1.2 ms    [User: 19.9 ms, System: 1.0 ms]
  Range (min … max):    20.1 ms …  31.0 ms    139 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet PC without any interferences from other programs. It might help to use the &#x27;--warmup&#x27; or &#x27;--prepare&#x27; op
tions.

Summary
  &#x27;target/release/perf-and-dhat-profiling-example test.csv&#x27; ran
   20.87 ± 1.30 times faster than &#x27;target/debug/perf-and-dhat-profiling-example test.csv&#x27;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To describe the above output we see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first benchmark&#x27;s report&lt;/li&gt;
&lt;li&gt;the second benchmark&#x27;s report&lt;/li&gt;
&lt;li&gt;the summary of which was fastest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this case running in release mode led to a 20.87 (give or take 1.30) times
improvement over non-release (debug).&lt;/p&gt;
&lt;h3&gt;Describing key metrics with &lt;code&gt;perf stat&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;perf stat&lt;/code&gt; gives you a summary of common hardware statistics. You almost always
want to run it with &lt;code&gt;-d&lt;/code&gt; and you can add more &lt;code&gt;d&lt;/code&gt;s to increase the number of
useful metrics it provides, but I find one is sufficient.&lt;/p&gt;
&lt;p&gt;Like any good profiling or benchmarking tool, we are unlikely to observe the
same sample every time, thus variations could be due to any number of factors,
and we can generally describe these as noise. Noise in data is normal, and
unless you are using a profiler like &lt;code&gt;cachegrind&lt;/code&gt; which runs your code in a
sandbox and records its own metrics, you re likely to see variations of all
sorts. What we want to know is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;are the variations extreme or minimal?&lt;/li&gt;
&lt;li&gt;what is center across all the variations?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus it&#x27;s a good idea to run our program with &lt;code&gt;-r 100&lt;/code&gt; which will rerun the
program under &lt;code&gt;perf stat&lt;/code&gt; 100 times.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;; perf stat -ad -r 100 target/release/perf-and-dhat-profiling-example test.csv
&amp;lt;snip, lots of output from the program itself&amp;gt;

 Performance counter stats for &#x27;system wide&#x27; (100 runs):

             84.14 msec cpu-clock                 #    3.838 CPUs utilized            ( +-  0.68%)
               130      context-switches          #    0.002 M/sec                    ( +-  2.67%)
                13      cpu-migrations            #    0.153 K/sec                    ( +-  4.42%)
               301      page-faults               #    0.004 M/sec                    ( +-  5.69%)
        72,827,084      cycles                    #    0.866 GHz                      ( +-  4.81%)  (10.15%)
       139,646,188      instructions              #    1.92  insn per cycle           ( +-  5.22%)  (25.71%)
        36,171,391      branches                  #  429.892 M/sec                    ( +-  4.06%)  (44.76%)
           108,807      branch-misses             #    0.30% of all branches          ( +-  2.81%)  (56.11%)
        48,664,574      L1-dcache-loads           #  578.372 M/sec                    ( +-  1.78%)  (56.60%)
           170,803      L1-dcache-load-misses     #    0.35% of all L1-dcache accesses  ( +-  4.5 1% )  (52.99%)
            67,067      LLC-loads                 #    0.797 M/sec                    ( +-  3.97%)  (36.22%)
            47,521      LLC-load-misses           #   70.86% of all LL-cache accesses  ( +-  4.80 % )  (17.61%)

         0.0219215 +- 0.0000420 seconds time elapsed  ( +-  0.19% )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;A handy tip:&lt;/strong&gt; &lt;em&gt;&lt;code&gt;perf&lt;/code&gt; subcommands often take a &lt;code&gt;pre&lt;/code&gt; and &lt;code&gt;post&lt;/code&gt; argument, which
you can use to plug in setup and teardown actions, such as &lt;code&gt;--pre&#x3D;cargo build --release&lt;/code&gt;, which is particularly handy to ensure you are getting results on the
latest results.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;perf stat&lt;/code&gt; will report output with bolded items of note. In my code above you
won&#x27;t notice this given how I format things but on your terminal you might. The
output can be deciphered as such, per column, in order, we have:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The arithmetic mean of the counted values&lt;/li&gt;
&lt;li&gt;The unit, if known&lt;/li&gt;
&lt;li&gt;The event name&lt;/li&gt;
&lt;li&gt;Summary of event in human terms (sometimes referred to as a shadow metric)&lt;/li&gt;
&lt;li&gt;Deviation across samples&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;The sixth column is a bit of a mystery to me. I think it has to do with scaling
the metrics somehow, but if you happen to know definitely, please get in touch!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It definitely helps to know a little bit about hardware architecture to get a
sense of where problems can arise. You can &lt;a href&#x3D;&quot;https://www.justanotherdot.com/posts/what-makes-up-a-cpu.html&quot;&gt;read my longer
summary&lt;/a&gt; of a
basic architecture, but what we care about here, in essence, is about
utilization. For example we want to make sure,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the CPU being utilized as effectively as it can be?
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;insn per cycle&lt;/code&gt; in the above output next to &lt;code&gt;instructions&lt;/code&gt; designates
roughly how many &quot;instructions per cycle&quot; are being executed on average.
If a CPU can run, say, four instructions at the same time in its pipeline,
then we&#x27;d hopefully see a value of around 4, but anything from one and
above is good, too.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are we switching between threads or are threads switching cores often?
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;context-switches&lt;/code&gt; and &lt;code&gt;cpu-migrations&lt;/code&gt; respectively show how often
how commonly threads are being scheduled or run on cores other than the
ones they were originally running on.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are the CPUs caches being maximized or we having to go to main memory a lot?
&lt;ul&gt;
&lt;li&gt;&lt;a href&#x3D;&quot;https://www.justanotherdot.com/posts/making-friends-with-caches.html&quot;&gt;Caches&lt;/a&gt;
are about reuse and speculation. If you want to get water from a well, it
would take you less time on average if you could bring a lot of water
closer to home. If we are frequently accessing data, it&#x27;s better if we can
architect a program that doesn&#x27;t have to communicate excessively with main
memory, although this may not always be possible, depending on context, of
course. All the &lt;code&gt;L1-*&lt;/code&gt; and &lt;code&gt;LLC-*&lt;/code&gt; metrics describe this, but there is a
difference between the &lt;code&gt;dcache&lt;/code&gt; and &lt;code&gt;icache&lt;/code&gt; which stand for &lt;code&gt;data cache&lt;/code&gt;
and &lt;code&gt;instruction cache&lt;/code&gt; respectively. In the same way we can utilize a
cache for data, we can do the same for instructions so we don&#x27;t have to
decode them repeatedly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are we trashing useful work in the CPU?
&lt;ul&gt;
&lt;li&gt;Speculative branch prediction tries to take a best-guess at which path on
a branch can be taken. This allows the CPU to do as much work as possible,
as early as possible, but if the CPU makes the wrong guess, it will have
to give up everything it&#x27;s doing at that moment and start over.
&lt;code&gt;branch-misses&lt;/code&gt; tells us how many of &lt;code&gt;branches&lt;/code&gt; have been wrong out of
these best-guesses.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We see that &lt;code&gt;L1&lt;/code&gt; (first level) cache hits for data (&lt;code&gt;dcache&lt;/code&gt;) aren&#x27;t that great,
and that the last level cache (&lt;code&gt;LLC&lt;/code&gt;) is missing a &lt;em&gt;lot&lt;/em&gt;. This implies that we
are having to go out to main memory often, which is a good lead on something to
improve! However, this output is failing to tell us &lt;em&gt;where&lt;/em&gt; we should be looking
in our program to see this manifest itself. For this we&#x27;ll use &lt;code&gt;perf record&lt;/code&gt; to
associate metrics to symbols in our program.&lt;/p&gt;
&lt;h3&gt;Digging deeper with &lt;code&gt;perf record&lt;/code&gt; and &lt;code&gt;perf report&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;perf record&lt;/code&gt; takes samples of events over a given period of time, and lines
them up with the specific instructions (and, therefore, symbols those
instructions belong to) giving us insight into which particular part of our code
is underutilizing or misusing our hardware,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;perf record -e L1-dcache-loads,LLC-load-misses --call-graph dwarf -- target/release/perf-and-dhat-profiling-example test.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above is going to track the two events &lt;code&gt;L1-dcache-loads&lt;/code&gt; and
&lt;code&gt;LLC-load-misses&lt;/code&gt; on the command, mentioned after the double hyphen &lt;code&gt;--&lt;/code&gt;. It is
going to use a &lt;code&gt;dwarf&lt;/code&gt; debug symbol format, which we enabled output of for
release builds above in our setup under our cargo manifest.&lt;/p&gt;
&lt;p&gt;When we run &lt;code&gt;perf report&lt;/code&gt;, we&#x27;ll be able to interactively explore hot spots in
our code. We should start with the biggest ones first. By digging into the
report we see that event &lt;code&gt;L1-dcache-loads&lt;/code&gt; has 98.42% of the event happening
around lots of code that looks to be &lt;code&gt;deserializing&lt;/code&gt; and allocating memory:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Samples: 91  of event &#x27;L1-dcache-loads&#x27;, Event count (approx.): 44200363
  Children      Self  Command          Shared Object                    Symbol
-   98.42%    41.08%  perf-and-dhat-p  perf-and-dhat-profiling-example  [.] perf_and_dhat_profiling_example::main                                                                                  ◆
   - 57.33% perf_and_dhat_profiling_example::main                                                                                                                                                  ▒
      - perf_and_dhat_profiling_example::go (inlined)                                                                                                                                              ▒
         - 52.78% perf_and_dhat_profiling_example::read_csv (inlined)                                                                                                                              ▒
            - 18.41% &amp;lt;csv::reader::ByteRecordsIter&amp;lt;R&amp;gt; as core::iter::traits::iterator::Iterator&amp;gt;::next (inlined)                                                                                   ▒
               - 11.44% csv::reader::Reader&amp;lt;R&amp;gt;::read_byte_record (inlined)                                                                                                                         ▒
                    csv::reader::Reader&amp;lt;R&amp;gt;::read_byte_record_impl (inlined)                                                                                                                        ▒
                  - csv_core::reader::Reader::read_record                                                                                                                                          ▒
                     - csv_core::reader::Reader::read_record_dfa (inlined)                                                                                                                         ▒
                          csv_core::reader::DfaClasses::scan_and_copy (inlined)                                                                                                                    ▒
               - 6.97% csv::byte_record::ByteRecord::clone_truncated (inlined)                                                                                                                     ▒
                    csv::byte_record::ByteRecord::new (inlined)                                                                                                                                    ▒
                    csv::byte_record::ByteRecord::with_capacity (inlined)                                                                                                                          ▒
                    alloc::boxed::Box&amp;lt;T&amp;gt;::new (inlined)                                                                                                                                            ▒
                    alloc::alloc::exchange_malloc (inlined)                                                                                                                                        ▒
                    &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (inlined)                                                                                                           ▒
                    alloc::alloc::Global::alloc_impl (inlined)                                                                                                                                     ▒
                    alloc::alloc::alloc (inlined)                                                                                                                                                  ▒
                    __rdl_alloc (inlined)                                                                                                                                                          ▒
                    std::sys::unix::alloc::&amp;lt;impl core::alloc::global::GlobalAlloc for std::alloc::System&amp;gt;::alloc (inlined)                                                                         ▒
                  - __GI___libc_malloc (inlined)                                                                                                                                                   ▒
                       2.77% tcache_get (inlined)                                                                                                                                                  ▒
                       1.43% checked_request2size (inlined)                                                                                                                                        ▒
            - 12.87% csv::byte_record::ByteRecord::deserialize (inlined)                                                                                                                           ▒
                 csv::deserializer::deserialize_byte_record (inlined)                                                                                                                              ▒
                 serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize (inlined)                                                            ▒
                 &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::Deserializer&amp;gt;::deserialize_map (inlined)                                                                                   ▒
               - &amp;lt;serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize::MapVisitor&amp;lt;K,V,S&amp;gt; as serde::de::Visitor&amp;gt;::visit_map (inlined)      ▒
                  + 7.33% serde::de::MapAccess::next_entry (inlined)                                                                                                                               ▒
                  + 5.55% std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;::insert (inlined)                                                                                                            ▒
            + 5.31% core::ptr::drop_in_place&amp;lt;std::collections::hash::map::IntoIter&amp;lt;alloc::string::String,serde_bytes::bytebuf::ByteBuf&amp;gt;&amp;gt; (inlined)                                                 ▒
            + 4.99% core::ptr::drop_in_place&amp;lt;csv::byte_record::ByteRecord&amp;gt; (inlined)                                                                                                               ▒
            + 4.26% perf_and_dhat_profiling_example::parse (inlined)                                                                                                                               ▒
            + 4.17% core::ptr::drop_in_place&amp;lt;alloc::string::String&amp;gt; (inlined)                                                                                                                      ▒
            + 1.70% core::ptr::drop_in_place&amp;lt;serde_bytes::bytebuf::ByteBuf&amp;gt; (inlined)                                                                                                              ▒
              1.08% &amp;lt;alloc::vec::Vec&amp;lt;T,A&amp;gt; as core::ops::deref::Deref&amp;gt;::deref                                                                                                                       ▒
         + 4.03% perf_and_dhat_profiling_example::histogram (inlined)                                                                                                                              ▒
         + 0.53% perf_and_dhat_profiling_example::read_file (inlined)                                                                                                                              ▒
Tip: Show individual samples with: perf script                                                                                                                                                     ▒
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifically, check out these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;snip&amp;gt;
-   98.42%    41.08%  perf-and-dhat-p  perf-and-dhat-profiling-example  [.] perf_and_dhat_profiling_example::main                                                                                  ◆
&amp;lt;snip&amp;gt;
         - 52.78% perf_and_dhat_profiling_example::read_csv (inlined)                                                                                                                              ▒
&amp;lt;snip&amp;gt;
            - 18.41% &amp;lt;csv::reader::ByteRecordsIter&amp;lt;R&amp;gt; as core::iter::traits::iterator::Iterator&amp;gt;::next (inlined)                                                                                   ▒
&amp;lt;snip&amp;gt;
               - 11.44% csv::reader::Reader&amp;lt;R&amp;gt;::read_byte_record (inlined)                                                                                                                         ▒
&amp;lt;snip&amp;gt;
            - 12.87% csv::byte_record::ByteRecord::deserialize (inlined)                                                                                                                           ▒
&amp;lt;snip&amp;gt;
               - &amp;lt;serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize::MapVisitor&amp;lt;K,V,S&amp;gt; as serde::de::Visitor&amp;gt;::visit_map (inlined)      ▒
&amp;lt;snip&amp;gt;
            + 5.31% core::ptr::drop_in_place&amp;lt;std::collections::hash::map::IntoIter&amp;lt;alloc::string::String,serde_bytes::bytebuf::ByteBuf&amp;gt;&amp;gt; (inlined)                                                 ▒
            + 4.99% core::ptr::drop_in_place&amp;lt;csv::byte_record::ByteRecord&amp;gt; (inlined)                                                                                                               ▒
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&#x27;s how I read this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;read_csv&lt;/code&gt; is taking up half the number of these last level cache load misses&lt;/li&gt;
&lt;li&gt;Part (18.41%) of that is reading a byte record; this is expected.&lt;/li&gt;
&lt;li&gt;another part (12.87%) is deserializing each byte record into a HashMap&lt;/li&gt;
&lt;li&gt;a small, but still sizeable, part is dropping the HashMap (5.31%), as well as the
ByteRecord (4.99%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which leads us to ask; can we get rid of all this deserialization and HashMap
construction work? A principle of performance is to be lazy, and the best way to
be lazy is to avoid doing work you don&#x27;t need to do! Before we get to tweaking
the code, let&#x27;s analyze this outside of the terminal with flamegraphs.&lt;/p&gt;
&lt;h3&gt;Making it more visual with flamegraphs&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;inferno&lt;/code&gt; is a collection of flamegraph related tooling that Jon Gjenset built
to produce different types of flamegraphs. The original flamegraph generation
was a perl script but I find it easier to install and reuse a compiled tool, so
we&#x27;ll use &lt;code&gt;inferno&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cargo install --force inferno
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to generate a flamegraph, we have to take the &lt;code&gt;perf.data&lt;/code&gt; file that is
generated as part of &lt;code&gt;perf record&lt;/code&gt; and run it through &lt;code&gt;perf script&lt;/code&gt;. Then we&#x27;ll
take &lt;code&gt;inferno-collapse-perf&lt;/code&gt; to turn the stack traces into a &quot;folded&quot; format.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;perf script | inferno-collapse-perf &amp;gt; stacks.folded
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we&#x27;ll spit the folded format into &lt;code&gt;inferno-flamegraph&lt;/code&gt; and dump the output
into an SVG.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat stacks.folded | inferno-flamegraph &amp;gt; profile.svg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can load this into a browser to interactively zoom into sections.
flamgegraphs are purely shown in terms of scale of magnitude. The x-axis has no
bearing on chronological ordering, but the relative sizes of elements is
comparable.&lt;/p&gt;
&lt;figure&gt;
  &lt;img
    src&#x3D;&quot;/assets/images/perf-flamegraph-screenshot-before.jpg&quot;
    alt&#x3D;&quot;A flamegraph of our unoptimised program&quot;
    title&#x3D;&quot;A flamegraph of our unoptimised program&quot;&gt;
  &lt;/img&gt;
&lt;/figure&gt;
&lt;p&gt;This visually shows us that &lt;code&gt;read_csv&lt;/code&gt; is the largest cost center. At a glance,
there is a fair amount of allocation from the byte record iteration that is
getting dropped, possibly needlessly, and that the deserialize code is also
spending a fair bit of time allocating memory and constructing HashMaps, as we
saw with the terminal. If we look at the code this makes sense; we are doing
this for every single record!&lt;/p&gt;
&lt;p&gt;What about specific allocations? &lt;code&gt;perf&lt;/code&gt; isn&#x27;t going to be able to tell us
specifics about allocations, and for that we&#x27;ll turn to Nicholas Nethercoate&#x27;s
wonderful &lt;code&gt;dhat&lt;/code&gt; Rust library, built out of the same tooling for &lt;code&gt;dhat&lt;/code&gt; on
valgrind.&lt;/p&gt;
&lt;h3&gt;Allocation analysis&lt;/h3&gt;
&lt;p&gt;We&#x27;ll plug in the &lt;code&gt;dhat&lt;/code&gt; stuff under a feature flag so we can easily turn it on
later when we want, and leave it out of the way when we don&#x27;t. I&#x27;ll be
unimaginative and call this feature flag &lt;code&gt;dhat-on&lt;/code&gt;, thus in our cargo manifest,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[features]
default &#x3D; []
dhat-on &#x3D; []
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plus, you&#x27;ll need to make sure you turn off &lt;code&gt;lto&lt;/code&gt; that we had on before with
&lt;code&gt;lto &#x3D; false&lt;/code&gt;. We want to be able to see the stack traces for the allocations,
and optimisations such as &lt;code&gt;lto&lt;/code&gt; will do as much inlining across dependencies as
possible.&lt;/p&gt;
&lt;p&gt;Here&#x27;s the patch plugging it in to the codebase,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;diff --git a/src/main.rs b/src/main.rs
index bd43846..b140355 100644
 a/src/main.rs
+++ b/src/main.rs
@@ -1,4 +1,6 @@
 use csv::Reader;
+#[cfg(feature &#x3D; &quot;dhat-on&quot;)]
+use dhat;
 use serde_bytes::ByteBuf;
 use std::collections::HashMap;
 use std::convert::TryInto;
@@ -7,6 +9,12 @@ use std::fs::File;
 use std::io::{BufReader, Read};
 use std::path::PathBuf;

+#[cfg(feature &#x3D; &quot;dhat-on&quot;)]
+use dhat::{Dhat, DhatAlloc};
+#[cfg(feature &#x3D; &quot;dhat-on&quot;)]
+#[global_allocator]
+static ALLOCATOR: DhatAlloc &#x3D; DhatAlloc;
+
 type Record &#x3D; HashMap&amp;lt;String, ByteBuf&amp;gt;;

 const NULL: &amp;amp;&#x27;static str &#x3D; &quot;NULL&quot;; // or whatever.
@@ -77,6 +85,9 @@ fn go(input: &amp;amp;str) -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn Error&amp;gt;&amp;gt; {
 }

 fn main() {
+    #[cfg(feature &#x3D; &quot;dhat-on&quot;)]
+    let _dhat &#x3D; Dhat::start_heap_profiling();
+
     go(&quot;test.csv&quot;).unwrap_or_else(|e| {
         eprintln!(&quot;[csv-count] {}&quot;, e);
         std::process::exit(1);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, just run the program as you would. The resulting analysis will dump out a
&lt;code&gt;dhat-heap.json&lt;/code&gt; that we can take and load into a
&lt;a href&#x3D;&quot;https://nnethercote.github.io/dh_view/dh_view.html&quot;&gt;viewer&lt;/a&gt;. You can also clone
the valgrind project to get access to the same HTML to view the output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;▼ PP 1/1 (7 children) {
    Total:     18,443,933 bytes (100%, 8,095,768.23/s) in 374,530 blocks (100%, 164,395.96/s), avg size 49.25 bytes, avg lifetime 121.47 µs (0.01% of program duration)
    At t-gmax: 2,312,738 bytes (100%) in 26 blocks (100%), avg size 88,951.46 bytes
    At t-end:  1,024 bytes (100%) in 1 blocks (100%), avg size 1,024 bytes
    Allocated at {
      #0: [root]
    }
  }
  ├── PP 1.1/7 {
  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 112,347 blocks (30%, 54,230.42/s), avg size 8.67 bytes, avg lifetime 13.41 µs (0% of program duration)
  │     Max:       26 bytes in 3 blocks, avg size 8.67 bytes
  │     At t-gmax: 26 bytes (0%) in 3 blocks (11.54%), avg size 8.67 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23a8ab5: &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23a8ab5: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::allocate_in (raw_vec.rs:195:45)
  │       #3: 0x556ef23a8ab5: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::with_capacity_in (raw_vec.rs:136:9)
  │       #4: 0x556ef23a8ab5: alloc::vec::Vec&amp;lt;T,A&amp;gt;::with_capacity_in (mod.rs:580:20)
  │       #5: 0x556ef23a8ab5: &amp;lt;T as alloc::slice::hack::ConvertVec&amp;gt;::to_vec (slice.rs:211:25)
  │       #6: 0x556ef23a8ab5: alloc::slice::hack::to_vec (slice.rs:163:9)
  │       #7: 0x556ef23a8ab5: alloc::slice::&amp;lt;impl [T]&amp;gt;::to_vec_in (slice.rs:476:9)
  │       #8: 0x556ef23a8ab5: alloc::slice::&amp;lt;impl [T]&amp;gt;::to_vec (slice.rs:453:9)
  │       #9: 0x556ef23a8ab5: &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::Deserializer&amp;gt;::deserialize_byte_buf::{{closure}} (deserializer.rs:449:50)
  │       #10: 0x556ef23a8ab5: core::result::Result&amp;lt;T,E&amp;gt;::and_then (result.rs:704:22)
  │       #11: 0x556ef23a8ab5: &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::Deserializer&amp;gt;::deserialize_byte_buf (deserializer.rs:448:9)
  │       #12: 0x556ef23a87a7: &amp;lt;serde_bytes::bytebuf::ByteBuf as serde::de::Deserialize&amp;gt;::deserialize (bytebuf.rs:251:9)
  │       #13: 0x556ef23a87a7: &amp;lt;core::marker::PhantomData&amp;lt;T&amp;gt; as serde::de::DeserializeSeed&amp;gt;::deserialize (mod.rs:785:9)
  │       #14: 0x556ef23a87a7: &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::MapAccess&amp;gt;::next_value_seed (deserializer.rs:654:9)
  │       #15: 0x556ef23a87a7: serde::de::MapAccess::next_entry_seed (mod.rs:1812:34)
  │       #16: 0x556ef23a87a7: serde::de::MapAccess::next_entry (mod.rs:1860:9)
  │       #17: 0x556ef23a87a7: &amp;lt;serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize::MapVisitor&amp;lt;K,V,S&amp;gt; as serde::de::Visitor&amp;gt;::visit_map (impls.rs:1284:61)
  │       #18: 0x556ef23a87a7: &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::Deserializer&amp;gt;::deserialize_map (deserializer.rs:520:13)
  │       #19: 0x556ef23a84f6: serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize (impls.rs:1293:17)
  │       #20: 0x556ef23a84f6: csv::deserializer::deserialize_byte_record (deserializer.rs:47:5)
  │       #21: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │     }
  │   }
&amp;lt;snip&amp;gt;
  ├─▼ PP 1.3/7 (2 children) {
  │     Total:     7,939,272 bytes (43.05%, 3,832,323.84/s) in 37,450 blocks (10%, 18,077.29/s), avg size 212 bytes, avg lifetime 22.18 µs (0% of program duration)
  │     At t-gmax: 212 bytes (0.01%) in 1 blocks (3.85%), avg size 212 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23a6e3b: &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23a6e3b: hashbrown::raw::alloc::inner::do_alloc (alloc.rs:11:9)
  │       #3: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&amp;lt;A&amp;gt;::new_uninitialized (mod.rs:1157:38)
  │       #4: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&amp;lt;A&amp;gt;::fallible_with_capacity (mod.rs:1186:30)
  │     }
  │   }
  │   ├── PP 1.3.1/2 {
  │   │     Total:     7,939,188 bytes (43.04%, 3,832,283.29/s) in 37,449 blocks (10%, 18,076.81/s), avg size 212 bytes, avg lifetime 22.1 µs (0% of program duration)
  │   │     Max:       212 bytes in 1 blocks, avg size 212 bytes
  │   │     At t-gmax: 212 bytes (0.01%) in 1 blocks (3.85%), avg size 212 bytes
  │   │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │   │     Allocated at {
  │   │       ^1: 0x556ef23a6e3b: &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (alloc.rs:226:9)
  │   │       ^2: 0x556ef23a6e3b: hashbrown::raw::alloc::inner::do_alloc (alloc.rs:11:9)
  │   │       ^3: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&amp;lt;A&amp;gt;::new_uninitialized (mod.rs:1157:38)
  │   │       ^4: 0x556ef23a6e3b: hashbrown::raw::RawTableInner&amp;lt;A&amp;gt;::fallible_with_capacity (mod.rs:1186:30)
  │   │       #5: 0x556ef23a0102: hashbrown::raw::RawTableInner&amp;lt;A&amp;gt;::prepare_resize (mod.rs:1396:29)
  │   │       #6: 0x556ef23a0102: hashbrown::raw::RawTable&amp;lt;T,A&amp;gt;::resize (mod.rs:788:17)
  │   │       #7: 0x556ef23a0102: hashbrown::raw::RawTable&amp;lt;T,A&amp;gt;::reserve_rehash (mod.rs:693:13)
  │   │       #8: 0x556ef23a6ca7: hashbrown::raw::RawTable&amp;lt;T,A&amp;gt;::reserve (mod.rs:646:16)
  │   │       #9: 0x556ef23a6ca7: hashbrown::raw::RawTable&amp;lt;T,A&amp;gt;::insert (mod.rs:827:17)
  │   │       #10: 0x556ef23a7d1e: hashbrown::map::HashMap&amp;lt;K,V,S,A&amp;gt;::insert (map.rs:1266:13)
  │   │       #11: 0x556ef23a87ff: std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;::insert (map.rs:845:9)
  │   │       #12: 0x556ef23a87ff: &amp;lt;serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize::MapVisitor&amp;lt;K,V,S&amp;gt; as serde::de::Visitor&amp;gt;::visit_map (impls.rs:1285:29)
  │   │       #13: 0x556ef23a87ff: &amp;lt;&amp;amp;mut csv::deserializer::DeRecordWrap&amp;lt;T&amp;gt; as serde::de::Deserializer&amp;gt;::deserialize_map (deserializer.rs:520:13)
  │   │       #14: 0x556ef23a84f6: serde::de::impls::&amp;lt;impl serde::de::Deserialize for std::collections::hash::map::HashMap&amp;lt;K,V,S&amp;gt;&amp;gt;::deserialize (impls.rs:1293:17)
  │   │       #15: 0x556ef23a84f6: csv::deserializer::deserialize_byte_record (deserializer.rs:47:5)
  │   │       #16: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │   │     }
  │   │   }
&amp;lt;snip&amp;gt;
  ├── PP 1.6/7 {
  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 37,449 blocks (10%, 18,076.81/s), avg size 26 bytes, avg lifetime 39.84 µs (0% of program duration)
  │     Max:       26 bytes in 1 blocks, avg size 26 bytes
  │     At t-gmax: 26 bytes (0%) in 1 blocks (3.85%), avg size 26 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x556ef23ad5e4: &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (alloc.rs:226:9)
  │       #2: 0x556ef23ad5e4: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::allocate_in (raw_vec.rs:195:45)
  │       #3: 0x556ef23ad5e4: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::with_capacity_in (raw_vec.rs:136:9)
  │       #4: 0x556ef23ad5e4: alloc::vec::Vec&amp;lt;T,A&amp;gt;::with_capacity_in (mod.rs:580:20)
  │       #5: 0x556ef23ad5e4: &amp;lt;T as alloc::slice::hack::ConvertVec&amp;gt;::to_vec (slice.rs:211:25)
  │       #6: 0x556ef23ad5e4: alloc::slice::hack::to_vec (slice.rs:163:9)
  │       #7: 0x556ef23ad5e4: alloc::slice::&amp;lt;impl [T]&amp;gt;::to_vec_in (slice.rs:476:9)
  │       #8: 0x556ef23ad5e4: alloc::slice::&amp;lt;impl [T]&amp;gt;::to_vec (slice.rs:453:9)
  │       #9: 0x556ef23ad5e4: csv::byte_record::ByteRecord::clone_truncated (byte_record.rs:508:23)
  │       #10: 0x556ef23ad5e4: &amp;lt;csv::reader::ByteRecordsIter&amp;lt;R&amp;gt; as core::iter::traits::iterator::Iterator&amp;gt;::next (reader.rs:2166:33)
  │       #11: 0x556ef23aa82f: perf_and_dhat_profiling_example::read_csv (main.rs:43:19)
  │     }
  │   }
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pay attention to a few things here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;blocks is just a specific call to an allocation, so &quot;100 blocks&quot; means 100
individual allocation calls, I think.&lt;/li&gt;
&lt;li&gt;the hashbrown (HashMap) calls end up producing more total bytes than the
actual deserialize calls themselves, but the byterecord iteration seem to be
broken up, likely because the byterecord needs to allocate for the individual
values or handing back the ByteRecord value itself.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Particularly, let&#x27;s look at this output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  │     Total:     973,674 bytes (5.28%, 469,997.01/s) in 112,347 blocks (30%, 54,230.42/s), avg size 8.67 bytes, avg lifetime 13.41 µs (0% of program duration)
  │     Max:       26 bytes in 3 blocks, avg size 8.67 bytes
&amp;lt;snip&amp;gt;
  │     Allocated at {
  │       #1: 0x556ef23a8ab5: &amp;lt;alloc::alloc::Global as core::alloc::Allocator&amp;gt;::allocate (alloc.rs:226:9)
&amp;lt;snip&amp;gt;
  │       #21: 0x556ef23aa85a: csv::byte_record::ByteRecord::deserialize (byte_record.rs:233:9)
  │     }
  │   }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although it&#x27;s useful to know the total, max, and etc. what this is telling us is
that each deserialize step is leading to plenty of allocations. We had
previously estimated, based on &lt;code&gt;perf&lt;/code&gt;s output, that it would be good idea to
stop deserializing if we can, but this confirms that it is definitely happening
a lot and is non-trivial in the scheme of things. Before we begin changing code,
we should put in benchmarks to let others running our code confirm results for
themselves (possibly documenting our process like how we&#x27;ve done in this
article, too!).&lt;/p&gt;
&lt;h3&gt;Benchmarks for reproduction&lt;/h3&gt;
&lt;p&gt;We now know we want to remove our &lt;code&gt;deserialize&lt;/code&gt; code and that means finding
something we can recycle on every iteration of our loop. Reading through the
&lt;code&gt;csv&lt;/code&gt; docs it looks like we can reuse a single &lt;code&gt;ByteRecord&lt;/code&gt;. Having benchmarks
in place will help us verify for ourselves, and others, that the change we&#x27;ve
put in has actually made a net positive gain.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#[cfg(test)]
mod tests {
    extern crate test;

    use super::*;
    use test::{black_box, Bencher};

    #[bench]
    fn bench_read_csv(b: &amp;amp;mut Bencher) {
        let bytes &#x3D; read_file(&amp;amp;&quot;test.csv&quot;.into()).expect(&quot;failed to read file&quot;);
        b.iter(|| {
            for _ in 1..2 {
                black_box(read_csv(&amp;amp;bytes)).expect(&quot;benchmark failure&quot;);
            }
        });
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&#x27;ll have to chuck &lt;code&gt;#[feature(test)]&lt;/code&gt; at the top of &lt;code&gt;main.rs&lt;/code&gt; and run the
benches with nightly. You can pass &lt;code&gt;+nightly&lt;/code&gt; to do that on the fly without
having to change the current toolchain.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;; cargo +nightly bench
   Compiling perf-and-dhat-profiling-example v0.1.0 (/home/rjs/repos/perf-and-dhat-profiling-exam
ple)
    Finished bench [optimized] target(s) in 1.54s
     Running unittests (target/release/deps/perf_and_dhat_profiling_example-3cb69d975d903441)

running 1 test
test tests::bench_read_csv ... bench:  18,044,486 ns/iter (+/- 551,993)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 0 filtered out; finished in 5.47s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows us an average of 18,044,486 nanoseconds per iteration. 1,000
nanoseconds is a microseconds, and 1000 microseconds is a millisecond, thus we have 18
milliseconds per iteration to run against our test case.&lt;/p&gt;
&lt;p&gt;Now let&#x27;s try the single allocation for ByteRecord,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;diff --git a/src/main.rs b/src/main.rs
index 8d425c1..a68b0ec 100644
 a/src/main.rs
+++ b/src/main.rs
@@ -3,7 +3,6 @@
 use csv::Reader;
 #[cfg(feature &#x3D; &quot;dhat-on&quot;)]
 use dhat;
-use serde_bytes::ByteBuf;
 use std::collections::HashMap;
 use std::convert::TryInto;
 use std::error::Error;
@@ -17,8 +16,6 @@ use dhat::{Dhat, DhatAlloc};
 #[global_allocator]
 static ALLOCATOR: DhatAlloc &#x3D; DhatAlloc;

-type Record &#x3D; HashMap&amp;lt;String, ByteBuf&amp;gt;;
-
 const NULL: &amp;amp;&#x27;static str &#x3D; &quot;NULL&quot;; // or whatever.

 #[derive(Clone, Debug, PartialEq, Eq, Hash)]
@@ -40,12 +37,12 @@ pub fn read_file(data: &amp;amp;PathBuf) -&amp;gt; Result&amp;lt;Vec&amp;lt;u8&amp;gt;, Box&amp;lt;dyn Error&amp;gt;&amp;gt; {

 pub fn read_csv(data: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Vec&amp;lt;Option&amp;lt;Field&amp;gt;&amp;gt;, Box&amp;lt;dyn Error&amp;gt;&amp;gt; {
     let mut reader &#x3D; Reader::from_reader(data);
-    let headers &#x3D; reader.headers().unwrap().clone().into_byte_record();
+    let _headers &#x3D; reader.headers().unwrap().clone().into_byte_record();
     let mut fields &#x3D; vec![];
-    for record in reader.byte_records() {
-        let record &#x3D; record?;
-        let record: Record &#x3D; record.deserialize(Some(&amp;amp;headers))?;
-        for (_, value) in record.into_iter() {
+    let mut record &#x3D; csv::ByteRecord::new();
+    while !reader.is_done() {
+        reader.read_byte_record(&amp;amp;mut record).unwrap();
+        for value in record.iter() {
             fields.push(parse(&amp;amp;value));
         }
     }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and rerun the benchmark,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   Compiling perf-and-dhat-profiling-example v0.1.0 (/home/rjs/repos/perf-and-dhat-profiling-exam
ple)
    Finished bench [optimized] target(s) in 1.22s
     Running unittests (target/release/deps/perf_and_dhat_profiling_example-3cb69d975d903441)

running 1 test
test tests::bench_read_csv ... bench:   4,528,433 ns/iter (+/- 1,328,048)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 0 filtered out; finished in 4.22s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&#x27;re down to four milliseconds! Which is a ~4x improvement in terms of timing. Let&#x27;s confirm it in a few ways:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;perf stat&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; Performance counter stats for &#x27;system wide&#x27; (100 runs):

             37.61 msec cpu-clock                 #    3.537 CPUs utilized            ( +-  1.93% )
                94      context-switches          #    0.002 M/sec                    ( +-  3.05% )
                11      cpu-migrations            #    0.300 K/sec                    ( +-  3.59% )
               300      page-faults               #    0.008 M/sec                    ( +-  6.57% )
        34,636,088      cycles                    #    0.921 GHz                      ( +-  6.86% )  (13.69%)
        57,927,487      instructions              #    1.67  insn per cycle           ( +-  6.59% )  (51.81%)
        15,272,168      branches                  #  406.078 M/sec                    ( +-  5.84% )  (88.26%)
            38,666      branch-misses             #    0.25% of all branches          ( +-  4.23% )  (86.61%)
        16,682,061      L1-dcache-loads           #  443.566 M/sec                    ( +-  5.51% )  (48.15%)
           452,163      L1-dcache-load-misses     #    2.71% of all L1-dcache accesses  ( +-  8.41% )  (11.68%)
             5,912      LLC-loads                 #    0.157 M/sec                    ( +- 70.99% )  (0.03%)
     &amp;lt;not counted&amp;gt;      LLC-load-misses                                               (0.00%)

         0.0106321 +- 0.0000668 seconds time elapsed  ( +-  0.63% )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where we can see &lt;code&gt;LLC-load-misses&lt;/code&gt; aren&#x27;t even counted anymore.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;perf record&lt;/code&gt; and friends, which show that read_byte_record takes just as long
as our &lt;code&gt;parse&lt;/code&gt; code.&lt;/p&gt;
&lt;figure&gt;
  &lt;img
    src&#x3D;&quot;/assets/images/perf-flamegraph-screenshot-after.jpg&quot;
    alt&#x3D;&quot;A flamegraph of our optimised program&quot;
    title&#x3D;&quot;A flamegraph of our optimised program&quot;&gt;
  &lt;/img&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;dhat&lt;/code&gt; instrumentation, which shows our overall memory footprint is drastically reduced,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;snip&amp;gt;
PP 1/1 (18 children) {
    Total:     3,539,231 bytes (100%, 115,733,004.15/s) in 40 blocks (100%, 1,308/s), avg size 88,480.78 bytes, avg lifetime 8,511.5 µs (27.83% of program duration)
    At t-gmax: 2,312,340 bytes (100%) in 16 blocks (100%), avg size 144,521.25 bytes
    At t-end:  1,024 bytes (100%) in 1 blocks (100%), avg size 1,024 bytes
    Allocated at {
      #0: [root]
    }
  }
  ├─▼ PP 1.1/18 (3 children) {
  │     Total:     262,280 bytes (7.41%, 8,576,567.15/s) in 22 blocks (55%, 719.4/s), avg size 11,921.82 bytes, avg lifetime 4,408.95 µs (14.42% of program duration)
  │     At t-gmax: 131,184 bytes (5.67%) in 5 blocks (31.25%), avg size 26,236.8 bytes
  │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │     Allocated at {
  │       #1: 0x565479a3edc2: &amp;lt;dhat::DhatAlloc as core::alloc::global::GlobalAlloc&amp;gt;::alloc (lib.rs:244:9)
  │       #2: 0x565479a37aa5: alloc::raw_vec::finish_grow (result.rs:0:23)
  │     }
  │   }
  │   ├── PP 1.1.1/3 {
  │   │     Total:     262,136 bytes (7.41%, 8,571,858.34/s) in 15 blocks (37.5%, 490.5/s), avg size 17,475.73 bytes, avg lifetime 1,068.07 µs (3.49% of program duration)
  │   │     Max:       131,072 bytes in 1 blocks, avg size 131,072 bytes
  │   │     At t-gmax: 131,072 bytes (5.67%) in 1 blocks (6.25%), avg size 131,072 bytes
  │   │     At t-end:  0 bytes (0%) in 0 blocks (0%), avg size 0 bytes
  │   │     Allocated at {
  │   │       ^1: 0x565479a3edc2: &amp;lt;dhat::DhatAlloc as core::alloc::global::GlobalAlloc&amp;gt;::alloc (lib.rs:244:9)
  │   │       ^2: 0x565479a37aa5: alloc::raw_vec::finish_grow (result.rs:0:23)
  │   │       #3: 0x565479a307f4: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::grow_amortized (raw_vec.rs:442:19)
  │   │       #4: 0x565479a307f4: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::reserve::do_reserve_and_handle (raw_vec.rs:333:28)
  │   │       #5: 0x565479a3819f: alloc::raw_vec::RawVec&amp;lt;T,A&amp;gt;::reserve (raw_vec.rs:337:13)
  │   │     }
  │   │   }
&amp;lt;snip&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which shows us that we&#x27;ve gone from 18,443,933 bytes to 3,539,231 bytes and we
are mostly spending time largely building &lt;code&gt;RawVec&lt;/code&gt;s now.&lt;/p&gt;
&lt;p&gt;Finally &lt;code&gt;hyperfine&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;; hyperfine &quot;target/release/perf-and-dhat-profiling-example test.csv&quot; &quot;target/release/perf-and-dhat-profiling-example-unoptimized test.csv&quot;
Benchmark #1: target/release/perf-and-dhat-profiling-example test.csv
  Time (mean ± σ):       8.9 ms ±   0.5 ms    [User: 8.0 ms, System: 0.9 ms]
  Range (min … max):     8.4 ms …  11.8 ms    291 runs

Benchmark #2: target/release/perf-and-dhat-profiling-example-unoptimized test.csv
  Time (mean ± σ):      23.7 ms ±   1.2 ms    [User: 22.5 ms, System: 1.1 ms]
  Range (min … max):    22.6 ms …  31.8 ms    121 runs

  Warning: Statistical outliers were detected. Consider re-running this benchmark on a quiet PC without any interferences from other programs. It might help to use the &#x27;--warmup&#x27; or &#x27;--prepare&#x27; options.

Summary
  &#x27;target/release/perf-and-dhat-profiling-example test.csv&#x27; ran
    2.66 ± 0.19 times faster than &#x27;target/release/perf-and-dhat-profiling-example-unoptimized test.csv&#x27;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lesson: &quot;trivial allocations&quot; are not so trivial!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;perf&lt;/code&gt; and a memory analysis tool like &lt;code&gt;dhat&lt;/code&gt; can give you a pretty solid
picture of what is going on in terms of hardware utilization. Obviously it&#x27;s not
the only profilers at your disposal, and we&#x27;ll likely tackle some other articles
doing the same above with different tooling, but discovering issues backed by
numbers is fundamentally crucial in demonstrating to yourself and others that a
change is worth accepting.&lt;/p&gt;

        </div>
        <!-- NEWSLETTER BEGIN -->
        <div class="centered">
          <hr>
          <div class="newsletter">
            <h2 class="centered">Want updates when I release new articles?</h2>
            <div>
              <script async data-uid="c3c1e70aed" src="https://crafty-experimenter-867.ck.page/c3c1e70aed/index.js"></script>
            </div>
          </div>
        </div>
        <!-- NEWSLETTER END -->
      </div>
    </div>
  </body>
</html>
